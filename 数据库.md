MySQL 见 `后端.md`

# 理论

## 概论

> ### 导学
>
> > 大数据, 或称巨量资料，是指需要新处理模式才能具有更强的决策力、洞察力和流程优化能力的海量、高增长率和多样化的信息资产
> >
> > 大数据的5V特点：Volume(大量)、Velocity(高速)、Variety(多样)、Value(价值密度)、Veracity(真实性)
>
> 数据库管理系统(database management system, DBMS)是由一个相互关联的数据的集合和一组用以访问、管理和控制这些数据的程序(建立在操作系统之上的系统软件)组成
>
> 这个相互关联的数据集合通常称为数据库(database, DB)，其中包含了关于某个企业信息系统的所有信息。 
>
> 设计DBMS的目的是为了有效地管理大量的数据，并解决操作系统的文件处理系统中存在的问题。
>
> 数据的有效管理，包括定义数据存储结构、提供数据操作机制。
> 不仅需要解决数据的共享性、独立性和数据之间的联系问题；
> 还需要解决数据的完整性、原子性、并发控制和安全性问题。
>
> 数据库系统(database system, DBS)，是指在计算机系统中引入数据库后的系统，一般由数据库、数据库管理系统(及其应用开发工具)、应用系统、数据库管理员和最终用户构成。——人机系统  
>
> > 服务外包：信息技术外包服务(ITO)，业务流程外包服务(BPO)

### 数据库系统的作用

数据：描述事物的符号记录。分为：

- 无结构的文本描述形式
- 结构化数据（表格，表中一行构成一条记录）
- 半结构化数据（XML，一般自描述，结构和内容混在一起)

数据处理：从大量的、可能是杂乱无章的、难以理解的数据中抽取并推导出对于某些特定的人们来说有价值、有意义的数据。

数据管理：对数据进行有效的分类、组织、编码、存储、检索、维护和应用——数据处理的中心问题。

**数据库管理技术的产生与发展**：

- 人工管理阶段(20c50s中以前)，数据面向应用程序，一个数据集对应一个程序，由应用程序定义和管理，无法共享利用，存在大量冗余数据

- 文件系统阶段(20c50s后-60s中)，计算机开始应用于数据管理，按文件名访问，按记录进行存取，数据放在多个数据文件

  ![image-20220531142235142](img/image-20220531142235142.png)

  主要特点：文件内的结构性，即一个文件内的数据是按记录进行组织的，这样的数据是有结构的(语义的) ，语义明确；整体上还是无结构的，即多个文件之间是相互独立的，无法建立全局的结构化数据管理模式

  弊端：数据共享性差 即数据冗余和不一致（不同应用程序难共享），数据独立性差（不易扩充，数据与应用程序之间缺乏逻辑独立性），数据孤立，数据获取困难（数据间的联系缺乏有效管理），完整性问题，安全性问题，原子性问题，并发访问异常

- 数据库管理系统阶段

  主要特点：(**数据库系统特点**)

  - 数据结构化。与文件系统的本质区别。不仅内部结构化，而是将数据以及数据之间的联系统一管理起来，使之结构化。不是仅仅针对某一个应用，而是面向全组织的所有应用。

  - 数据的共享度高，冗余度低，易扩充

  - 数据独立性高

  - 数据由数据库管理系统（DBMS）统一管理和控制 

    （数据的安全性保护，数据的完整性检查，并发控制，数据库恢复

  <u>助记：独统共结(独(自)统(治)共(\*党)解(\*军))</u>



数据库管理系统(DBMS)是由一个相互关联的数据的集合(数据库, DB)和一组用以访问、管理和控制这些数据的程序组成，是系统软件。

数据的完整性是指数据的正确性、有效性和相容性，也称为一致性约束

数据独立是指数据的使用(即应用程序)与数据的说明(即数据的组织结构与存储方式)分离，使得应用程序只需要考虑如何使用数据，而无须关心数据库中的数据是如何构造和存储的(表现为不会因为系统数据存储结构与数据逻辑结构的变化而影响应用程序)

- 数据独立性用来描述应用程序与数据结构之间的依赖程度，包括数据的物理独立性和数据的逻辑独立性
  - 物理独立性是指用户的应用程序与数据库中数据的物理结构是相互独立的；应用程序只关心数据的逻辑结构
  - 逻辑独立性是指用户的应用程序与数据库中数据的逻辑结构是相互独立的；应用程序只关心数据的局部逻辑结构(即应用视图) 




### 数据模型

数据库结构的基础是数据模型(data model)，是一个描述数据结构、数据操作以及数据约束的数学形式体系(即概念及其符号表示系统)(**三要素**)

- 数据结构：描述数据库的组成对象以及对象之间的联系 

- 数据操作：指对数据库中各种对象(型)的实例(值)允许执行的操作集合，包括操作及有关的操作规则 

- 数据完整性约束：一组数据完整性规则，是数据、数据语义和数据联系所具有的制约和依存规则，以保证数据库中数据的正确、有效和相容

  > 数据约束是对数据结构和数据操作的一致性、完整性约束，亦称为数据完整性约束 

<u>助记：完结操(玩节操)</u>

根据数据抽象的不同级别，将数据模型划分为3类：(**数据模型的分层**)

- 概念模型：概念层次的数据模型，也称为信息模型
- 逻辑模型：用于描述数据库数据的逻辑结构
- 物理模型：用来描述数据的物理存储结构和存取方法 



概念模型有实体-联系模型(E-R模型)和面向对象模型(OO模型) 

OO模型是用面向对象观点来描述现实世界实体(对象)的逻辑组织、对象间限制和联系等的模型

E-R模型基于对现实世界的如下认识：现实世界是由一组称作实体的基本对象以及这些对象间的联系构成

实体是现实世界中可区别于其他对象的一件“事情”或一个“物体”



逻辑模型是是用户通过数据库管理系统看到的现实世界，是按计算机系统的观点对数据建模，即数据的计算机实现形式，主要用于DBMS的实现。不同的DBMS提供不同的逻辑数据模型

- 层次模型(hierarchical model)

  优点：数据结构比较简单清晰，查询效率高，提供了良好的数据完整性支持

  缺点：不能很好地表达多对多联系，解决的办法：一是通过引入冗余数据(易产生不一致性)，二是创建非自然的数据结构(引入虚拟结点) ；查询孩子结点必须通过双亲结点；由于结构严密，层次命令趋于程序化 

- 网状模型(network model)

  条件：允许一个以上的结点无双亲，一个结点可以有多个双亲

  优点：能够更为直接地描述现实世界；具有良好的性能，存取效率较高

  缺点：结构比较复杂，而且随着应用规模的扩大，数据库的结构会变得越来越复杂，不利于最终用户掌握；操作语言比较复杂 

- 关系模型(relational model)

  优点：

  - 严格的数学基础：有关系代数作为语言模型，有关系数据理论作为理论基础
  - 概念单一：无论实体还是实体之间的联系都是用关系来表示，对数据(关系)的操作(查询和更新)结果还是关系。所以其数据结构简单、清晰，用户易懂易用
  - 存取路径透明：具有更高的数据独立性、更好的安全保密性，简化了程序员的工作，提高了软件的开发和维护效率

  缺点：

  - 由于存取路径对用户透明，查询效率往往不如非关系数据模型
  - 为了提高性能，DBMS必须对用户的查询请求进行查询优化，这样就增加了DBMS的开发难度

- 面向对象模型(即OO模型)

- XML模型

- 对象关系模型(object relational model)



物理模型：物理层是数据抽象的最低层。

> 从现实世界到概念模型的转换是由数据库设计人员完成
>
> 从概念模型到逻辑模型的转换可以由数据库设计人员完成，也可以用数据库设计工具协助设计人员完成
>
> 从逻辑模型到物理模型的转换一般由DBMS来完成 
>
> 概念模型：从现实世界到信息世界
> 逻辑模型：信息世界到逻辑机器世界
> 物理模型：逻辑机器世界到物理机器世界

三层模型是数据库设计的工具和方法(要满足DBMS体系结构的要求)，目的

1. 较真实地模拟现实世界、容易被人理解、便于计算机实现。
2. 提供全局逻辑模型：支持整体结构化，从而实现数据共享度高、冗余度低。
3. 同时满足DBMS三级模式结构要求。



关系模型：

- 关系(relation)：一个关系对应一张二维表，每一个关系有一个名称，即关系名

- 元组(tuple)：表中的一行称为一个元组

- 属性(attribute)：表中的一列称为一个属性，每一个属性有一个名称，即属性名

- 码(key)：也称为码键或键。表中的某个属性或属性组，它可以唯一地确定关系中的一个元组

- 域(domain)：属性的取值范围

- 分量(component)：元组中的一个属性值

- 外码(foreign key)：表中的某个属性或属性组，用来描述本关系中的元组(实体)与另一关系中的元组(实体)之间的联系

  外码的取值范围对应于另一个关系的码的取值范围的子集

- 关系模式(relational schema)：通过关系名和属性名列表对关系进行描述，即二维表的表头部分(表格的描述部分)

  如：关系名(属性名1，属性名2，…，属性名n)

  带下画线的属性为码属性，斜体的属性为外码属性 

  要求：关系必须是规范化：

  - 关系的每一个元组必须是可分区的，即存在码属性 
  - 关系的每一个属性(即元组的分量)必须是一个不可分的数据项，即不允许表中有表 

  完整性约束：实体完整性、参照完整性和用户自定义完整性 
  
  <u>助记：自实参(自(己)食铲)</u>



### 数据抽象与数据库的三级模式

隐藏关于数据存储和维护的某些细节，为用户提供数据在不同层次上的视图，即数据抽象

- 物理层抽象：最低层次的抽象，描述数据实际上是怎样存储的
- 逻辑层抽象：描述数据库中存储什么数据以及这些数据之间存在什么关联。数据库管理员和数据库应用开发人员使用
- 视图层抽象：只描述整个数据库的某个部分，即局部逻辑结构；同一数据库提供多个视图，每一个视图对应一个具体的应用，亦称为应用视图

提供观察数据库的3个不同角度。这就是数据库的三级模式结构，是DBMS的体系结构：(**三级模式结构**)

- 内模式：存储模式。对应于物理层数据抽象，它是数据的物理结构和存储方式的描述，是数据在数据库内部的表示方式
- 模式：逻辑模式。对应于逻辑层数据抽象，是数据库中全体数据的逻辑结构和特征的描述，是所有用户的公共数据视图。模式的一个具体值称为模式的一个实例(instance)。是DBMS模式结构的中间层
- 外模式：用户模式，子模式。对应视图层数据抽象。是数据库用户能够看见和使用的局部数据的逻辑结构和特征的描述，是数据库用户的数据视图，是与某一具体应用有关的数据的逻辑表示。外模式是保证数据库安全性的一个有力措施

目的：

1. 隐藏数据的存储和维护的细节，为用户提供数据在不同层次上的视图，方便不同的使用者可以从不同的角度去观察和利用数据库中的数据
2. 支持数据独立性的实现
3. 提供全局逻辑视图(模式)：支持整体结构化，从而实现数据共享度高、冗余度低、易扩充
4. 部分支持安全性的实现



**两层映像**功能：(**数据库独立性**)

- 模式/内模式映像。数据库中只有一个模式，也只有一个内模式，模式/内模式映像是唯一的，它定义了数据全局逻辑结构与存储结构之间的对应关系。在模式描述中定义模式/内模式映像。保证了数据与应用程序的物理独立性，简称为数据的物理独立性 
- 外模式/模式映像。对应于一个模式可以有多个外模式。对于每一个外模式，数据库管理系统都有一个模式/外模式映像，它定义了该外模式与模式之间的对应关系。在各自的外模式描述中定义外模式/模式映像。保证了数据与应用程序的逻辑独立性，简称为数据的逻辑独立性

模式即全局逻辑结构是数据库的核心和关键。设计数据库模式结构时，应首先确定数据库的逻辑模式

> 要保证数据库的数据独立性，应该修改的是两种映射



### 数据库系统

数据库系统(database system, DBS)，是指在计算机系统中引入数据库后的系统，一般由数据库、数据库管理系统(及其应用开发工具)、应用系统、数据库管理员和最终用户构成(**组成**)

<u>助记：系库应管用(西裤应(该)管用)</u>

数据库管理员(database administrator, DBA)，是指数据库的建立、使用和维护等的工作人员

- 决定数据库中的信息内容和结构
- 决定数据库的存储结构和存取策略
- 定义数据的安全性要求和完整性约束条件
- 监控数据库的使用和运行
- 数据库的改进和重组重构

在不引起混淆的情况下，常常把数据库系统简称为数据库，数据库管理系统简称为数据库系统 



数据库管理系统(DBMS)是一组软件，负责数据库的访问、管理和控制。功能：

- 数据定义：DBMS提供数据定义语言(DDL)
- 数据组织、存储和管理：DBMS要分类组织、存储和管理各种数据，包括数据字典、用户数据、数据的存取路径等 
- 数据操纵：DBMS还提供数据操纵语言(DML) 
- 数据库的事务管理和运行管理：数据库在建立、运行和维护时由DBMS统一管理、统一控制，以保证数据的安全性、完整性(一致性)，以及多用户对数据并发操作时的数据库正确性(称为并发控制)和系统发生故障后的数据库正确性(称为恢复与备份) 
- 数据库的建立和维护
- 其他功能 

组成：

- 查询处理器：对用户请求的SQL操作进行查询优化，从而找到一个最优的执行策略，然后向存储管理器发出命令，使其执行
- 存储管理器：根据执行策略，从数据库中获取相应的数据,或更新数据库中相应的数据
- 事务管理器：负责保证系统的完整性，保证多个同时运行的事务不发生冲突操作，以及保证当系统发生故障时数据不会丢失。

![image-20220531150428769](img/image-20220531150428769.png)





## 关系模型与关系代数

### 关系模型

> 关系模型的数据结构是二维表，亦称为关系
>
> 关系数据库是表的集合，即关系的集合
>
> 表是一个实体集，一行就是一个实体，它由有关联的若干属性的值所构成
>
> 关系模型中，现实世界的实体以及实体间的各种联系都是用关系来表示
>
> 基本特征有：与行的次序、列的次序无关

**关系数据结构**的形式化定义：(**关系模型的相关概念**)

域：是一组具有相同数据类型的值的集合($D$)

笛卡尔积：$D_1\times \cdots D_n=\{(d_1,\cdots ,d_n)|d_i\in D_i,i=1,\cdots n\}$

给定一组域 $D_i$，集合中每一个元素 $(d_1,\cdots ,d_n)$ 是 $n$ 元组，简称元组，每一个值 $d_i$ 是一个分量。$D_i$ 为有限集且基数为 $m_i$，则笛卡尔积基数为 $\prod_{i=1}^nm_i$。

关系：一组域的笛卡尔积的有限子集是该组域上的关系，即笛卡尔积的有限子集，表示为 $r(D_1,\cdots, D_n)$。$r$ 是关系名，$n$ 是关系的目或度(degree)。$n=1$ 单元关系，$n=2$ 二元关系。 表的一行对应关系一个元祖，一列对应一个域。列名是属性（因不同列的域可以相同）。$n$ 目关系共有 $n$ 个属性

关系的要求：

- 关系中的每个属性的域必须是原子的，即域中的每个值都是不可再分的一个完整单元。
- 关系中的每个元组都是可区分的，即存在唯一标识不同元组的属性(集)——码

空值(null)是所有可能的域的一个取值。

关系模式(relation schema)是关系的描述。表示为 $r(U,D,DOM,F)$，$U$ 是属性名集合，$DOM$ 是属性向域的映像集合，$F$ 是属性间数据依赖关系集合。简记 $r(U)$ 或 $r(A_1,\cdots,A_n)$，后者即属性名集合。 表头部分对应关系模式，是型。表体部分对应关系，是元祖集合，实例，是值。

> 人们经常把关系数据库模式和关系数据库实例都笼统地称为关系数据库，即型+值。

超码(super key)：关系 $r$ 的一个或多个属性的集合 $A$ 满足可以唯一地标识关系 $r$ 中的一个元组

候选码(candidate key)：关系 $r$ 的超码 $A$，满足任意真子集不是超码

主码：人为选定的若干候选码的一个

外码(foreign key)：$F$ 是关系 $r$ 的一个属性(集)满足对 $K_s$ 是关系 $s$ 的主码，$F$ 与 $K_s$ 相对应(关系 $r$ 中属性 $F$ 取值范围对应 $s$ 中 $K_s$ 取值范围的子集)，那么 $F$ 是关系 $r$ 参照关系 $s$ 的外码，简称 $F$ 是关系 $r$ 的外码



**关系完整性约束**条件：(**完整性规则**)

- 实体完整性：主码每个属性都非空
- 参照完整性：外码要么空，要么为对应的主码值
- 用户自定义完整性。如取值范围，取值间逻辑关系，唯一性等。



数据库模式导航图：一个含有主码和外码依赖的数据库模式的图例

![image-20220531152359187](img/image-20220531152359187.png)

- 下划线：主码
- 斜体：外码



**关系操作**特点：集合为单位。非关系数据模型操作方式：记录为单位。分为查询（选择select，投影project，连接join，除divide，并union，交intersection，差except，笛卡尔积，更名等）和更新（增删改）。查询是最主要操作。

关系操作表示方式：代数方式、逻辑方式。

- 关系代数是用代数方式表达的关系查询语言。
- 关系演算是用逻辑方式表达的关系查询语言。

对于关系代数、关系演算均是抽象的查询语言，在表达能力上是完全等价的。



### 关系代数

传统集合运算，(前三者)要求具有相同数量属性，域对应相同，即两关系的模式/结构相同。

- 并运算 $r\cup s=\{t|t\in r\vee t\in s\}$ 
- 差 $r-s=\{t|t\in r\wedge t\notin s\}$
- 交 $r\cap s=\{t|t\in r\wedge t\in s\}=r-(r-s)$ 
- 笛卡尔积 $r\times s=\{t_r\cdot t_s|t_r\in r,t_s\in s\}$ 得到 $n+m$ 目元组

关系运算：(**基本关系代数运算**)

选择：$\sigma_P(r)=\{t|t\in r\wedge P(t)\}$，$P$ 是谓词，逻辑表达式

投影：$\prod_A(r)=\{t[A]|t\in r\}$，选出若干列组成子关系

连接：$r\bowtie_\theta s=\{t_r\cdot t_s|t_t\in r\wedge t_s\in s\wedge(r.A\ op\ s.B)\}$，记作 $A\ op\ B$，两者为属性个数相等且可比的连接属性集，$op$ 是谓词，等效 $\sigma_\theta(r\times s)$。

$\theta$ 为等值比较谓词时，称为等值连接(equijoin)。自然连接(natural join)是特殊等值连接，要求具有公共属性集，并进行去重。

除：$r\div s=\{t_r[R-S]|t_r\in r\wedge s\subseteq S_x\}$，是 $R-S$ 上投影，条件为 $r$ 中 $R-S$ 取值象集包含 $s$。也就是先找到 $r$ 中元组，按 $R-S$ 分组，使得每组能取遍属性集 $R\wedge S$ 的所有取值。

![image-20220627011719759](img/image-20220627011719759.png)

> 如：
> $$
> (\prod_{stuNo,cNo}(Score))\div(\prod_{cNo}(\sigma_{name=语文 \vee name=数学}(Course)))
> $$

> 构造关系代数表达式的步骤：
>
> - 明确该查询涉及到哪些属性
> - 明确该查询涉及到哪些关系
> - 根据数据库模式导航图，通过多对一联系(或一对多联系)把所有涉及的关系连接起来，每一个多对一联系(或一对多联系)都可以表示为外码属性的自然连接

五种基本关系代数运算：$\cup,-,\times,\sigma,\pi$。(和更名)

其他运算：$\cap,\bowtie,\div,=$(赋值)。



## SQL查询语句

### SQL概述

包括：

- 数据操纵语言 **DML**： select, insert, update, delete (M: manipulation)

- 数据定义语言 **DDL**： create, drop, alter (D: definition)

- 数据控制语言 DCL： grant, revoke (C: control)

- 其他（嵌入式和动态，重新组织，备份，恢复）

**SQL特点**：

- 综合统一 (DDL,DML,DCL于一体)
- 高度非过程化
- 面向集合的操作方式
- 同一种语法结构提供两种使用方式 (自含式、嵌入式)
- 语言简洁，易学易用 (动词少)

<u>助记：统过两简集(捅过两碱基/剑姬/只鸡)</u>

非过程化。

SQL语言支持三级模式结构，外模式对应视图和部分基本表，模式对应基本表，内模式对应存储文件 

基本表：数据库中独立存在的表称为基本表，在SQL中一个关系对应一个基本表，一个(或多个)基本表对应一个存储文件，一个表可以带若干索引，索引存放在存储文件中

视图：指从一个或几个基本表(或视图)导出的表，是虚表，只存放视图的定义而不存放对应数据

查询表：指查询结果对应的表

存储文件：指数据库中存放关系的物理文件



**单表查询、连接查询、嵌套子查询**都是重点

单表查询：(各子句用法)

1. 投影运算：查询列、消除重复元组、别名、计算列
2. 选择运算：比较运算、范围查询、集合查询、空值查询、字符匹配、逻辑查询
3. 排序运算、聚合查询

连接查询：等值/非等值连接

嵌套子查询：IN/NOT IN，比较运算符(=ANY,!ALL等价于IN, NOT IN)

### 单表查询

> SQL Server函数：`lower,getdate,year,month` 等

>  举例：年龄大于或等于19岁 (==da:周岁==)
>
> ```sql
> SELECT studentNo, studentName, birthday
> FROM Student
> WHERE year(getdate()) - year(birthday)>=19
> ```
>
> 不含 `福` ：
>
> ```mysql
> SELECT studentNo, studentName
> FROM Student WHERE studentName NOT LIKE '%福%'
> ```
>
> with 的使用：
>
> ```sql
> with a as(
> SELECT * 
> FROM Student 
> WHERE sex='女')
> SELECT studentNo, studentName, convert(varchar(10),birthday, 111) as birthday
>    FROM  a
>    WHERE year(birthday)=1999 # convert 是 SQL Server
> ```



### 连接查询

> 同时选修两门课：
>
> ```mysql
> SELECT a.studentNo, studentName, b.courseNo, b.score, c.courseNo, c.score
> FROM Student a, Score b, 
> (SELECT * FROM Score WHERE courseNo='002') c
> WHERE b.courseNo='001' 
>       AND a.studentNo=b.studentNo # 表a与表b的连接条件
>       AND a.studentNo=c.studentNo # 表a与表c的连接条件
> ```
>
> ```sql
> SELECT a.studentNo, studentName, b.courseNo, b.score, c.courseNo, c.score
>      FROM Student a, Score b, Score c
>      WHERE a.studentNo=b.studentNo # 表a与表b的连接条件
>        AND a.studentNo=c.studentNo # 表a与表c的连接条件
>            AND b.courseNo='001' # 表b上的选择条件
>            AND c.courseNo='002' # 表c上的选择条件
>      ORDER BY a.studentNo
> ```
>
> 
>
> SQL不直接支持自然连接，完成自然连接的方法是在等值连接的基础上消除重复列
>
> 若某个表与自己进行连接，称为自表连接 
>



### 嵌套子查询

> 常用：
>
> - 元素与集合间的属于关系
> - 集合之间的包含和相等关系
> - 集合的存在关系
> - 元素与集合元素之间的比较关系

> 子查询中，不允许使用ORDER BY子句，该子句仅用于最后结果排序
>
> 分为非相关子查询，相关子查询。
>
> 选修过包含`系统`二字的课程的学生信息：
>
> ```mysql
> SELECT studentNo, studentName, classNo
> FROM Student
> WHERE studentNo IN 
>               ( SELECT studentNo FROM Score
>                WHERE courseNo IN 
>                ( SELECT courseNo FROM Course
>                 WHERE courseName LIKE '%系统%' ))
> ```
>
> ```mysql
> SELECT studentNo, studentName, classNo
> FROM Student
> WHERE studentNo IN 
>               ( SELECT studentNo 
>                FROM Score b, Course c
>                WHERE b.courseNo=c.courseNo 
>                AND courseName LIKE '%系统%' )
> ```
>
> ```mysql
> SELECT DISTINCT a.studentNo, studentName, classNo
> FROM Student a, Score b, Course c
> WHERE a.studentNo=b.studentNo
>     AND b.courseNo=c.courseNo 
>     AND courseName LIKE '%系统%' 
> ```
>
> 并不是每一个IN子查询都可以转化为连接运算来实现
>
> 
>
> 除法运算，同时选修两门课的人选的所有课：(也可以用 in 三层)
>
> ```mysql
> SELECT  a.studentNo, studentName, courseName, score
> FROM Student a, Course b, Score c
> WHERE  a.studentNo=c.studentNo AND b.courseNo=c.courseNo
>    AND   a.studentNo IN 
>     ( SELECT studentNo FROM Score x, Course y
>                 WHERE x.courseNo=y.courseNo 
>                 AND courseName='计算机原理' )
>    AND   a.studentNo IN 
>      ( SELECT studentNo FROM Score x, Course y
>                 WHERE x.courseNo=y.courseNo 
>                 AND courseName='高等数学' )
> ORDER BY a.studentNo, score DESC
> ```
>
> 
>
> 查询最高分：
>
> ```mysql
> SELECT studentNo, courseNo, score
> FROM Score
> WHERE score=( SELECT max(score) FROM Score )
> ```
>
> 年龄小于班上某个人(即出生日期晚于)
>
> ```mysql
> SELECT studentNo, studentName, year(getdate())-year(birthday) AS age
> FROM Student
> WHERE birthday>ANY
> ( 	SELECT birthday
> 	FROM Student a, Class b
> 	WHERE className='计算机科学与技术16-01班' 
> 	AND a.classNo=b.classNo )
> ```
>
> 
>
> exists是逻辑表达式，若子句非空条件为真
>
> exists 的使用：(将元组扔到 exists 子句里判断)(选修了课程的人)
>
> ```mysql
> SELECT studentName, classNo 
> FROM Student x
> WHERE EXISTS 
>         ( SELECT * FROM Score a, Course b
>          WHERE a.courseNo=b.courseNo 
>          AND a.studentNo=x.studentNo 
>          AND courseName='计算机原理' )
> ```
>
> 
>
> 全称量词转存在量词：
> $$
> (\forall c)P(x,c)\Leftrightarrow\lnot(\exists c(\lnot P(x,c)))
> $$
> 如选修了所有课程的学生：
>
> ```mysql
> SELECT studentName 
> FROM Student x
> WHERE NOT EXISTS 
>   ( SELECT * FROM Course c
>    WHERE NOT EXISTS   
>    --判断学生x.studentNo没有选修课程c.courseNo
>        ( SELECT * FROM Score
>         WHERE studentNo=x.studentNo 
>         AND courseNo=c.courseNo )
>   )
> ```
>
> 解析：
>
> - 整体逻辑：先找一个集合，集合里的人没选修全部课程，然后找出不属于这个集合的人；对于这个集合，等价于这些人不在选修关系里
> - 如果一个人选修了所有课程，那么每门课它都修了即最内层select结果非空，所以exists(非空)为真，not exists为假，那么中间层select没有结果，是空集，那么最外层成立
> - 如果一个人有一门课没选修，那么这门课not exists为真，中间层不空，所以not exists为假
>
> 如：至少选修了特定学号学生所选修的所有课程的学生：
>
> 即：不存在一门课程，特定学号学生选修了，且该学生没选修
>
> ```mysql
> SELECT studentName
> FROM Student x
> WHERE NOT EXISTS 
>     ( SELECT * FROM Score y      // 不能用Course表
>      WHERE studentNo='1600002'  
>      --查询学生'1600002'所选修课程的情况
>      AND NOT EXISTS    
>      --判断学生x.studentNo没有选修课程y.courseNo
>          ( SELECT * FROM Score
>           WHERE studentNo=x.studentNo 
>           AND courseNo=y.courseNo )
>     )
> ```
>
> 至少选修了学号为1600002学生所选修的所有课程的学生学号、姓名以及该学生所选修的1600002学生选修过的所有课程的课程名和成绩：
>
> ```mysql
> SELECT  x.studentNo,  studentName,  courseName,  score 
> FROM  Student x, Course y, Score z
> WHERE  x.studentNo=z.studentNo AND y.courseNo=z.courseNo 
>       AND   NOT EXISTS
>    ( SELECT * FROM Score b
>     WHERE studentNo='1600002'   --查询学生'1600002'所选修课程的情况
>     AND NOT EXISTS    --判断学生x.studentNo没有选修课程b.courseNo
>         ( SELECT * FROM Score
>          WHERE studentNo=x.studentNo AND courseNo=b.courseNo )
>    )
>     AND  y.courseNo IN ( SELECT courseNo FROM Score WHERE studentNo='1600002')
> ```
>
> 
>
> 至少拿了 28 学分的人：
>
> ```mysql
> SELECT a.studentNo, studentName, courseName, score, creditHour
> FROM Student a, Course b, 
>     ( SELECT studentNo, courseNo, max(score) score
>      FROM Score
>      WHERE score>=60    -- 仅列示已经获得学分(即及格了)的课程
>      GROUP BY studentNo, courseNo ) AS c      -- 查询表c
> WHERE a.studentNo=c.studentNo AND c.courseNo=b.courseNo 
>        AND a.studentNo IN 
>    ( SELECT studentNo     -- 子查询Q
>      FROM Course x, 
>     ( SELECT studentNo, courseNo, max(score) score
>      FROM Score
>      WHERE score>=60    -- 只有及格才能获得学分
>      GROUP BY studentNo, courseNo ) AS y
>     WHERE y.courseNo=x.courseNo
>     GROUP BY studentNo
>     HAVING sum(creditHour)>=28 )
> ORDER BY a.studentNo
> ```
>
> 
>
> 查询至少选修了5门课程且课程平均分最高的同学的学号和课程平均分。如果一个学生选修同一门课程多次，则选取最高成绩：
>
> ```mysql
> SELECT studentNo, avg(score) avgScore
> FROM  ( SELECT studentNo, courseNo, max(score) score
>        FROM Score
>        GROUP BY studentNo, courseNo ) AS a
> GROUP BY studentNo
> HAVING count(*)>=5 
>  AND avg(score)=
>     ( SELECT max(avgScore)    -- 子查询Q2
>      FROM ( SELECT studentNo, avg(score) avgScore    -- 子查询Q1
>            FROM  ( SELECT studentNo, courseNo, max(score) score
>                   FROM Score
>                   GROUP BY studentNo, courseNo ) AS b
>            GROUP BY studentNo
>            HAVING count(*)>=5 ) AS x)
> ```
>
> 非相关子查询指子查询的结果不依赖于上层查询
> 相关子查询指当上层查询的元组发生变化时，其子查询必须重新执行
>
> 
>
> 查询平均分最高的课程：
>
> ```mysql
> SELECT a.courseNo, courseName, avg(score) 最高平均分
> FROM Course a, Score b
> WHERE a.courseNo=b.courseNo
> GROUP BY a.courseNo, courseName
> HAVING avg(score)=
>    ( SELECT max(avgScore)
>     FROM ( SELECT avg(score) avgScore
>           FROM Score
>           GROUP BY courseNo ) x)
> ```
>
> 
>
> 集合运算 union(并), intersect(交), except(减)可以用其他语句改写
>
> order by表达式可以是列名或表达式

SQL 语句一般格式：

```mysql
SELECT [ALL | DISTINCT] <目标列表达式> [AS] [<别名>]
                 [, <目标列表达式> [AS] [<别名>] ... ]
FROM {<表名> | <视图名> | <查询表>} [AS] [<别名>]
             [, {<表名> | <视图名> | <查询表>} [AS] [<别名>] ... ]
[ WHERE <条件表达式> ]
[ GROUP BY <列名1> [, <列名2> ... ]
   [ HAVING <条件表达式> ] ]
[ ORDER BY <列名表达式> [ASC | DESC] 
                [, <列名表达式> [ASC | DESC] ... ] ]
```



## 数据库建模

### 数据库设计过程

**全节重点**

数据库设计过程概述：

数据库设计就是根据各种应用处理的要求、硬件环境及操作系统的特性等，将现实世界中的数据进行合理组织，并利用已有的数据库管理系统(DBMS)来建立数据库系统的过程。
具体地说，对于一个给定的应用环境，构造出最优的数据库逻辑模式和物理模式，并建立数据库及其应用系统，使之能够有效地存储和管理数据，满足用户的信息要求和处理要求。 

```mermaid
graph LR
	a[需求分析]-->b[概念设计]
	b-->c[逻辑设计]
	c-->d[模式求精]
	d-->e[物理设计]
	e-->f[应用与安全统计]
```

- 需求分析：基础，最困难和耗时

  功能需求、数据需求、数据使用业务规则、数据联系及约束、性能需求等；分析用户行为、需求分析、建立数据流图

- 概念设计：得到概念模型(E-R图)(E-R模型是语义模型，能映射到数据库概念模型)(使用到的抽象方法：聚合、概括、分类；不包括合并)(概念模型独立于硬件设备和DBMS)

- 逻辑设计：逻辑数据模型(数据库(关系)模式) 考虑概念模式、处理要求、DBMS特性，不考虑存取方式

- 模式求精：分析并发现数据库逻辑模式存在的问题，并进行改进和优化(减少数据冗余，消除更新、插入与删除异常)(基于关系理论)

- 物理结构：数据库文件组织格式、文件内部存储结构、索引、表的聚集；关注存取方式、索引与入口设计、安全性完整性一致性问题

- 应用与安全设计：访问权限(角色)，存储过程

E-R图合并可能会出现属性、结构和命名冲突(不会出现语法冲突)、

实施阶段的工作：建立数据库、加载数据、系统调试(无扩充功能)



### E-R模型基本概念及表示

**全节重点**

实体是客观世界中可区别于其他事物的“事物”或“对象”，特征：独立存在、可区别于其他实体。可以是有形、实在或抽象、概念(课程、单据)

实体集是具有相同类型即相同性质(或属性)的实体组成的集合

属性是实体集中每个实体都具有的特征描述。实体集具有相同属性，有属性值和属性的域

- 简单属性：不能分为更小部分
- 复合属性：可以进一步划分为更小部分 (如住址拆成省市街道)
- 单值属性：特定实体任何时候只能取一个值
- 多值属性：0到任意多个(如电话号码) (转为弱实体集/多个单值)
- 派生属性：值可以从其它相关属性或实体(集)派生出来 (如年龄)
- 空值NULL：在某个属性没有值，表示未知/不存在

联系：多个实体相互关联

联系集：同类联系的集合。是 $n(n\ge 2)$ 个实体集 $E$ 上的数学关系，实体集不必互异。联系集是 $\{(e_1,\cdots,e_n)|e_i\in E_i\}$ 的子集。可具有自身的描述属性

- 度：参与联系集的实体集的数目。划分为 $x$ 元联系集

多联系：在相同的实体集上可能存在多个不同的联系集

多值联系：是指在同一个给定的联系集中，相关联的相同实体之间可能存在多个联系 (如客户与银行可能发生多次贷款)

角色：实体在联系中的作用



### 约束

主要有：

1. **映射约束**：

   - 实体集A中的一个实体通过某联系集R能与实体集B中的实体相联系的数目，称为实体集A到实体集B之间的联系集R的映射基数(mapping cardinality)，简称为联系集R的映射基数

     共有一对一，一对多和多对多三种映射基数

   - 一对一: A中的一个实体至多(允许不)同B中的一个实体相联系，B中的一个实体也至多(允许不)同A中的一个实体相联系。如发票与销货单(可以存在实体与另一方无联系(即如不开发票))

   - 一对多:A中一个实体可以同B中任意数目(可以为0)的实体相联系，而B中一个实体至多(允许不)同A中一个实体相联系。如班级和学生一对多(学生可以不加入班级，也可以有空班)

   - 多对多:A中的一个实体可以同B中任意数目(可以为0)的实体相联系，而B中的一个实体也可以同A中任意数目(可以为0)的实体相联系。如学生与选课

2. **码约束**：

   - 超码：一个或多个属性的集合，且这些属性的集合在一实体集中能够唯一标识一个实体。超码的任意超集是超码
   - 候选码和超码是实体集客观存在的特性，它们表示实体集中任意两个实体不允许在码属性上有相同的值
   - 主码选择原则：
     选择属性长度最短的候选码；
     选择包含单个属性的码，而不是复合候选码；
     选择在数据库系统生命周期内属性值最少变化的候选码；
     选择在数据库系统生命周期内更可能包含唯一值的候选码
   - 一个联系集中的每一个联系也要求是可区分的，也必须存在候选码，并选择其中的一个候选码作为主码，用于唯一标识该联系集中的一个联系。联系集的超码是与它联系的所有实体集主码的并
   - 二元联系集的主码选择，依赖于联系集的映射基数：
     - 一对一：主码可以使用参与联系集中的任何一方实体集的主码
     - 一对多(多对一)：主码由“多”的一方实体集的主码组成
     - 多对多：主码由参与联系集中所有实体集的主码组成
   - 二元联系集**联系属性安置**：
     - 一对一联系集的属性：安置于联系集或任一边的实体集上
     - 一对多联系集的属性：安置于联系集或多的一方实体集上
     - 多对多联系集的属性：它是描述相关联实体集间的交互性语义，因此，联系属性只能安置于联系集上，不能放到相关联的实体集上去

3. **依赖约束**：

   - 指联系中一种实体的存在依赖于该联系集中联系或其他实体集中实体的存在
     - 联系中一种实体的存在依赖于该联系集中联系的存在，称为实体集与联系集之间的依赖约束，并将依赖于联系集而存在的实体集称为**依赖实体集**
     - 联系中一种实体的存在依赖于其他实体集中实体的存在，称为实体集之间的依赖约束，并将依赖于其他实体集而存在的实体集称为**弱实体集**

4. 参与约束：

   - 如果实体集A中的每个实体都参与到联系集R中至少一个联系中，则称实体集A全部参与联系集R
   - 如果实体集A中只有部分实体参与到联系集R的联系中，则称实体集A部分参与联系集R



### 弱实体集

**全节重点**

与之相对是强实体集

弱实体集所依赖的强实体集称为标识实体集 (identifying entity set)

弱实体集必须与一标识实体集相关联才有意义，该联系集称为标识联系集 (identifying relationship set)

一个弱实体集中用来标识弱实体的属性(集) 称为该弱实体集的部分码 (partial key)

弱实体集中的实体是由其标识实体集中的主码与其部分码共同标识

限制：

- 标识实体集和弱实体集必须是“一对多”联系集 
- 弱实体在标识联系集中是全部参与

应用：

1. 多值属性建模为弱实体集

   (依赖约束和多值联系可能会导致将联系集建模为依赖实体集或弱实体集)

2. 语义上存在包含关系的实体 (如学院与系)

   (没有必要在系统全局范围内唯一标识的实体集可建模为弱实体集)

弱实体集可以同时依赖于多个标识实体集



### 扩展E-R特征

使用实体集的继承和ISA联系(超类/子类联系)描述子集。子类会继承父类的所有属性

联系集之间的联系：使用聚合

聚合是一种抽象，它将一个联系集及其相关联的实体集抽象为一个高层实体集(称为联系实体集)对待，然后建立该高层实体集与其它实体集之间的联系集

![image-20220625211125740](img\image-20220625211125740.png)



### E-R建模问题

基本原则：[(属性冲突、命名冲突、结构冲突)]

1. 忠实性 (首要，最重要) (忠于需求)
2. 简单性 (不要添加非必要成分)
3. 避免冗余 (一个对象只存放在一个地方)

通常满足下述两条规则，均可作为属性对待：

- 作为属性，不能再具有要描述的性质；
- 属性不能和其它实体相联系

注意：

- 不能将实体集主码表示为另一实体集属性 (要使用联系)
- 不能将相关实体集的主码属性作为联系集属性 (已经隐含了)

一事物是描述为实体集还是联系集并没有一个绝对的标准。通常原则：

- 实体对应于现实世界中实际存在的事物，是名词
- 联系对应的概念一般为一种动作，即描述实体间的一种行为

三元联系转二元联系：

- 通过聚合将二元联系集建模成一个联系实体集，再加上它与原来联系的实体集之间的二元联系，如图(b)所示
- 或者建立一个依赖实体集或弱实体集，再与原实体集之间建立二元联系，如图(c)、图(d)所示

![image-20220625211620612](img\image-20220625211620612.png)

下图 b,c 是错误示范(语义不对)，下下图是对的

![image-20220625211741675](img\image-20220625211741675.png)

![image-20220625211938484](img/image-20220625211938484.png)

依赖约束：依赖于联系集而存在的实体集一般是指伴随着业务发生而形成的单据。如员工、客户、商品之间发生销售/购买商品等业务时，伴随着会产生销货单/购货单。
在E-R建模时，一般将依赖于业务的发生而产生的销货单/购货单等直接建模为依赖实体集(而不是联系集)，并将它直接与所依赖的联系集关联起来

> 类似的业务有：
> 领料员/采购员、仓库保管员、材料之间发生的出库/入库业务会伴随着产生出库单/入库单；
> 读者、图书管理员、图书之间发生的借书业务会伴随着产生借书单；
> 客户、员工、现金之间发生的存款/取款业务会伴随着产生存款单/取款单;
> 病人、医生、药品之间发生的诊断业务会伴随着产生病历记录-处方单；
> 旅客、员工、客房之间发生的入住业务会伴随着产生入住单；
> 司机、警察、违章处罚目录之间发生的违章处罚业务会伴随着产生违章处罚单；
> 员工、游客、景点之间发生的旅游业务会伴随着产生旅游安排单；
> 公交车、车站之间发生的运行安排业务会伴随着产生公交线路。 



概念设计任务：

- 定义实体集及属性，实体集的主码，并用数据字典描述实体集；
- 定义联系集及属性，联系集的主码，联系的映射基数及参与约束,联系中实体的角色，并用E-R图描述被建模的联系集；
- 分析初步E-R图中是否存在依赖约束、多值联系，并将其建模为依赖实体集或弱实体集；
- 利用扩展E-R特征对对象进行分类及聚合(建模为联系实体集) ；
- 将多元联系转化为二元联系进行建模(联系实体集、依赖实体集或弱实体集)；
- 去除冗余数据，并保证满足所有数据需求不冲突；
- 对照需求分析规格说明书，检查E-R模型，看其是否包含了所有数据、能否满足所有功能需求等



### E-R图图例

E-R图图例：

- 实体集：矩形
- 属性：椭圆 (主码：下划线)
- 多值属性：双椭圆
- 派生属性：虚线椭圆
- 属性与实体之间：连线
- 联系：菱形
- 联系与实体之间：连线 (箭头指向一，横线指向多，双横线是全部参与)
- 依赖实体集：带填充背景的矩形 
- 依赖实体集所依赖的联系集：带填充背景的菱形
- 弱实体集：双矩形
- 标识实体集：双菱形
- 部分码：虚下划线
- ISA联系：三角形
- 联系实体集：内部包含无填充背景虚线菱形的填充背景矩形
- 聚合：虚线矩形

![image-20220625202842323](D:\_lr580\大学\数据库\img\image-20220625202842323.png)



### 逻辑设计

**全节重点**

逻辑模型(数据库模式，关系模式的集合)

- 强实体集：直接转

- 弱实体集：用外码加部分码当主码

- 联系集：

  - 一对多：多方增加一方的主码作为外码和联系属性(如公司与员工的聘用，把公司名和聘期放到员工去)
  - 一对一：把任意一方看成多，同上处理
  - 多对多：按映射基数确定主码，外码是各实体集主码并

- 标识联系集：不需要转化

- 复合属性：拆成多个简单属性

- 多值属性：创建关系模型(建模为弱实体集)，属性为所在实体集(/联系集)主码(是外码)和本来多值属性对应的属性

- 类层次：

  - 父类是公共属性，子类是特殊属性和父类主码，子类主码与父类主码相同
  - 只转换子类，删掉父类

- 聚合：

  - 内层联系集、外层联系集都是按其映射基数决定是否需要单独转化为一个独立的关系模式(多对多联系集才需要)
  - 外层联系集的主码根据映射基数不同分别由内层联系集(即联系实体集)的主码、外层实体集的主码按不同方式产生

  说人话就对被联系的练习集，跳过聚合这玩意本身，就是拆掉聚合，用它对应的实体集去做，如：

  ![image-20220625214734892](img/image-20220625214734892.png)

  ![image-20220625214909078](img/image-20220625214909078.png)

  ![image-20220625214922154](img/image-20220625214922154.png)



## 关系数据理论与模式求精

### 问题提出

数据冗余是指同一信息在数据库中存储了多个副本。它可能引起下列问题：

- 冗余存储：信息被重复存储，导致浪费大量存储空间
- 更新异常：当重复信息的一个副本被修改，所有副本都必须进行同样的修改。因此当更新数据时，系统要付出很大的代价来维护数据库的完整性，否则会面临数据不一致的危险
- 插入异常：只有当一些信息事先已经存放在数据库中时，另外一些信息才能存入数据库中
- 删除异常：删除某些信息时可能丢失其它信息

模式分解存在的问题：

- 有损分解：两个分解后的关系通过连接运算还原得到的信息与原来关系的信息不一致

  反之称为无损分解(lossless decomposition)

- 丢失依赖关系

  反之，如果被分解关系模式上的所有依赖关系都在分解得到的关系模式上保留，称该分解为依赖保持 (dependency preserving)分解

好的关系模式：

- 数据冗余尽可能少(即数据共享尽可能高)
- 不发生插入异常、删除异常、更新异常等问题。
- 模式分解时，分解后的模式应具有无损连接、保持依赖等特性

理论依据：关系规范化理论

候选关键字的属性可以有 1 个或多个。候选关键字可能有 1 个或多个，但主关键字只有一个

关系模式的任何属性不可再分



### 函数依赖定义

**全节重点**

函数依赖(functional dependency, 简称FD)是一种完整性约束,  是现实世界事物属性之间的一种制约关系

函数依赖定义：设 $r(R)$ 为关系模式，$\alpha\subseteq R,\beta\subseteq R$，对任意合法关系 $r$ 及其中任两个元组 $t_i,t_j(i\neq j)$，若 $t_i[\alpha]=t_j[\alpha]$ 则 $t_i[\beta]=t_j[\beta]$，则称 $\alpha$ 函数确定 $\beta$，或 $\beta$ 函数依赖于 $\alpha$，记作 $\alpha\to\beta$。

函数依赖那么 $\alpha,\beta$ 可能是多对一关系

函数依赖是语义范畴的，只能根据数据语义确定，不能被证明

码约束是函数依赖特例，码属性(集)是 $\alpha$，关系中所有属性是 $\beta$

非平凡函数依赖：$\alpha\to\beta,\beta\not\subseteq\alpha$；否则是平凡函数依赖($\beta\subseteq\alpha$)。对于任一关系模式，平凡函数依赖都是必然成立的，它不反映新的语义

完全函数依赖：对非平凡函数依赖，$\forall\gamma\subset\alpha,\gamma\to\beta$ 都不成立。简称函数依赖。否则是部分函数依赖，简称部分依赖

对完全函数依赖 $\alpha\neq \beta$，则 $\alpha\to\beta$ 一定是非平凡函数依赖

显然 $\alpha$ 是单属性时函数依赖一定是完全函数依赖

若 $\gamma\subseteq R,\alpha\to\beta,\beta\to\gamma$ 则存在函数依赖 $\alpha\to\gamma$，若 $\alpha\to\beta,\beta\to\gamma,\alpha\to\gamma$ 非平凡函数依赖，且 $\beta\not\to\alpha$，则 $\alpha\to\gamma$ 是传递函数依赖，简称传递依赖

因为是非平凡函数依赖，所以共四个条件：$\beta\not\subseteq\alpha,\gamma\not\subseteq\beta,\gamma\not\subseteq\alpha,\beta\not\to\alpha$

因为最后一个条件，所以 $\beta$ 一定不是候选码

传递依赖也可能会导致数据冗余及产生各种异常

函数依赖是指关系模式中属性之间存在的一种约束关系。这种约束关系既可以是现实世界事物或联系的属性之间客观存在的约束，也可以是数据库设计者根据应用需求或设计需要强加给数据的一种约束
但不论是哪种约束，一旦确定，进入数据库中的所有数据都必须严格遵守



### 范式

**全节重点**

第一范式 $\mathbf{1NF}$：关系模式每个属性值域都不可分(原子的)，则 $r(R)\in1NF$

目标：将基本数据划分成称为实体集或表的逻辑单元，当设计好每个实体后，需要为其指定主码

主属性：$\alpha\subseteq R,\alpha$ 包含在 $r(R)$ 的某个候选码中；否则是非主属性

第二范式 $\mathbf{2NF}$：$1NF$ 的所有非主属性完全函数依赖于 $r(R)$ 的候选码(不存在非主属性对候选码的部分依赖)

目标：将只部分依赖于候选码（即依赖于候选码的部分属性）的非主属性移到其他表中(全部是码)

第三范式 $\mathbf{3NF}$：$2NF$ 的所有非主属性直接函数依赖于 $r(R)$ 候选码(不存在非主属性传递依赖于候选码)

目标：去掉表中不直接依赖于候选码的非主属性(仅仅是码)

> 如：$r(R)=r(A,B,C),F=\{AB\to C,C\to A\}$，计算得候选码为 $AB$ 或 $BC$，故 $r(R)\in 3NF$；理由：都是主属性，没有非主属性

Boyce-Codd 范式 $\mathbf{BCNF}$：$1NF$ 的 $F^+$ 所有函数依赖 $\alpha\to\beta$ 要么是平凡函数依赖，要么 $\alpha$ 是一个超码(即包含 $r(R)$ 的候选码) (即 $F^+$ 的每个非平凡函数依赖的决定属性集 $\alpha$ 包含候选码)

BCNF 排除了：任何属性(包括主属性和非主属性)对候选码的部分依赖和传递依赖；主属性之间的传递依赖

> 如：$r(R)=r(A,B,C)$，则
>
> - $F=\{A\to B,B\to C\}$ 不是 $BCNF$
> - $F=\{AB\to C,C\to A\}$ 不是 $BCNF$
> - $F=\{AB\to C,BC\to A\}$ 是 $BCNF$

$BCNF\subset 3NF\sub 2NF\sub 1NF$

BCNF 与 3NF 的区别在于是否 $\beta-\alpha$ 的每个属性都是候选码的一部分(3NF 不要求都在一个候选码里，多个候选码时不一定都在)，即 3NF 允许存在主属性对候选码的传递依赖和部分依赖

若 $\beta-\alpha$ 每个属性都是候选码的一部分，那么 $\alpha$ 也一定包含候选码的一部分

> 满足BCNF范式的模式分解，可能不是保持依赖分解。3NF分解既是无损分解，又是保持依赖分解

3NF存在数据冗余和异常问题，而BCNF是基于函数依赖理论能够达到的最好关系模式(任何二元关系模式的最高范式)

总结：

- 2NF：不存在非主属性对候选码的部分依赖

- 3NF：不存在非主属性对候选码的传递依赖

  ​		允许存在主属性对候选码的部分和传递依赖

- BCNF：每个非平凡函数依赖的左部是超码 

  (不存在对候选码的部分依赖和传递依赖和主属性传递依赖)

任何一个关系模式都一定有码



### 函数依赖理论

给定函数依赖集 $F$，可以证明其他成立的函数依赖是被 $F$ 逻辑蕴涵的函数依赖。 

**函数依赖集闭包**：$F$ 逻辑蕴涵的所有函数依赖组成的集合 $F^+$

Armstrong 公理：

- 自反律(reflexivity rule)  $\beta\subseteq\alpha\Rightarrow\alpha\to\beta$ 
- 增补律(augmentation rule) $\alpha\to\beta\Rightarrow\gamma\alpha\to\gamma\beta$
- 传递律(transitivity rule) $\alpha\to\beta,\beta\to\gamma\Rightarrow\alpha\to\gamma$

推论：

- 合并律(union rule) $\alpha\to\beta,\alpha\to\gamma\Rightarrow\alpha\to\beta\gamma$
- 分解律(decomposition rule) $\alpha\to\beta\gamma\Rightarrow\alpha\to\beta,\alpha\to\gamma$
- 伪传递律(pseudo-transitivity rule) $\alpha\to\beta,\beta\gamma\to\delta\Rightarrow\alpha\gamma\to\delta$ 

**属性集闭包**：$A\subseteq R,F$ 下由 $A$ 函数确定的所有属性的集合为 $F$ 下属性集 $A$ 的闭包 $A^+$

作用：

- 判断 $\alpha\to\beta$ 是否在 $F^+$ 等价于 $\beta\subseteq\alpha_F^+$ 是否成立
- 判断 $\alpha$ 是否是超码，计算 $\alpha^+$ 是否包含 $R$ 所有属性
- 判断 $\alpha$ 是否是候选码，检验 $\alpha$ 所有子集闭包是否不存在包含 $R$ 所有属性的
- 求函数是否在 $F^+$ 。$\forall\gamma\subseteq R$，求 $\gamma^+$，若 $\forall S\subseteq\gamma^+$，可知 $\gamma\to S$

**候选码计算** 

1. 将属性分类：
   - $L$ 类：只在函数依赖左部出现
   - $R$ 类：只在函数依赖右部出现
   - $N$ 类：在函数依赖左右都未出现
   - $LR$ 类：在函数依赖左右都出现
2. 结论：
   - $L$ 类属性一定是任一候选码成员；若闭包为 $U$ 就是唯一候选码
   - $R$ 类属性不在任何候选码中
   - $N$ 类属性在任一候选码中
3. 步骤：
   1. 找出 $X=L\cup N$，若 $X^+=R$ 就是唯一候选码
   2. 否则，尝试增加一个属性(在 $LR$ 找)，增加后闭包全了就是一个候选码，这样的结果可能有零个或多个
   3. 在找到的之外，继续尝试增加两个、三个……属性，看看还有没有别的候选码

> 如：
>
> -  $r(R)=(A,B,C,D),F=\{B\to C,D\to A\}$，找到 $L=BD$ 为候选码一部分，且 $(BD)^+=R$，所以是唯一候选码
> - $r(R)=(A,B,C,D,E),F=\{A\to B,BC\to E,ED\to A\}$，$L=CD,(CD)^+=CD$；所以判断，发现 $(ACD)^+=(BCD)^+=(CDE)^+=R$，所以存在三个候选码；由于 $R-CD-ABE=\varnothing$，所以不用继续判断了
> - $r(R)=(A,B,C,D,E,G)$, $F=\{B\to ADE,A\to BE,AC\to G,BC\to D\}$；得 $L=C,C^+=C$，那么在 $AB$ 里抽单属性，发现 $(AC)^+=(BC)^+=R$，之后不用往下判断了

> 正则覆盖略

无损连接：$r_1(R_1),r_2(R_2)$ 子模式，对任意一个满足 $F$ 的关系实例 $r$ 都有 $\prod_{R_1(r)}\bowtie\prod_{R_2(r)}=r$，则该分解对于 $F$ 是无损连接的

**无损连接分解**当且仅当 $F^+$ 包含 $R_1\cap R_2\to R_1$ 或 $R_1\cap R_2\to R_2$，即 $F$ 下 $R_1\subseteq(R_1\cap R_2)^+$ 或  $R_2\subseteq(R_1\cap R_2)^+$。充要条件：公共属性包含 $r_1(R_1)$ 或 $r_2(R_2)$ 的码，即 $R_1\cap R_2$ 是 $r_1(R_1)$ 或 $r_2(R_2)$ 的超码

> 如：$r(R)=r(A,B,C,D,E)$, $F=\{A\to BC,CD\to E,B\to D,E\to A\}$ 
>
> - $r_1(R_1)=r_1(A,B,C),r_2(R_2)=r_2(A,D,E)$，由于 $R_1\cap R_2=A,A\to R_1$ 所以是
> - $r_1(R_1)=r_1(A,B,C),r_2(R_2)=r_2(C,D,E)$ 因为 $C\not\to R_1,C\not\to R_2$ 所以不是

若多个子关系模式，判断是否为无损连接分解

1. 逐步连接分解法：选择两个满足无损连接条件的子关系模式进行连接，不断重复上述操作，直到连成一个则是无损分解；如果剩下两个子模式不能无损连接就是有损

2. 表格判断法：

   1. 建立行为子关系模式，列为属性的表格，一开始哪个子关系模式有哪些属性就填到表上方格，填 $a_x$ 代表有第 $x$ 个属性
   2. 对 $F$ 每个函数依赖，当一个依赖在某一行左右都在表里时，对另外的所有行若左边所有属性在一个表格列都有，那么把右边那些属性也填到该表格对应列上
   3. 重复上述操作，若某一表格行填满了就是无损连接分解；若无论如何都填不满(遍历 $F$ 后表格不变)，那么有损

   ![image-20220626150927814](img/image-20220626150927814.png)

保持依赖分解：若 $F$ 在 $R_i$ 投影为 $F^+$ 中所有只包含 $R_i$ 属性依赖的集合 $F_i$ 满足 $\alpha\to\beta$ 在 $F_i$ 中，则 $\alpha,\beta$ 的所有属性都在 $R_i$ 中。

保持依赖分解：$(F_1\cup F_2\cup\cdots\cup F_n)^+=F^+$

> 例如 $r(R)=r(A,B,C),F=\{A\to B,B\to C\}$，则：
>
> - $r_1(R_1)=r_1(A,B),r_2(R_2)=r_2(B,C)$ 是
> - $r_1(R_1)=r_1(A,B),r_2(R_2)=r_2(A,C)$ 不是



### 模式分解算法

分解的基本原则：无损分解、保持依赖

数据库设计目标(基于函数依赖)：BCNF，无损连接，保持依赖

若不能同时达到，需要在 BCNF, 3NF 选一个

**BCNF 分解**：若 $\alpha\to\beta$ 的 $\alpha$ 不是超码，将 $r(R)$ 分解为：

- $R_1=\alpha\beta,F_1=\{\alpha\to\beta\}$，若 $\alpha\cap\beta=\varnothing$ 则 $\alpha$ 为候选码 
- $R_2=R-(\beta-\alpha)$ 

直到 $R_2$ 属于 $BCNF$ 为止，否则递归执行上述操作

> 如：
>
> - $r(R)=r(A,B,C),F=\{AB\to C,C\to A\}$，计算得候选码为 $AB$ 或 $BC$，因为 $C\to A$ 不符合 $BCNF$，故拆分为 $r_1(R_1)=r_1(A,C),r_2(R_2)=r_2(B,C)$。分解后丢失了函数依赖 $AB\to C$
> - $r(R)=(A,B,C,D,G,H)$, $F=\{A\to BC,DG\to H,D\to A\}$，求得候选码 $DG$，所以拆分为 $r_1(A,B,C),r_{21}(D,A),r_{22}(D,G,H)$ 
> - $r(R)=r(A,B,C,D,G,H)$, $F=\{AB\to GH,CD\to GH,B\to A,D\to B\}$，求得 $CD$ 候选码，拆分为 $r_1(A,B,G,H),r_{21}={BA},r_{221}={D,B},r_{222}={C,D}$ 

上述算法得到的分解不仅是BCNF分解，而且是无损分解（但可能不是保持函数依赖分解），用该算法生成的BCNF分解不是唯一的

> 3NF 分解算法：略



### 数据库模式求精

模式求精是运用关系理论(如函数依赖理论、多值依赖理论等)对已有关系模式进行结构调整、分解、合并和优化的过程，以满足应用系统的功能及性能等需求

模式分解。根据范式要求(是选择BCNF还是3NF)，运用规范化方法将关系模式分解成所要求的关系模式
模式合并。在分解过程中可能进行模式合并。如当查询经常涉及到多个关系模式的属性时，系统将经常进行连接操作，而连接运算的代价是相当高的。此时，可考虑将这几个关系合并为一个关系



## SQL数据定义、更新及数据库编程

### SQL数据定义语言

包括对数据库、基本表、视图、索引的增删改(后两个不能改)

包括：数据库、关系模式、每个属性的值域、完整性约束、每个关系的索引集合和关系的物理存储结构等

数据库保存了企业所有的数据，以及相关的一些控制信息，如安全性和完整性约束、关系的存储路径等

数据库包含了基本表、视图、索引以及约束等对象

![image-20220626163833868](img/image-20220626163833868.png)

数据库作为一个整体存放在外存的物理文件中
物理文件(即磁盘文件)有两种：
一是数据文件，存放数据库中的对象数据
二是日志文件，存放用于恢复数据库的企业冗余数据
物理文件可以是多个，可以将一个或若干个物理文件设置为一个逻辑设备
数据库可以有多个逻辑设备，必须在定义数据库时进行定义
数据库的对象存放在逻辑设备上，由逻辑设备与物理文件进行联系，从而实现数据库的逻辑模式与存储模式的独立

数据库定义(SQL server)：长度 1 到 30 之丰富，首必须字母、下划线或@，之后可以有数字，不能有空格。数据库的大小可以被扩展或者收缩

> 如：
>
> ```sql
> CREATE DATABASE ScoreDB 
> ON    -- 定义第一个逻辑设备(默认为主逻辑设备)及其数据文件
>       (  NAME=ScoreDB,     -- 数据文件的逻辑文件名(即别名)
>          FILENAME='e:\SQLDatabase\ScoreDB.mdf',   -- 物理(磁盘)文件名 
>          SIZE=2, 
>          MAXSIZE=10, 
>          FILEGROWTH=1 ) 
> LOG ON         -- 定义日志逻辑设备及其日志文件
>       (  NAME=ScoreLog,     -- 日志文件的逻辑文件名(即别名)
>          FILENAME='e:\SQLDatabase\ScoreLog.ldf',   -- 日志(磁盘)文件名 
>          SIZE=1, 
>          MAXSIZE=5, 
>          FILEGROWTH=1 )
> ```
>
> SQL Server 可以 `alter database 数据库名 动词 名词……` 

通过将基本表与逻辑设备相关联，使得一个基本表可以放在一个数据文件(磁盘文件)上，也可以放在多个数据文件上

> SQL中的基本数据类型是：
> 整型：int (4B)，smallint (2B)，tinyint (1B)；
> 实型：float，real (4B)，decimal(p, n)，numeric(p, n)；
> 字符型：char(n)，varchar(n)，text；
> 2进制型：binary(n)，varbinary(n)，image；
> 逻辑型：bit，只能取0和1，不允许为空；
> 货币型：money (8B, 4位小数)，small money (4B, 2位小数)；
> 时间型：datetime (4B, 从1753.1.1开始)，
>                      smalldatetime (4B, 从1900.1.1开始)
> 其中：image为存储图象的数据类型，text存放大文本数据

基本**表的定义**：(基本表的名称，最多可包含 128 个字符) (**约束定义**)

```sql
 CREATE TABLE <tableName>
      (  <columnName1> <dataType>
                  [DEFAULT <defaultValue>] [null | NOT null], 
         [ <columnName2> <dataType>
                  [DEFAULT <defaultValue>] [null | NOT null],  … ]
        [ [CONSTRAINT <constraintName1>] {UNIQUE | PRIMARY KEY}
                  (<columName> [, <columName>…]) [ON <filegroupName>], ... ] 
        [ [CONSTRAINT <constraintName2>] 
                  FOREIGN KEY (<columName1> [, <columName2>…] ) 
                  REFERENCE [<dbName>.owner.]<refTable>
                         (<refColumn1> [, <refColumn2>… ]) [ON <filegroupName>], ... ] 
     ) [ON <filegroupName>] 
```

> 如：
>
> ```sql
> CREATE TABLE Course (          -- 创建课程表Course
>     courseNo	      char(3) 	                             NOT NULL,     --课程号
>     courseName   varchar(30)   UNIQUE            NOT NULL,     --课程名
>     creditHour     numeric(1)     DEFAULT  0     NOT NULL,     --学分
>     courseHour    tinyint            DEFAULT  0      NOT NULL,     --课时数
>     priorCourse   char(3)		                NULL,               --先修课程
>     /* 建立命名的主码约束和匿名的外码约束 */
>     CONSTRAINT CoursePK PRIMARY KEY (courseNo),
>     FOREIGN KEY (priorCourse)  REFERENCES  Course(courseNo)  
> )              -- 外码约束是匿名的
> ```
>
> ```sql
> CHECK (studentNo LIKE '[0-9][0-9][0-9][0-9][0-9][0-9][0-9]'),    --学号
> ```

修改：

增加列（新增一列的值为空值）：
ALTER TABLE <tableName>
     ADD <columnName> <dataType>
增加约束：
ALTER TABLE <tableName>
     ADD CONSTRAINT <constraintName>
删除约束：
ALTER TABLE <tableName>
     DROP <constraintName>
修改列的数据类型：
ALTER TABLE <tableName>
     ALTER COLUMN <columnName> <newDataType>

> ```sql
> [例7.6]  在MyTempDB数据库中为TempTable表增加一列。
> ALTER TABLE TempTable
> ADD xsex int DEFAULT 0
> [例7.7]  在MyTempDB数据库中为TempTable表的xname 列修改数据类型。
> ALTER TABLE TempTable
> ALTER COLUMN xname char(10)
> [例7.8]  在MyTempDB数据库中为TempTable表的xname列增加唯一约束
> ALTER TABLE TempTable
> ADD CONSTRAINT UniqueXname UNIQUE (xname)
>     注意：基本表在修改过程中，不可以删除列，一次仅执行一种操作。
> ```

删除：DROP TABLE <tableName> [RESTRICT | CASCADE]

若选择RESTRICT，则该基本表的删除有限制条件
即该基本表不能有视图、触发器以及被其它表所引用(如检查约束CHECK，外码约束FOREIGN KEY)，该项为缺省项。
若选择CASCADE，则该基本表的删除没有限制条件
在删除基本表的同时，也删除建立在该基本表上的所有索引、完整性规则、触发器和视图等。

> SQL Server不支持 [RESTRICT | CASCADE]选项

索引：一个基本表可以建立多个索引。由<搜索码值，指针>的记录构成
索引中的记录(称为索引项)按照搜索码值的顺序进行排列，但不改变基本表中记录的物理顺序；索引和基本表分别存储。

索引的类型有：聚集或非聚集，非聚集索引就是普通索引，一个基本表可以建立多个普通索引。
每个基本表仅能建立一个聚集索引

聚集索引按搜索码值的某种顺序(升/降序)来重新组织基本表中的记录
即索引的顺序就是基本表记录存放的顺序
聚集索引可以极大地提高查询速度，但是给数据的修改带来困难
建立了聚集索引的基本表一般仅执行查询操作，很少进行更新操作，这在数据仓库中使用得较多。

索引建立：

```sql
CREATE [UNIQUE] [CLUSTERED | NONCLUSTERED] INDEX <indexName>
 ON <tableName> ( <columnName1> [ASC | DESC]
                                  [, <columnName2> [ASC | DESC] … ] )
 [ON <filegroupName>]
```

UNIQUE：建立唯一索引(要求索引属性或属性组上的取值必须唯一)；
CLUSTERED | NONCLUSTERED ：表示建立聚集或非聚集索引，默认为非聚集索引；

> 如：
>
> ```mysql
> CREATE NONCLUSTERED INDEX InstituteIdx 
> ON Class(institute)
> CREATE INDEX ClassBirthIdx 
> ON Student(classNo, birthday DESC)
> DROP INDEX <indexName> ON <tableName>
> ```



### SQL数据更新语言

**全节重点**

包括插入、删除和修改数据

插入：(没有列出的属性列自动取空值NULL或默认值)

```sql
INSERT INTO <tableName> [ (<columnName1> [, <columnName2> ... ] ) ] 
VALUES (<value1> [, <value2> ... ] ) 
```

> 如：
>
> ```sql
> insert into student(sno,sname,ssex,sdept,sage) 
> select fields from table
> insert into student(sno,sname,ssex,sdept,sage)
> select '200215128','陈冬','男','IS','18'
> union all
> select '200215129','陈冬','男','IS','18'
> ```
>
> ```sql
> DELETE FROM Score 
> WHERE studentNo IN (
>     SELECT studentNo 
>     FROM Score 
>     GROUP BY studentNo
>     HAVING avg(score) BETWEEN 60 AND 70 )
> ```
>
> ```sql
> UPDATE Score 
> SET score=88
> WHERE courseNo='002' AND termNo='151' 
>       AND studentNo IN 
>       (  SELECT studentNo FROM Student 
>                     WHERE studentName='王红敏' )
> ```
>
> ```sql
> UPDATE Score 
> SET score=88
> FROM Score a, Student b
> WHERE a.studentNo=b.studentNo AND courseNo='002' 
>       AND termNo='151' AND studentName='王红敏'
> ```
>
> ```sql
> UPDATE Class 
> SET classNum=sCount 
> FROM Class a, 
>   ( SELECT classNo, count(*) sCount
>    FROM Student
>    GROUP BY classNo ) b
> WHERE a.classNo=b.classNo
> ```

插入、删除和修改操作会破坏数据的完整性，如果违反了完整性约束条件，其操作会失败



### 视图

**全节重点**

视图是虚表，是从一个或几个基本表(或视图)中导出的表。(**概念**)

基于视图的操作包括：查询、删除、受限更新和创建基于该视图的新视图

视图的主要作用是：(**特点**)

- 简化用户的操作；
- 使用户能以多种角度看待同一数据库模式；
- 对重构数据库模式提供了一定程度的逻辑独立性；
- 能够对数据库中的机密数据提供一定程度的安全保护；
- 适当的利用视图可以更清晰的表达查询

```sql
CREATE VIEW <viewName> [(<columnName1> [, <columnName2> ... ]) ]
AS <subquery>
[WITH CHECK OPTION] -- 有的话增删改必须满足
```

> ```sql
> CREATE VIEW StudentView1999
> AS SELECT * FROM Student WHERE year(birthday)=1999      -- 创建视图中的谓词条件
> WITH CHECK OPTION
> ```
>
> ```sql
> CREATE VIEW StudentAgeView
> AS SELECT studentNo, studentName, year(getdate())-year(birthday) age FROM Student
> ```
>
> ```mysql
> SELECT * FROM SourceView WHERE courseAvg>=80
> -- 对聚合函数的限制转化为 having
> ```

当视图是基于一个基本表创建的，且保留了主码属性，这样的视图称为行列子集视图

从系统的角度来看，查询视图的过程是：

- 有效性检查：检查查询中涉及的基本表和视图是否存在？
- 从数据字典中取出创建视图的语句，将创建视图的子查询与用户的查询结合起来，转换成等价的对基本表的查询；
- 执行改写后的查询

**更新限制条件**

如果创建视图的语句中包含了表达式，或聚合运算，或消除重复值运算，则不能对视图进行更新操作

一般来讲，如果是行列子集视图，则可以对该视图进行更新操作；其它类型的视图，具体的数据库系统有具体的定义，一般不对其进行更新操作

```sql
DROP VIEW <viewName> [CASCADE]
```

如果该视图上还导出了其他视图，使用CASCADE级联删除语句，把该视图和由它导出的所有视图一起删除

删除基本表时，由该基本表导出的所有视图都必须显式地使用DROP VIEW语句删除



### T-SQL语言简介

```sql
局部变量：变量名前加1个@符号
全局变量：变量名前加2个@@符号。如:
@@ERROR：当事务成功时为0，否则为最近一次的错误号
@@ROWCOUNT：返回受上一语句影响的行数
@@FETCH_STATUS：返回最近的FETCH语句执行后的游标状态
变量的声明与赋值
声明变量的语法：
    DECLARE <@variableName> <datatype> [, <@variableName> <datatype> … ]
单个变量赋值的语法：SET <@variableName> = <expr>
变量列表赋值(或显示表达式的值)的语法：
  SELECT <@variableName> [= <expr | columnName>] 
   [, <@variableName> [= <expr | columnName>] … ]
```

![image-20220626182518593](img/image-20220626182518593.png)

![image-20220626182526072](img/image-20220626182526072.png)

![image-20220626182538681](img/image-20220626182538681.png)

![image-20220626182548079](img/image-20220626182548079.png)

程序块范围用 BEGIN-END



### 游标

对SELECT语句的结果集进行逐行处理，需使用游标。
游标(cursor)是系统为用户开设的一个数据缓冲区，用于存放SQL语句的执行结果(元组集合)。每个游标都有一个名字，用户可以用SQL提供的语句从游标中逐一获取元组(记录)，并赋给主变量，交由主语言进一步处理。
可对游标的当前位置进行更新、查询和删除，使用游标需要经历5个步骤：
定义游标：DECLARE
打开游标：OPEN
逐行提取游标集中的行：FETCH
关闭游标：CLOSE
释放游标：DEALLOCATE

```sql
DECLARE <cursorName> CURSOR 
FOR <SQL-Statements>
 [FOR{ READ ONLY | UPDATE [OF <columnName_list>]}] 
```



### 存储过程

存储过程是为了完成特定功能汇集而成的一组命名了的SQL语句集合 (**概念**)

使用存储过程具有如下优点：(**特点**)

- 将业务操作封装
- 便于事务管理
- 实现一定程度的安全性保护
- 减少网络通信量，特别适合统计和查询操作

使用存储过程时，必须执行命令EXECUTE

注意：由于存储过程是在服务器端执行，程序中不需要有输出命令SELECT，由SELECT引出的输出不会在客户端出现。预编译好的。是数据库对象



### 触发器

触发器(trigger)是用户定义在关系表上的一类由事件驱动的存储过程，由服务器自动激活。(**概念**)

是一种特殊的存储过程，不管什么原因造成的数据变化都能自动响应。常用于保证完整性，一定程度上实现完全性 (**特点**)

可以同步数据库的相关表进行级联更新，SQL Server 支持 DDL 触发器

特殊的触发器表：

在SQL Server中使用deleted表和inserted表；
Oracle数据库使用old表和new表

触发器仅在当前数据库中被创建
触发器有3种类型，即插入、删除和修改；
插入、删除或修改也可组合起来作为一种类型的触发器

> ```mysql
> CREATE TRIGGER sexIns    -- 创建插入类型的触发器
> ON Student                                -- 触发器作用的基本表
> FOR INSERT,UPDATE          -- 触发器的类型，即触发该触发器的事件
> AS
>     IF EXISTS (SELECT * FROM inserted WHERE sex NOT IN ('男', '女'))
>           ROLLBACK    -- 事务的回滚操作，即终止触发该触发器的插入操作
> ```



## 数据库存储结构与查询处理

### 索引与散列

**索引的基本概念、作用、优缺点**

索引可以提升查询、分组和排序的性能；全表扫描不一定比使用索引的执行效率低；索引应该建在选择性高的字段上；针对某些字段建立索引,能够有效的减少相关数据库表的磁盘空间占用

顺序索引(ordered index)：索引中的记录(索引项)基于搜索码值顺序排列

组织结构：在索引中按搜索码值的顺序存储索引项，并将索引项与包含该索引项中搜索码值的文件记录关联起来(通过指针).

用于支持快速地对文件中的记录进行顺序或随机地访问

![image-20220626193916032](img/image-20220626193916032.png)

散列索引(hash index)：索引中的记录(索引项)基于搜索码值的散列函数(即哈希函数)的值平均、随机地分布到若干个散列桶中

建立了索引的文件称为索引文件(指数据文件)。索引文件中的记录自身可以按照某种排序顺序存储。一个索引文件可以有多个索引，分别对应于不同的搜索码。
如果索引文件中的记录按照某个搜索码值指定的顺序物理存储，那么该搜索码对应的索引就称为主索引(primary index)，也叫聚集索引(clustering index)。 (存储了实际数据)
与此相反，搜索码值顺序与索引文件中记录的物理顺序不同的那些索引称为辅助索引(secondary index)或非聚集索引(non-clustering index)

对于索引顺序文件，顺序索引有两类：稠密索引和稀疏索引。稀疏索引就是没有每个搜索码都建立索引

![image-20220626194201541](img/image-20220626194201541.png)

通过多级索引技术能够较好地解决上述问题。所谓多级索引就是在索引之上再建立索引

辅助索引必须是稠密索引，即对于每个搜索码值都必须有一个索引项，而且该索引项要存放指向数据文件中具有该搜索码值的所有记录的指针

可以通过指针桶的方式实现，即将数据文件中具有该搜索码值的所有记录的指针存放在一个指针桶中，索引项中的指针域再存放指向指针桶的指针(可以理解为指向指针数组的指针)

![image-20220626194433626](img/image-20220626194433626.png)



**B+树索引**

B+树索引是一个多级索引，但其结构不同于多级顺序索引

B+树索引采用平衡树结构，即每个叶结点到根结点的路径长度相同

B+树索引中的所有结点的结构都相同，它最多包含n-1个搜索码值和n个指针，每个节点搜索码值升序存放

每个非叶子节点有 $[\lceil\dfrac n2\rceil,n]$ 个儿子节点

指针指向具有搜索码值的一条文件记录或一个指针桶，且指针桶中的每个指针指向具有搜索码值的一条文件记录

![image-20220626194605145](img/image-20220626194605145.png)

![image-20220626194736964](img/image-20220626194736964.png)

![image-20220626194837620](img/image-20220626194837620.png)

B+树索引中的非叶结点形成叶结点上的一个多级(稀疏)索引

非叶结点的结构与叶结点相同，只不过非叶结点中的所有指针都是指向B+树中下一层结点的指针

根结点与其他非叶结点不同，它包含的指针数可以小于 $\lceil\dfrac n2\rceil$，但是除了整棵B+树只有一个结点之外，根结点中必须至少包含两个指针

插入和删除比查找更加复杂，因为结点可能因为插入而变得过大需要分裂，或因为删除而变得过小(指针数少于 $\lceil\dfrac n2\rceil$ )而需要合并

此外，当一个结点分裂或一对结点合并时，必须保证B+树能保持平衡

B+树文件组织是通过在B+树的叶结点层直接包含真实的数据记录，以解决索引顺序文件组织中随着文件的增大而性能下降的缺点

在B+树文件组织中，B+树结构不仅用做索引，同时也是文件中记录的组织者，树叶结点中存储的是记录

基于B+树的索引技术能有效地支持范围检索，并且它的等值检索效果也很好



散列索引(hash index)将搜索码值及其相应的文件记录指针组织成一个散列索引项

将散列函数作用于一条文件记录的搜索码值，以确定该文件记录所对应的散列索引项的散列桶；
将由该搜索码值以及相应文件记录指针组成的散列索引项存入散列桶(或溢出桶)中

![image-20220626200352878](img/image-20220626200352878.png)

散列其实就是一种不通过值的比较，而通过值的含义来确定存储位置的方法，它是为有效地实现等值查询而设计的；基于散列技术不支持范围检索



### 查询处理

查询处理(query processing)是指从数据库中提取数据时所涉及的一系列活动 (**概念**)

包括：(**查询处理过程**)

- 语法分析与翻译
- 查询优化
- 查询执行

<u>助记：分翻优执(纷繁幼稚)</u>

![image-20220626202024684](img/image-20220626202024684.png)

- 语法分析与翻译：检查用户查询的语法，并利用数据字典验证查询中出现的关系名、属性名等是否正确；构造该查询语句的语法分析树，并将其翻译成关系代数表达式

- 查询执行计划与查询优化器：一个给定的查询任务，一般都会有多种计算结果的方法 (最主要的模块)

  执行一个查询，不仅需要提供关系代数表达式，还要对该关系代数表达式加上注释来说明如何执行每个关系运算。生成查询执行计划

  不同的查询执行计划会有不同的代价。构造具有最小查询执行代价的查询执行计划称为查询优化，由查询优化器来完成

  查询优化是影响RDBMS性能的关键因素

- 查询执行引擎：根据输入的查询执行计划，调用相关算法实现查询计算，并将计算结果返回给用户

  有效地对内存缓冲区进行管理是影响查询执行性能的非常重要的方面

查询处理的代价：主要包括磁盘存取时间、执行一个查询所用的CPU时间、以及在并行/分布式数据库系统中的通信开销等

对于大型数据库系统而言，在磁盘上存取数据的代价通常是最重要的代价 ，可以通过传输磁盘块数以及搜索磁盘次数来度量

在代价估算时，通常假定是最坏的情形

用于选择运算的搜索算法有：
不用索引的搜索算法——文件扫描，包括

线性搜索算法A1
二分搜索算法A2 

使用索引的搜索算法——索引扫描，包括

在主索引的码属性上的等值比较算法A3
在主索引的非码属性上的等值比较算法A4
在辅助索引上的等值比较算法A5
在主索引上的范围比较算法A6
在辅助索引上的范围比较算法

数据库的很多查询都涉及到连接运算，因此连接运算的效率就成为衡量RDBMS(关系数据库管理系统)性能的一个主要指标

实现连接运算的主要算法有： (全表扫描和索引扫描)

- (块)嵌套循环连接算法(nested-loop join, NLJ) ——最基本的、通用的算法 
- 索引嵌套循环连接算法(index nested-loop join, INLJ) 
- 归并连接算法
- 散列连接算法

其他运算：

- 排序
- 去除重复元组
- 投影
- 集合运算
- 聚集运算 



### 查询优化

处理一个给定的查询，尤其是复杂的查询，通常会有许多种执行策略。
查询优化(query optimization)就是从这许多策略中找出最有效的查询执行计划的处理过程。 
期望RDBMS能够构造并选择出一个具有最小查询执行代价的查询执行计划

查询优化三步：（1,3交叉进行，为主要步骤）

1. 逻辑优化，即产生逻辑上与给定关系代数表达式等价的关系代数表达式；

2. 代价估计，即估计每个查询执行计划的代价；

3. 物理优化，即对所产生的表达式以不同方式作注释，产生不同的查询执行计划 (根据系统所提供的存取路径,选择合理的存取策略)

等价规则：(**关系表达式转换**)

![image-20220626202930549](img/image-20220626202930549.png)

如何为一个给定查询选择最佳查询执行计划？(**查询优化策略**)
一种方法是搜索所有的查询执行计划，基于代价估计选择最佳的查询执行计划
第二种是使用启发式策略选择查询执行计划
实际中的查询优化器将两种方法结合起来使用

基于代价的优化器(cost-based optimizer)通过使用等价规划从给定的查询语句产生一系列查询执行计划，并选择代价最小的一个。对于一个复杂的查询，等价于给定查询的不同查询执行计划可能很多。

基于代价的优化器在实际应用中，不可能也没必要对所有可能的查询执行计划进行穷举搜索，通常寻找接近最优的查询执行计划

**启发式优化准则** 启发式(heuristics)规则

1. 尽早执行选择操作
2. 尽早执行投影运算(以1为前提)

(而不是先要对文件进行预处理)

将频繁访问的视图物化并进行维护,有利于提高查询效率；将频繁地向数据库发送的某条查询语句用存储过程来代替,可以提高查询效率

笛卡尔积和连接运算最费时间



### 物理数据库设计

**全节重点**

数据库在物理设备上的存储结构与存取方法称为数据库的物理结构，它依赖于给定的计算机系统。
为一个给定的逻辑数据模型选取一个最适合应用环境的物理结构的过程，就是数据库的物理设计。

目标：
提高数据库性能，以满足应用的性能需求；
有效利用存储空间；
在性能和空间之间做出最优平衡。

内容：
确定数据库的存储结构；
为数据选择合适的存取路径，即索引的设计；
对物理结构进行评价，重点是评价时间和空间效率

确定数据存放位置 ：为了提高系统性能，数据应该根据应用情况将易变部分和稳定部分、经常存取部分和存取频率较低部分分开来存放。

确定数据库存储结构：确定数据库存储结构时要综合考虑存取时间、存储空间利用率和维护代价三个方面的因素。这三个方面常常是相互矛盾的。例如，消除一切冗余数据虽然能够节约存储空间，但往往会导致检索代价的增加，因此必须进行权衡，选择一个折衷方案

在关系数据库中，选择存取路径主要是指确定如何建立索引。例如:
应选择哪些属性作为搜索码建立索引
建立多少个索引
建立聚集索引(主索引)，还是非聚集索引(辅助索引)
建立单码索引还是组合索引，等等

常用的文件存储方式有三种：索引方法、聚集方法和散列(Hash, 哈希)方法。目前使用最普遍的是B+树索引 

通常情况下，系统配置参数包括：同时使用数据库的用户数，同时打开数据库对象数，使用的缓冲区长度、个数，时间片大小，数据库的大小，装填因子，锁的数目等。这些参数值影响存取时间和存储空间的分配，在数据库物理设计时要根据应用环境确定这些参数值，以使系统性能最优

注意，在数据库物理设计时对系统配置参数的调整只是初步的，在系统运行时还要根据系统实际运行情况做进一步的调整，以期切实改进系统性能

数据库物理设计过程中，需要对时间效率、空间效率、维护代价和各种用户要求进行权衡，其结果可以产生多种方案

数据库设计人员必须对这些方案进行细致的评价，从中选择一个较优的方案作为数据库的物理结构

影响物理设计的主要因素：

- 应用处理需求。在进行数据库物理设计前，应先弄清应用的处理需求，如吞吐量、平均响应时间、系统负荷、事务类型及发生频率等，这些需求直接影响着设计方案的选择，而且它们还会随应用环境的变化而变化。
- 数据特征。数据本身的特性对数据库物理设计也会有较大影响。如关系中每个属性值的分布、记录的长度与个数等，这些特性都影响到数据库的物理存储结构和存取路径的选择。
- 运行环境。数据库物理设计与运行环境有关，因此在设计时还要充分考虑DBMS、操作系统、网络、计算机硬件等运行环境的特征和限制。
- 物理设计的调整。数据库物理设计是基于数据库的当前状况选择的一个合适的设计方案。数据库的状态和特性也会发生变化，因此可能导致以前的物理设计不能再满足目前的应用需求，因此，需对物理设计不断调整，甚至是重新设计

<u>助记：应运特调</u>



## 数据库安全性与完整性

### 数据库安全性

**概述**

数据库的安全性：保护数据库以防止不合法使用所造成的数据泄密、更改或破坏

数据库的完整性：防止数据库中存在不符合语义的数据，其防范对象是不合语义的、不正确的数据 （正确性和相容性）

安全性、完整性、并发控制是必须的，可移植性不是

数据库系统中大量数据集中存放，且为许多最终用户直接共享

数据库安全保护目标是确保只有授权用户才能访问数据库，未被授权的人员则无法接近数据

安全措施是指计算机系统中用户直接或通过应用程序访问数据库所要经过的安全认证过程

![image-20220626205816440](img/image-20220626205816440.png)

- 用户标识与鉴别(identification & authentication)

  当用户访问数据库时，要先将其用户名(user name)与密码(password)提交给数据库管理系统进行认证；
  只有在确定其身份合法后，才能进入数据库进行数据存取操作

- 数据库安全保护

  通过身份认证的用户，拥有了进入数据库的“凭证”；
  用户在数据库中执行什么操作，需通过“存取控制”或视图进行权限分配

存取控制：决定用户对数据库中的哪些对象进行操作，进行何种操作。存取控制机制主要包括两部分：

- 定义用户权限及将用户权限登记到数据字典中；

- 合法权限检查：当用户发出操作请求后，DBMS查找数据字典并根据安全规则进行合法权限检查，若操作请求超出了定义的权限，系统将拒绝执行此操作

- 操作系统安全保护
  通过操作系统提供的安全措施来保证数据库的安全性 

- 数据密码存储

  访问控制和存取控制可将用户的应用系统访问范围最小化和数据对象操作权限最低化，但对一些敏感数据进行“加密存储”也是系统提供的安全策略；
  数据加密(data encryption)：防止数据库中数据存储和传输失密的有效手段；
  加密的基本思想：先根据一定的算法将原始数据(即明文, plaintext)加密为不可直接识别的格式(即密文, ciphertext)，然后数据以密文的方式存储和传输

视图：通过为不同的用户定义不同的视图，达到限制用户访问范围的目的。(提高了安全性，不提高隔离性、完整性)

视图机制能隐藏用户无权存取的数据，从而自动地对数据库提供一定程度的安全保护；
视图的主要功能在于提供数据库的逻辑独立性，其安全性保护不太精细，往往不能达到应用系统的要求；
在实际应用中，通常将视图与存取控制机制结合起来使用，如先通过视图屏蔽一部分保密数据，然后进一步定义存取权限

审计：是一种监视措施，用于跟踪并记录有关数据的访问活动

审计追踪把用户对数据库的所有操作自动记录下来，存放在审计日志(audit log)中；
审计日志的内容一般包括：
操作类型(如修改、查询、删除)；
操作终端标识与操作者标识；
操作日期和时间；
操作所涉及到的相关数据(如基本表、视图、记录、属性)；
数据库的前映像(即修改前的值)和后映像(即修改后的值)。
利用这些信息，可找出非法存取数据库的人、时间和内容等；
数据库管理系统往往将审计作为可选特征，允许操作者打开或关闭审计功能

定义审计功能，必然会带来系统的开销，DBMS一般不启用审计功能

制定审计方针的一些参考意见：
最小化审计选项来降低审计跟踪记录个数，如仅跟踪user01用户。
监视或定期删除审计跟踪记录。
避免审计跟踪记录被非法用户删除。定义的审计表只能DBA操作，任何其它用户没有操作权限

主体：指数据库的访问者，包括用户、进程和线程等；
客体：指数据库中的数据和载体，如基本表、视图、存储过程和数据文件等；
主体与客体是独立的，一个主体可以在一定条件下访问某些客体。

自主存取控制(Discretionary Access Control, 简记为DAC)
DAC是一种基于存取矩阵的存取控制模型；
此模型由三种元素组成，即主体、客体和存取操作。三种元素构成一个矩阵，列表示主体，行表示客体，矩阵中的元素表示存取操作，如读(R)、写(W)、删除(D)和修改(U)操作
在DAC中，主体按存取矩阵的要求访问客体，存取矩阵中的元素可通过授权方式进行修改

强制存取控制(Mandatory Access Control, 简记为MAC)
MAC不是用户能直接感知或进行控制的；
MAC适用于对数据有严格而固定密级分类的部门，如军事部门或政府部门；
对于主体和客体，DBMS为每个实例(值)指派了一个敏感度标记(label)；
敏感度标记被分成若干级别，例如绝密(top secret)、机密(secret)、可信(confidential)和公开(public)等；
主体的敏感度标记称为许可证级别(clearance level)，客体的敏感度标记称为密级(classification level)；
MAC对比主体和客体的敏感度标记，确定主体是否能够存取客体
规则1：仅当主体的许可证级别大于或等于客体的密级时，该主体才能读取相应的客体；
规则2：仅当主体的许可证级别等于客体的密级时，该主体才能写相应的客体
仅当主体的许可证级别小于或等于客体的密级时，才能写相应的客体；
即用户可以为写数据对象赋予高于自己的许可证级别的密级，这样一旦数据被写入，该用户自己也不能再读该数据对象了
两种规则的共同点在于：
禁止拥有高许可证级别的主体更新低密级的数据对象，从而防止敏感数据泄漏.
强制存取控制是对数据本身进行密级标记，标记与数据是不可分的整体；只有符合密级标记要求的用户才可操纵数据，从而提供更高级别的安全性

隐蔽通道
在主体访问客体时，一般通过正常路径访问；但在网络数据流中利用隐蔽通道进行非法通信已成为威胁网络数据库安全的一种重要手段；
在数据库安全中，一定要寻找和防止隐蔽通道的出现，一旦发现要采取措施加以阻塞。
数据库安全的形式化
由于数据库的安全在整个系统中的重要性，必须建立一套有效的形式化体系用于保证其自身正确性，发现并填补安全漏洞，防止隐蔽通道，为数据库安全的研究提供理论依据；
目前数据库安全的形式化已成为高级数据库安全的必要条件.
访问监控器
访问监视器是一个独立的物理机构，由一定的软件和硬件共同组成，它能够监视主体对客体的全部访问活动

最有影响的标准为TCSEC（Trusted Computer System Evaluation Criteria，美国国防部标准，1985年）和          CC（Common Criteria for IT Security Evaluation，ISO标准，1999年）。 
TCSEC将系统划分为４组共七个级别，依次为D，C(C1, C2)，B(B1, B2, B3)和A(A1)：
D：最低级，DOS属于该级别，几乎没有专门的安全机制;
C1：初级的自主安全保护。将用户和数据分离，实现自主存取控制，限制用户权限的传播；
C2：安全产品的最低档，提供受控的存取保护，实施审计和资源隔离：
如Windows 2008、Oracle11g、SQL Server 2014等；
在SQL中，通过授权语句GRANT和REVOKE来实现
B1：标记安全保护。对数据加以标记，对主体和客体实施强制存取控制：
这类产品一般加上安全或信任字样，如Trusted Oracle7；
该标准的核心是强制存取控制。
B2：隐蔽通道和形式化，适合于网络工作方式，目前国内外尚无符合该标准的数据库系统。
B3：访问监控器，适合于网络工作方式，目前国内外尚无符合该标准的数据库系统。
A1：验证设计，较高的形式化要求，仅仅是一种理想化的等级，目前没有相应的系统

国际标准化组织提出的CC文本由三部分组成：简介及一般模型、安全功能要求、安全保证要求。
我国于1999年颁布了国家标准，其标准与TCSEC标准相似



SQL支持受控的存取保护：
在自主存取控制中，用户对不同的数据对象有不同的存取权限；
不同的用户对同一对象有不同的权限；
用户可将其拥有的存取权限转授给其他用户。
自主存取控制通过SQL的**GRANT**和**REVOKE**语句实现

用户权限：是指用户可以在哪些数据对象上进行哪些类型的操作。它由两个要素组成：数据对象和操作类型。
定义存取权限称为授权(authorization)；
授权粒度可以精细到字段级，也可以粗到关系级；
授权粒度越细，授权子系统就越灵活，但是系统的开销也会相应地增大
自主存取控制：用户可自由地决定将数据的存取权限授予何人，决定是否将“授权”的权限授予别人

授权分为数据库级、表级和列级权限。
在SQL Server中权限只能由担任不同角色的用户来分配；
不同类型的用户有不同的等级；
下图给出了授权等级图

![image-20220626210640325](img/image-20220626210640325.png)

对数据库模式的授权则由DBA在创建用户时实现

GRANT和REVOKE有两种权限：目标权限和命令权限

命令权限的授予与收回
主要指DDL操作权限，语法分别为：

```sql
GRANT {all | <command_list>} TO {public | <username_list>} 
REVOKE {all | <command_list>} FROM {public | <username_list>}
<command_list>可以是create database、create default、create function、create procedure、create rule、create table、create view、create index、backup database和backup log等
```

如果具有创建对象的create权限，则自动具有其创建对象的修改alter权限和删除drop权限

对于基本表，自动具有在所创建表上创建、删除和修改触发器的权限；
修改alter和删除drop权限不额外授权；

all：表示上述所有权限

public：表示所有的用户



目标权限(DML)：

```sql
 GRANT {all | <command_list>} ON <objectName> [(<columnName_list>)] 
TO {public | <username_list>} [WITH GRANT OPTION] 
 REVOKE {all | <command_list>} ON <objectName> [(<columnName_list>)] 
FROM {public | <username_list>} [CASCADE | RESTRICT]
```

<command_list>可以是update、select、insert、delete、execute和all
execute针对存储过程授予执行权限；
update、select、insert、delete针对基本表和视图授权

对象的创建者自动拥有该对象的插入、删除、修改和查询操作权限；存储过程的创建者自动拥有所创建过程的执行权限

CASCADE：级联收回；
RESTRICT：缺省值，若转赋了权限，则不能收回；
WITH GRANT OPTION：允许将指定对象上的目标权限授予其它安全用户

不允许循环授权，即不允许将得到的权限授予其祖先

> ```sql
> GRANT create table, create view TO u1, u2
> GRANT all ON Score TO u1 WITH GRANT OPTION
> GRANT select, insert ON Score TO u5
> REVOKE select ON Score FROM u6 
> ```

数据库角色是指被命名的一组与数据库操作相关的权限；
角色是权限的集合，可以为一组具有相同权限的用户创建一个角色



### 数据库完整性

**概述**

为维护数据库的完整性，数据库管理系统提供如下功能:
完整性约束条件定义机制
完整性约束条件也称为完整性规则，是数据库中的数据必须满足的语义约束条件；
由SQL的DDL实现，作为模式的一部分存入数据库中。
完整性检查方法
检查数据是否满足已定义的完整性约束条件称为完整性检查；
一般在insert、delete、update执行后开始检查，或事务提交时进行检查。
违约处理措施
若发现用户操作违背了完整性约束条件，应采取一定的措施，如拒绝操作等

完整性约束条件作用的对象可以是关系、元组、列三种：
列约束：在定义属性的同时定义该属性应满足的约束条件，主要是定义属性的数据类型、取值范围和精度、默认值、是否允许空值、是否唯一、单一属性主码等约束条件；
元组约束：定义元组中属性间的联系的约束条件；在定义属性之后单独定义；
关系约束：定义若干元组间、关系集合上以及关系之间的联系的约束条件，亦称为表约束；在定义属性之后单独定义。

完整性约束，其状态可是静态的，也可是动态的。

静态约束：指数据库每一确定状态时的数据对象所应满足的约束条件。
反映数据库状态合理性的约束；

静态约束主要表现在：

静态列约束：对列的取值域的说明，包括以下几方面：
对数据类型的约束，包括数据的类型、长度、单位、精度等；
对数据格式的约束；
对取值范围或取值集合的约束；
对空值的约束；
其他约束。
静态元组约束：规定元组的各个列之间的约束关系。
静态关系约束：在一个关系的各个元组之间或者若干关系之间常存在各种联系或约束。常见的静态关系约束有：
实体完整性约束；
参照完整性约束；
函数依赖约束，大部分函数依赖约束都在关系模式中定义；
统计约束，字段值与关系中多个元组的统计值之间的约束关系。

动态约束：指数据库从一种状态转变为另一种状态时的新、旧值之间所应满足的约束条件。
反映数据库状态变迁的约束；
动态约束主要表现在：
动态列约束：修改列定义或列值时应满足的约束条件。
修改列定义时的约束。如，将允许空值的列改为不允许空值时，如果该列已存在空值，则拒绝这种修改。
修改列值时的约束。修改列值有时需要参照其旧值，并且新旧值之间需要满足某种约束条件。
例如，职工工资调整不得低于其原来工资，学生年龄只能增长等。
动态元组约束：指修改元组的值时元组中各个字段间需要满足某种约束条件。
例如，职工工资调整时，新工资不得低于原工资+工龄*1.5。
动态关系约束：动态关系约束是加在关系变化前后状态上的     限制条件。
例如，事务一致性、原子性等约束条件。

完整性约束又分为立即执行的约束和延迟执行的约束：
立即执行约束(immediate CONSTRAINTS)：检查是否违背完整性约束的时机是在一条语句执行完后立即检查。
延迟执行约束(deferred CONSTRAINTS)：需要延迟到整个事务执行结束后再进行检查。



**SQL Server 完整性** **实现方法**

用户可以为完整性约束命名，命名格式如下：

一是便于理解约束的含义；
二是修改约束方便，不必查询数据字典

使用ALTER TABLE语句修改基本表中的完整性约束。
要修改约束，必须先删除约束，然后加入新的约束

```sql
[ CONSTRAINT <constraintName> ]
  PRIMARY KEY (<constraintExpr>)
[ CONSTRAINT <constraintName> ]
  FOREIGN KEY (<constraintExpr>) REFERENCES <refTable>(<constraintExpr>) 
[ CONSTRAINT <constraintName> ] 
  CHECK (<constraintExpr>)
```



实体完整性要求基本表的主码值唯一且不允许为空值

- 用 `primary key` 或 `alter` 里 `add primary key` 

- 对单属性构成的主码可定义为列约束, 也可定义为元组约束;
  对多个属性构成的主码，只能定义为元组约束(也有的书将其划归关系约束，即表约束) 

  > ```sql
  > CONSTRAINT ClassPK PRIMARY KEY (classNo)
  > CONSTRAINT ScorePK PRIMARY KEY (studentNo, courseNo, termNo)
  > PRIMARY KEY (studentNo, courseNo, termNo)
  > ```

- 检查主码值是否唯一，如果不唯一则拒绝插入或修改；
  检查主码的各个属性是否为空，只要有一个为空则拒绝插入或修改

- unique 关键字(唯一性约束)

参照完整性为若干个基本表中的相应元组建立联系

- `foreign key` `references` 或 `alter` 的 `add`

- 给出FOREIGN KEY定义的关系称为参照关系；
  由REFERENCES指明的基本表称为被参照关系

- > ```sql
  > CONSTRAINT ScoreFK1 FOREIGN KEY (studentNo)
  > REFERENCES Student(studentNo), -- 表约束
  > FOREIGN KEY (studentNo) REFERENCES Term (studentNo)
  > ```

- 如果外码是主码的一部分，则外码不允许为空值

  (外码是某个关系的码，体现了关系间的联系手段，是一个关系的码同时是另一个关系的属性)

- 当在被参照关系中删除元组时，仅当参照关系中没有任何元组的外码值与被参照关系中要删除元组的主码值相同时，系统才执行删除操作，否则拒绝此操作

- 参照关系中可以随意删除元组

- 级联(CASCADE)操作。当删除或修改被参照关系的某些元组造成了与参照关系的不一致时，则自动级联删除或修改参照关系中所有不一致的元组

  > ```sql
  > CONSTRAINT ScoreFK1 FOREIGN KEY (studentNo)
  >    REFERENCES Student (studentNo)
  >    ON DELETE CASCADE     /* 级联删除参照关系Score中相应的元组 */
  >    ON UPDATE CASCADE,    /* 级联修改参照关系Score中相应的元组 */
  > CONSTRAINT ScoreFK2 FOREIGN KEY (courseNo)  
  >       REFERENCES Course (courseNo)
  >       ON DELETE NO ACTION 	  -- 该定义为默认值，可以不定义 
  >       -- 当修改被参照表Course中的courseNo时, 级联修改参照关系Score中相应的元组 
  >       ON UPDATE CASCADE
  > ```

用户自定义完整性就是定义某一具体应用中数据必须满足的语义要求，由RDBMS提供，而不必由应用程序承担。
用户自定义完整性包括属性上的约束和元组上的约束两种

- 列约束

  - 属性上的约束包括：数据类型、列值非空、列值唯一、设置默认值、满足CHECK (<predicate>)定义等；
    属性上的约束：当向基本表中插入或修改属性值时，系统检查是否满足约束条件，若不满足，则拒绝相应的操作

  - > ```sql
    > sex char(2) NULL CHECK (sex IN ('男', '女')),
    > age	tinyint DEFAULT 16 NULL 
    > CONSTRAINT ageCK CHECK ( age > 0 AND age < 60 ),
    > CHECK ( studNo LIKE '[0-9][0-9][0-9][0-9][0-9]' ) -- 五位数
    > CHECK ( studNo LIKE '[S, M, D, s, m, d, 1, 2, 3][0-9][0-9][0-9][0-9]' )
    > ```

- 元组约束

  - 元组上的约束可以设置不同属性之间的取值的相互约束条件；
    用短语CHECK (<predicate>)引出的约束；
    插入元组或修改属性的值时，RDBMS检查元组上的约束条件是否被满足，若不满足，则拒绝相应的操作

  - > ```sql
    > CONSTRAINT SexCK CHECK (sex='女' OR studName NOT LIKE '刘%'),
    > ```

如果在定义属性的同时定义约束条件，则为列约束；
如果单独定义约束条件，则为元组约束

```sql
ALTER TABLE <tableName> DROP CONSTRAINT <constraintName>
ALTER TABLE <tableName>
   ADD CONSTRAINT <constraintName> 
   < CHECK | UNIQUE | PRIMARY KEY | FOREIGN KEY >  (<constraintExpr>)
```

> ```sql
> ALTER TABLE Stud
>     ADD CONSTRAINT sNoCK 
>     CHECK ( studNo BETWEEN '15001' AND '25999' )
> ALTER TABLE Stud
>     DROP CONSTRAINT ageCK
> ```

也可以规则和触发器实现完整性



## 事务管理与恢复

### 事务

**全节重点**

为解决上述问题，数据库管理系统引入了事务概念，它将这些有内在联系的操作当作一个逻辑单元看待，并采取相应策略保证一个逻辑单元内的全部操作要么都执行成功，要么都不执行。
对数据库用户而言，只需将具有完整逻辑意义的一组操作正确地定义在一个事务之内即可

**概念**

对于用户而言，事务是具有完整逻辑意义的数据库操作序列的集合。
对于数据库管理系统而言，事务则是一个读写操作序列。这些操作是一个不可分割的逻辑工作单元，要么都做，要么都不做。
事务是数据库管理系统中竞争资源、并发控制和恢复的基本单元。 它是由数据库操作语言(如SQL)或高级编程语言（如Java、C、C++）提供的事务开始语句、事务结束语句以及由它们包含的全部数据库操作语句组成

事务结束的类型：

- 事务提交(commit)：将成功完成事务的执行结果(即更新)永久化，并释放事务占有的全部资源。
- 事务回滚(rollback)：中止当前事务、撤销其对数据库所做的更新，并释放事务占有的全部资源

三种事务模式：

- 显式事务是指用户使用Transact-SQL事务语句所定义的事务，其事务语句包括：
  - 事务开始：BEGIN TRANSACTION
  - 事务提交：COMMIT TRANSACTION，COMMIT WORK
  - 事务回滚：ROLLBACK TRANSACTION，ROLLBACK WORK
- 隐式事务是指事务提交或回滚后，系统自动开始新的事务。该类事务不需要采用BEGIN TRANSACTION语句标识事务的开始。
- 自动定义事务：当一个语句成功执行后，它被自动提交，而当执行过程中出错时，则被自动回滚

**特性** 为了保证事务并发执行或发生故障时数据库的一致性（完整性），事务应具有以下ACID特性

- 原子性(atomicity)。事务的所有操作要么全部都被执行，要么都不被执行。
- 一致性(consistency)。一个单独执行的事务应保证其执行结果的一致性，即总是将数据库从一个一致性状态转化到另一个一致性状态。
- 隔离性(isolation)。当多个事务并发执行时，一个事务的执行不能影响另一个事务，即并发执行的各个事务不能互相干扰。
- 持久性(durability)。一个事务成功提交后，它对数据库的改变必须是永久的，即使随后系统出现故障也不会受到影响

原子性也称为故障原子性或(故障)可靠性
由DBMS通过撤销未完成事务对数据库的影响来实现。

一致性是指单个事务的一致性，也称为并发原子性或正确性
由编写该事务代码的应用程序员负责，但有时也可利用DBMS提供的数据库完整性约束(如触发器)的自动检查功能来保证。

隔离性也称为执行原子性或可串行化，可以看作是多个事务并发执行时的一致性或正确性要求
由DBMS的并发控制模块保证。

持久性也称为恢复原子性或恢复可靠性
它是利用已记录在稳固存储介质(如磁盘阵列)中的恢复信息(如日志、备份等)来实现丢失数据(如因中断而丢失的存放在主存中但还未保存到磁盘数据库中去的数据等)的恢复。
它是由DBMS的恢复管理模块保证。



数据库管理系统允许多个事务并发执行：
优点
增加系统吞吐量(throughput)。吞吐量是指单位时间系统完成事务的数量。当一事务需等待磁盘I/O时，CPU可去处理其它正在等待CPU的事务。这样，可减少CPU和磁盘空闲时间，增加给定时间内完成事务的数量。
减少平均响应时间(average response time)。事务响应时间是指事务从提交给系统到最后完成所需要的时间。事务的执行时间有长有短，如果按事务到达的顺序依次执行，则短事务就可能会由于等待长事务导致完成时间的延长。如果允许并发执行，短事务可以较早地完成。因此，并发执行可减少事务的平均响应时间。
缺点
若不对事务的并发执行加以控制，则可能破坏数据库的一致性

问题：

读脏数据。如果事务T2读取事务T1修改但未提交的数据后，事务T1由于某种原因中止而撤销，这时事务T2就读取了不一致的数据。数据库中将这种读未提交且被撤销的数据为读“脏数据”。
丢失更新。两个或多个事务都读取了同一数据值并修改，最后提交事务的执行结果覆盖了前面提交事务的执行结果，从而导致前面事务的更新被丢失
不可重复读。是指事务Ti两次从数据库中读取的结果不同，可分为三种情况：
事务Ti读取一数据后，事务Tj对该数据进行了更改。当事务Ti再次读该数据时，则会读到与前一次不同的值。	
事务Ti按某条件读取数据库中某些记录后，事务Tj删除了其中部分记录。当事务Ti再次按相同条件读取时，发现记录数变少了。（幻影现象1）
事务Ti按某条件读取数据库中某些记录后，事务Tj插入了新的记录。当事务Ti再次按相同条件读取时，发现记录数变多了。（幻影现象2）

死锁不属于并发操作带来的问题



启示：如果一组并发执行事务的执行结果与它们串行执行得到的结果是相同的，那么就可以认为该并发执行的结果是正确的

事务并发执行顺序是随机的，将由多个事务操作组成的随机执行序列称为一个调度。
对由一组事务操作组成的调度序列而言，应满足下列条件：
 该调度应包括该组事务的全部操作；
 属于同一个事务的操作应保持在原事务中的执行顺序

在调度S中，如果属于同一事务的操作都是相邻的，则称S是串行调度。
对由n个事务组成的一组事务而言，共有n!种有效串行调度

事务串行执行可保证数据库的一致性，如果能判断一个并发调度的执行结果等价于一个串行调度的结果，就称该并发调度可保证数据库的一致性

假设调度S包含两个事务Ti与Tj，若两个相邻操作OiTi，OjTj访问不同的数据对象，则交换Oi与Oj不会影响调度中任何操作的结果。若Oi与Oj访问相同的数据对象，并且有一个为写操作时，则不能改变它们被调度执行的顺序

在一调度S中，如果Oi与Oj是不同事务在相同数据对象上的操作，并且其中至少有一个是写操作，则称Oi与Oj是冲突操作；否则称为非冲突操作

如果一调度S可以经过交换一系列非冲突操作执行的顺序而得到一个新的调度S’，则称S与S’是冲突等价的(conflict equivalent)

如果一调度S与一串行调度是冲突等价的，则称S是冲突可串行化的(conflict serializable).

冲突可串行化仅仅是正确调度的充分条件，并不是必要条件，即冲突可串行化调度执行结果一定是正确的，而正确的调度不一定都是冲突可串行化的

设S是一个调度。由S构造一个有向图，称为优先图，记为G=(V, E)，其中V是顶点集，E是边集。顶点集由所有参与调度的事务组成，边集由满足下列3个条件之一的边Ti→Tj组成：
Ti执行了Wi(Q)后Tj执行Rj(Q)；
Ti执行了Ri(Q)后Tj执行Wj(Q)；
Ti执行了Wi(Q)后Tj执行Wj(Q)

![image-20220626231738804](img/image-20220626231738804.png)

基于优先图的冲突可串行化判别准则：如果优先图中无环，则S是冲突可串行化的；如果优先图中有环，则S是非冲突可串行化的

测试冲突可串行化的算法：

构建S的优先图；
采用环路测试算法（如基于深度优先搜索的环检测算法）检测S中是否有环；
若S包含环，则S是非冲突可串行化的，否则调度S是冲突可串行化的

![image-20220626231904182](img/image-20220626231904182.png)

![image-20220626231926179](img/image-20220626231926179.png)



### 并发控制

**封锁概念**

DBMS 普遍采用封锁保证调度正确性(解决并发操作带来的数据不一致问题)

并发控制机制大体上可分为悲观的和乐观的两种。
悲观的并发控制方法认为数据库的一致性经常会受到破坏，因此在事务访问数据对象前须采取一定措施加以控制，只有得到访问许可时，才能访问数据对象，如基于封锁的并发控制方法。——事前控制
乐观的并发控制方法则认为数据库的一致性通常不会遭到破坏，故事务执行时可直接访问数据对象，只在事务结束时才验证数据库的一致性是否会遭到破坏，如基于有效性验证方法。——事后验证
本章介绍基于封锁的并发控制方法

基本思想：当事务T需访问数据对象Q时，先申请对Q的锁。如批准获得，则事务T继续执行，且此后不允许其他任何事务修改Q，直到事务T释放Q上的锁为止。
基本锁类型：
共享锁（shared lock, 记为S）：如果事务T获得了数据对象Q的共享锁，则事务T可读Q但不能写Q。
排它锁（exclusive lock, 记为X）：如果事务T获得了数据对象Q上的排它锁，则事务T既可读Q又可写Q。

“锁相容”是指如果Ti已持有数据对象Q的某类型锁后，事务Tj也申请对Q的封锁。如果允许事务Tj获得对Q的锁，则称事务Tj申请锁类型与事务Ti的持有锁类型相容；否则称为不相容。
基本锁类型的封锁相容性原则：
共享锁与共享锁相容
排它锁与共享锁、排它锁与排它锁是不相容的。

申请和释放锁操作：
SL(Q)——申请数据对象Q上的共享锁；
XL(Q)——申请数据对象Q上的排它锁；
UL(Q)——释放数据对象Q上的锁

死锁是事务故障



### 恢复与备份

对于一个事务而言，它是通过三个地址空间同数据库进行交互：
保存数据库记录的磁盘块空间——物理数据库；
缓冲区管理器所管理内存地址空间——数据缓冲区；
事务的局部地址空间——事务工作区。
数据元素A在不同地址空间之间移动的操作原语包括：
INPUT(X)：将包含数据库记录A的磁盘块X拷贝到数据缓冲区。
READ(A)：将数据记录A从缓冲区拷贝到事务工作区。如果包含A的磁盘块X不在缓冲区中，则首先执行INPUT(X)操作。
WRITE(A)：将A的修改值从事务工作区拷贝到缓冲区X块中以替换A的值。如果此时X块不在内存缓冲区中，则应先执行INPUT(X)后再替换。
OUTPUT(X)：将更新后的X块写回磁盘

注意的是发出这些命令的对象是不同的：READ和WRITE由事务发出，而INPUT和OUTPUT由缓冲区管理器发出

对于不同类型的故障在恢复时应做不同的恢复处理。
恢复的本质是利用存储的冗余数据（如日志、影子、备份副本(数据库后备副本)等）来重建数据库中已经被破坏或已经不正确的那部分数据。
DBMS中的恢复管理模块由两部分组成：
正常事务处理过程中：系统需记录冗余的恢复信息，以保证故障发生后有足够的信息进行数据库恢复；
故障发生后：利用冗余信息进行UNDO或REDO等操作，将数据库恢复到一致性状态。 

**日志概念** DBMS记录数据库全部更新操作的序列文件

主要特点有：
日志文件记录了数据库的全部更新顺序。
日志文件是一个追加（append-only）文件。
DBMS允许事务的并发执行导致日志文件是“交错的”。
属于单个事务的日志顺序与该事务更新操作的执行顺序是一致的。
日志记录通常是先写到日志缓冲区中，然后写到稳固存储器（如磁盘阵列）中

类型：
记录数据更新操作的日志记录，包括UPDATE、INSERT和DELETE操作；
记录事务操作的日志记录, 包括START、COMMIT和ABORT操作

格式：

<Ti, A, V1, V2>表示事务Ti对数据元素A执行了更新操作，V1表示A更新前的值(前映像)， V2表示A更新后的值(后映像)。对于插入操作，V1为空；对于删除操作，V2为空。
<Ti, START>表示事务Ti已经开始。此时DBMS完成对事务的初始化工作，如分配事务工作区等。
<Ti, COMMIT>表示事务Ti已经提交，即事务Ti已经执行成功（该事务对数据库的修改必须永久化）。事务提交时其更新的数据都写到了数据缓冲区中，但是由于不能控制缓冲区管理器何时将缓冲块从内存写到磁盘。因此当看到该日志记录时，通常不能确定更新是否已经写到磁盘上。
<Ti, ABORT>表示事务已经中止，即事务执行失败。此时，如果Ti所做的更新已反映到磁盘上，DBMS必须通过UNDO操作来消除Ti对磁盘数据库的影响

**基于日志的故障恢复策略**

为了保证数据库能运用日志进行恢复，要求日志文件必须放到稳固存储器(如磁盘阵列)上，并且要求每条日志记录必须在其所包含数据记录的更新值写到外存储器之前先写到稳固存储器上，即先写(write-ahead)日志规则

对于要UNDO的事务T，日志中记录有<T, START>以及T对数据库的所有更新操作的日志记录。
UNDO过程为：从T的最后一条更新日志记录开始，从日志尾向日志头(反向)依次将T更新的数据元素值恢复为旧值(V1)。
如果增加一条规则：T对数据库的所有更新值在日志<T, COMMIT>写到磁盘前不能写到磁盘。这样，如果没有看到事务T 的<T, COMMIT>日志记录，就可以确定T的更新值一定没有写到磁盘，从而可避免UNDO操作

与UNDO相反，REDO操作是对已提交事务进行重做，将数据库状态恢复到事务结束后的状态。
对于要REDO的事务T，日志中已经记录了<T, START>、T的所有更新操作日志以及<T, COMMIT>。
REDO过程为：从T的第一条更新日志记录开始，从日志头向日志尾(顺向)依次将T更新的数据元素值恢复为新值(V2)。
如果增加一条规则：事务对数据库的所有更新值必须在提交日志<Ti, COMMIT>写到磁盘之前写到磁盘，就可以避免REDO操作

总结：

- 崩溃区间有 start 没 commit，执行 undo
- 崩溃区间有 start 有 commit ，执行 redo

三个阶段：

分析阶段：从日志头开始顺向扫描日志，确定重做事务集（REDO-set）和撤销事务集（UNDO-set）。将既有<T, START>又有<T, COMMIT>日志记录的事务T加入REDO-set；将只有<T, START>没有<T, COMMIT>日志记录的事务T加入UNDO-set。
撤销阶段：从日志尾反向扫描日志，对每一条属于UNDO-set中事务的更新操作日志依次执行UNDO操作。
重做阶段：从日志头顺向扫描日志，对每一条属于REDO-set中事务的更新操作日志依次执行REDO操作

UNDO与REDO必须是幂等的，即重复执行任意次的结果与执行一次的结果是一样的

为了减少扫描开销和提高恢复效率，引入了检查点技术

检查点是周期性地向日志中写一条检查点记录并记录所有当前活跃的事务，为恢复管理器提供信息，以决定从日志的何处开始恢复。
检查点工作主要包括：
将当前位于日志缓冲区的所有日志记录输出到磁盘上；
将当前位于数据缓冲区的所有更新数据块输出到磁盘上；
记录日志记录<Checkpoint L>并输出到磁盘上，其中L是做检查点时活跃事务的列表

在检查点执行过程中，不允许事务执行任何更新动作，如写缓冲块或写日志记录，称其为静态检查点技术。
如果事务T在做检查点之前就已提交，那么它的<T, COMMIT>记录一定出现在<Checkpoint L>记录前，并且其更新在做Checkpoint时都已写到磁盘中，因此不需要对T做任何恢复操作，这样可大大减少恢复工作量。
如果数据缓冲区及日志缓冲区中缓存的更新数据很多时，就会导致系统长时间不能接受事务处理，这对响应时间要求较严格的系统来说是不可忍受的。为避免这种中断，可使用模糊检查点(fuzzy checkpoint)技术，允许在做检查点的同时接受数据库更新操作

![image-20220626233719454](img/image-20220626233719454.png)

撤销阶段：首先，从日志尾反向扫描日志文件至遇到最后一次检查点止，对每一条属于UNDO-set中事务的更新操作日志依次执行UNDO操作

从最后一次检查点开始顺向扫描日志，对每一条属于REDO-set中事务的更新操作日志依次执行REDO操作

![image-20220626234023882](img/image-20220626234023882.png)



数据备份(Backup)是数据库管理系统用来进行介质故障恢复的常用方法。它是由DBA周期性地将整个数据库的内容复制到其他外存储器上(通常为大容量的磁带或磁鼓)保存起来

数据备份操作可分为静态备份和动态备份。
静态备份是在系统中无运行事务时进行的备份操作。优点是简单。但由于备份必须等待用户事务结束后才能进行，而新的事务必须等待备份结束后才能执行，因此会降低数据库的可用性。
动态备份是指备份操作与用户事务的执行并发进行，备份期间允许对数据库进行存取或修改。动态备份克服了静态备份的缺点，它不用等待正在运行的用户事务结束，也不会影响新事务的运行。但它不能保证副本中的数据正确有效

具体进行数据备份时可以分全备份和增量备份。
全备份是指每次备份全部数据库，而增量备份是只备份上次备份后更新过的数据。
从恢复角度看，一般说来使用全备份得到的后备副本进行恢复会更方便些。但如果数据库很大，事务处理又十分频繁，则增量备份方式更实用更有效。
不管是那种形式的备份及介质故障的恢复处理，都需要DBA介入。由于备份又是十分耗费时间和资源的，不能频繁进行。所以DBA应该根据数据库使用情况确定适当的备份周期和备份方法







# NoSQL

暂未学



# 分布式

## 理论

### 大数据

传统关系型数据库：`Oracle、openGauss、SQL Server、Mysql、DB2、SyBase、SqlLite`，容易理解、使用方便、易于维护。

缺点：

- 扩展性差（仅几十台），成本高；仅本地磁盘存储
- 难以满足海量数据（TB级）的高效率读写；
- 难以满足非常高的并发读写需求；

![image-20230228110559638](img/image-20230228110559638.png)

大数据：分布式存储，分布式计算； 普通服务器搭建；节点故障不影响集群

大数据：无法用常规工具对其内容进行分析处理的海量数据集合。

大数据技术：从多种类型的海量数据中快速获得有价值信息的技术

NoSQL（Not Only SQL）技术；对海量数据进行存储、管理、分析的系统：大数据库。大数据库+ 软件环境：大数据库系统

<img src="img/image-20230228110644593.png" alt="image-20230228110644593" style="zoom:50%;" />

<img src="img/image-20230228110655049.png" alt="image-20230228110655049" style="zoom: 50%;" />

NoSQL技术：根据CAP理论，遵循BASE原则设计。

**CAP理论**：一个分布式系统不能同时满足一致性(consistency)、可用性(availability)和分区容错性(partition tolerance)这三个需求，最多只能同时满足两个

> 分布式系统中，节点组成的网络应该是连通的。然而因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据散布在这些不连通的区域中，这就叫分区。
>
> 当一个数据项只在一个节点中，分区出现后，和这个节点不连通的部分就访问不到这个数据。这时分区是无法容忍的。
>
> 提高分区容忍性的办法：一个数据项复制到多个节点上，出现分区后，这一数据项就可能分布到各个区里。容忍性就提高了。
>
> 数据复制到多个节点：一致性问题，多个节点的数据可能不一致。要保证一致，每次写操作要等待全部节点写成功，等待会带来可用性问题。
>
> 数据存在的节点越多，分区容忍性越高，要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低

**BASE原则**是指Basically Available（基本可用）、Soft State（软状态）和Eventually Consistent（最终一致性）。

- 基本可用是指分布式系统在出现不可预知的故障的时候，允许损失部分可用性。 
   a、时间上的损失：正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户的查询结果，但由于出现故障，查询结果的响应时间增加到1-2秒。 
   b、功能上的损失：正常情况下，在一个电子商务网站上进行购物，消费者几乎能够顺利的完成每一笔订单；

  购物高峰（双十一），购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面

- 软状态: 不用完全符合ACID的原子性
  把订单状态改成已支付成功，告诉用户已经成功了；再异步发送消息通知积分服务和仓库服务，即使消息失败,MQ消息也会重新发送（重试）

  <img src="img/image-20230228110909914.png" alt="image-20230228110909914" style="zoom:50%;" />

  <img src="img/image-20230228110917075.png" alt="image-20230228110917075" style="zoom:50%;" />

  ACID原则：Atomicity原子性 Consistency一致性 Isolation隔离性 Durability持久性

- 最终一致性：系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一致的状态。

  不需要实时保证强一致性

BASE原则： 对CAP中一致性和可用性权衡的结果，大规模分布式系统实践的总结；

BASE原则的核心：无法做到强一致性，可以根据自身业务特点，采用适当的方式来使系统达到最终一致性



按数据存储方式和处理数据的类型分类：

![image-20230228111145543](img/image-20230228111145543.png)

<img src="img/image-20230228111219808.png" alt="image-20230228111219808" style="zoom: 50%;" />



### Hadoop

#### 简介

Hadoop是一个由Apache基金会所开发的[分布式系统](https://baike.baidu.com/item/分布式系统/4905336)基础架构

三大核心组件：

- 分布式文件系统 解决海量数据存储（HDFS）Hadoop Distributed File System
- 并行计算框架 海量数据分析（MapReduce）
- 资源管理平台 资源管理调度问题（YARN）Yet Another Resource Negotiator

<img src="img/image-20230228111448743.png" alt="image-20230228111448743" style="zoom:50%;" />

<img src="img/image-20230228111544670.png" alt="image-20230228111544670" style="zoom:50%;" />

#### HDFS

HDFS是根据Google的GFS来实现的

HDFS是主从结构的，有主节点（NameNode）和从节点（DataNode）。一个主节点可关联多个从节点，一个从节点也可关联多个主节点。从节点又称数据节点。每一个block会在多个DataNode上存储多份副本

<img src="img/image-20230228112314376.png" alt="image-20230228112314376" style="zoom:50%;" />

在存储数据的时候，会将数据进行切块。Block是HDFS的基本存储单位。可以通过dfs.blocksize来更改块的大小，在更改的时候，单位是字节。如果一个文件的大小不足128M，那么这个文件是多大，在HDFS上就占多大

在HDFS中，会对Block进行编号，我们称之为Block ID

分块的好处：便于存储超大文件，便于进行快速的备份

为了防止产生数据丢失，会将数据进行备份，备份称为副本。在Hadoop中，默认的副本数量为3

<img src="img/image-20230228112324388.png" alt="image-20230228112324388" style="zoom: 33%;" />

<img src="img/image-20230228112503917.png" alt="image-20230228112503917" style="zoom:33%;" />

namenode 管理DataNode和记录元数据Meta，元数据包括：①记录数据的虚拟存储路径、记录文件的切块数量、记录每一个数据块的存放位置、数据块的副本数量、文件权限。元数据的大小是在150B左右（130B-180B）

NameNode将元数据维系在内存以及磁盘中，在内存中的目的是为了快速查询，在磁盘中的目的是为了崩溃恢复。元数据在磁盘中是以edits文件和fsimage文件的形式存在。dits：记录写操作；fsimage:记录元数据，fsimage中的元数据和内存中的元数据并不是同步的

当NameNode接收到写请求之后，会将该请求记录到edits_inprogress文件中，如果记录成功，则将该请求同步更新到内存中，修改内存中的元数据，内存修改完成之后会给客户端返回一个ack表示成功

hadoop.tmp.dir属性：其他临时目录的基础，如果不配置，就是根目录下的/tmp

在HDFS中，会给每一个写操作分配一个编号叫做事务id，在HDFS中叫txid。当edits文件达到条件的时候会将操作更新到fsimage文件中，修改fsimage文件的元数据：①空间维度：当edit_inprogess文件达到指定大小的时候就会触发更新，默认是64M，大小可以由fs.checkpoint.size（core-site.xml）来进行指定，默认单位是字节；②时间维度：当距离上次更新时间达到指定间隔时间时就会触发更新，默认是1小时，时间大小由fs.checkpoint.period来指定，默认单位是秒；③重启更新：NameNode重启之后，会自动的将edits_inprogress中的操作更新到fsimage中；④强制更新：hadoop dfsadmin -rollEdits。

在更新的时候，会将edits_inprogress重命名为edits_xxxxxx-xxxxxx,同时会产生一个新的edits_inprogress。在Hadoop中，如果存在SecondaryNameNode，则更新过程是发生在SecondaryNameNode上。在HDFS中，最核心的节点是NameNode，但是在Hadoop1.0版本中只能有一个NameNode,在Hadoop2.0版本中，允许多设置一个NameNode，代价是丢掉SecondaryNameNode。

NameNode通过心跳机制来管理DataNode：DataNode每隔定长时间会给NameNode发送心跳信息，默认情况下，DataNode每隔3秒给NameNode发送一条信息，如果NameNode长时间（默认是10分钟）没有收到某个DataNode的心跳信息，则认为这个DataNode已经lost（丢失），此时NameNode会将这个DataNode中的数据再次备份，保证副本数量。心跳信息包括：当前节点的状态、当前节点中所存储的数据块信息。

NameNode重新启动的时候，将edits中的操作更新到fsimage中，将fsimage中的元数据加载到内存中，等待DataNode的心跳（如果DataNode一直没有心跳过来，则要重新备份保证副本数量，校验数据总量），这个过程称之为安全模式(safe mode)。如果所有的校验都成功则HDFS会自动退出安全模式。

退出安全模式的命令（极少用）：hadoop dfsadmin -safemode leave

因为安全模式问题，所以在伪分布下，副本数量必须为1-如果副本数量不是为1，则重启NameNode的时候，会导致HDFS一直处于安全模式

副本放置策略：

\1)  第一个副本：如果是外部客户端上传数据，则此时NameNode会选择一个相对空闲的节点，存放第一个副本；如果是内部上传，则第一个副本是放在本节点上。

\2)  第二个副本：在hadoop2.7以前，第二个副本要放在第一个副本的不同**机架**的节点上。从hadoop2.7以后的版本，第二个副本是放在第一个副本相同机架的节点上。

\3)  第三个副本：在hadoop2.7以前，第三个副本是放在和第二副本的同个机架上。从hadoop2.7以后的版本，第三个副本是放在第二个副本不同机架的节点上。

\4)  如果有更多副本：则随机挑选空闲节点存放。

在Hadoop所指的机架并不是物理结构，而是逻辑结构。

可以通过映射关系来指定节点对应的机架，也就意味着同一个物理机架上的节点可以映射到不同的逻辑机架上。

实际使用中，一般会将一个或者几个物理机架上的节点放在一个逻辑机架上

datanode

\1.  用于存储数据，注意数据是以Block形式存储

\2.  数据在DataNode上的存储位置由hadoop.tmp.dir属性决定。存储目录是：

/home/hadoop-2.7.3/tmp/hdfs/data/current/块池/current/finalized/subdir0/subdir0

\3.  DataNode会通过心跳机制（RPC方式）来向NameNode发送心跳信息

<img src="img/image-20230228112525278.png" alt="image-20230228112525278" style="zoom:33%;" />

#### MapReduce

分布式计算框架，基于它写出来的应用程序能够运行在Hadoop集群上。MapReduce采用“分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个从节点共同完成，然后通过整合各个节点的中间结果，得到最终结果。简单地说，MapReduce就是“任务的分解与结果的汇总”

<img src="img/image-20230228112556074.png" alt="image-20230228112556074" style="zoom:50%;" />

Map阶段负责数据的整理，Reduce阶段负责数据的汇总

Reducer中的迭代器只能遍历一次，如果Mapper和Reducer结果类型一致，可以只设置一个

果输入路径是一个文件，则MapReduce只处理这个文件，如果输入的是一个目录，则处理这个目录下的所有文件。注意：如果文件以“\_”开头，则该文件会被跳过，在MapReduce中，以“\_”开头的文件被认为是隐藏文件而不需要处理

mapper中的4个泛型：

`Mapper<LongWritable, Text, Text, Text>`

根据实际情况改变以上参数的类型

keyin：数据偏移量；长整型

valuein：输入的一行数据； 字符串

keyout：输出的健类型：

valueout：输出的值类型

传输的数据要能够被序列化



#### Yarn

资源管理、任务调度分开

| **组件**                | **功能**                                                     |
| ----------------------- | ------------------------------------------------------------ |
| ResourceManager(RM)     | 负责对各NM上的资源进行统一管理和调度。将AM分配空闲的Container运行并监控其运行状态。对AM申请的资源请求分配相应的空闲Container。 |
| NodeManager (NM)        | NM是每个节点上的资源和任务管理器。它会定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态；同时会接收并处理来自AM的Container 启动/停止等请求。 |
| ApplicationMaster  (AM) | 应用框架，它负责向ResourceManager协调资源，并且与NodeManager协同工作完成Task的执行和监控 |
| Container               | Container是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container 表示的。 |

#### 其他

| 其它Hadoop生态圈组件 | 功能                                                         |
| -------------------- | ------------------------------------------------------------ |
| HBase                | 一个建立在HDFS之上，面向列的针对结构化数据的可伸缩、高可靠、高性能、分布式和面向列的动态模式数据库。 |
| Hive                 | Hive定义了一种类似SQL的查询语言(HQL),将SQL转化为MapReduce任务在Hadoop上执行，通常用于离线分析。 |
| Spark                | 一种与  Hadoop 相似的开源集群计算环境，它基于内存计算，数据分析速度更快。 |
| Mahout               | 创建一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。 |
| Pig                  | 提供一种基于MapReduce的数据分析工具。                        |
| Zookeeper            | 解决分布式环境下的数据管理问题                               |
| Sqoop                | 主要用于传统数据库和Hadoop之间传输数据。                     |



### HBase

#### 简介

> BigTable是一个分布式存储系统，起初用于解决典型的互联网搜索问题
>
> 1 爬虫持续不断地抓取新页面，这些页面每页一行地存储到BigTable里
>
> 2 MapReduce计算作业运行在整张表上，生成索引，为网络搜索应用做准备
>
> •BigTable是一个分布式存储系统
>
> •利用谷歌提出的MapReduce分布式并行计算模型来处理海量数据
>
> •使用谷歌分布式文件系统GFS作为底层数据存储
>
> •可以扩展到PB级别的数据和上千台机器，具备广泛应用性、可扩展性、高性能和高可用性等特点
>
> •谷歌的许多项目都存储在BigTable中，包括搜索、地图、财经、打印、社交网站Orkut、视频共享网站YouTube和博客网站Blogger等

HBase是一个高可靠、高性能、面向列、可伸缩的分布式数据库，是谷歌BigTable的开源实现，主要用来存储非结构化和半结构化的松散数据。HBase的目标是处理非常庞大的表，可以通过水平扩展的方式，利用廉价计算机集群处理超过10亿行数据和数百万列元素组成的数据表 

![image-20230302163501835](img/image-20230302163501835.png)

|              | **BigTable** | **HBase**         |
| ------------ | ------------ | ----------------- |
| 文件存储系统 | GFS          | HDFS              |
| 海量数据处理 | MapReduce    | Hadoop  MapReduce |
| 协同服务管理 | Chubby       | Zookeeper         |

Hadoop：解决大规模数据的离线批量处理问题；MapReduce:高延迟数据处理机制，无法满足大规模数据实时处理应用的需求

HDFS面向批量访问模式，不是随机访问模式

传统的关系数据库无法应对数据规模剧增时导致的系统扩展性和性能问题（分库分表也不能很好解决）

传统关系数据库在数据结构变化时一般需要停机维护；空列浪费存储空间

HBase已成功应用于互联网服务领域、在线数据分析处理系统

HBase与传统关系数据库的区别：

（1）数据类型：关系数据库采用关系模型，具有丰富的数据类型和存储方式；HBase采用简单的数据模型，把数据存储为未经解释的字符串

（2）数据操作：关系数据库包含丰富的操作，涉及复杂的多表连接。HBase不存在复杂的表与表之间的关系，只有简单的插入、查询、删除、清空；HBase在设计上避免复杂的表和表之间的关系

（3）存储模式：关系数据库：基于行模式存储。

Hbase：基于列存储，每个列族都由几个文件保存，不同列族的文件是分离的

（4）数据索引：关系数据库：可以针对不同列构建复杂的多个索引，以提高数据访问性能；

 HBase只有一个行键索引，通过巧妙的设计，HBase中的所有访问方法，或者通过行键访问，或者通过行键扫描，从而使得整个系统不会慢下来

（5）数据维护：关系数据库：更新操作会用最新的当前值去替换记录中原来的旧值，旧值被覆盖后不会存在。

 HBase中执行更新操作时，不会删除旧版本，而是生成一个新的版本

（6）可伸缩性：关系数据库很难实现横向扩展，纵向扩展的空间也比较有限。

 HBase能轻易地通过在集群中增加或者减少硬件数量来实现性能的伸缩

<img src="img/image-20230302164008689.png" alt="image-20230302164008689" style="zoom:50%;" />

用户在表中存储数据，每一行都有一个可排序的行键和任意多的列

表在水平方向由一个或者多个列族组成，一个列族中可以包含任意多个列，同一个列族里面的数据存储在一起

列族支持动态扩展，可以很轻松地添加一个列族或列，无需预先定义列的数量以及类型，所有列均以字符串形式存储，用户需要自行进行数据类型转换

<img src="img/image-20230302164300173.png" alt="image-20230302164300173" style="zoom:67%;" />

表：HBase采用表来组织数据，表由行和列组成，列划分为若干个列族

行：每个HBase表都由若干行组成，每个行由行键（row key）来标识。

列族：一个HBase表被分组成许多“列族”（Column Family）的集合，它是基本的访问控制单元

列限定符：列族里的数据通过列限定符（或列）来定位

单元格：在HBase表中，通过行、列族和列限定符确定一个“单元格”（cell），单元格中存储的数据没有数据类型，总被视为字节数组byte[]

时间戳：每个单元格都保存着同一份数据的多个版本，这些版本采用时间戳进行索引

“四维坐标”，即[行键, 列族, 列限定符, 时间戳]，如：

| 键                                            | 值            |
| --------------------------------------------- | ------------- |
| [“201505003”, “Info”, “email”, 1174184619081] | “xie@qq.com”  |
| [“201505003”, “Info”, “email”, 1174184620720] | “you@163.com” |

<img src="img/image-20230302165311810.png" alt="image-20230302165311810" style="zoom:67%;" />

<img src="img/image-20230302165339030.png" alt="image-20230302165339030" style="zoom:50%;" />

•HBase包括2个主要组件：

–（1）一个Master主服务器

–（2）多个Region服务器

主服务器Master负责管理和维护HBase表的分区信息，维护Region服务器列表，分配Region，负载均衡

Region服务器负责存储和维护分配给自己的Region，处理来自客户端的读写请求

客户端不是直接从Master主服务器上读取数据，而是在获得Region的存储位置信息后，直接从Region服务器上读取数据

客户端不依赖Master，通过Zookeeper来获得Region位置信息，大多数客户端不和Master通信，这种设计方式使得Master负载很小 

一个HBase表被划分成多个Region，一个Region会分裂成多个新的Region。开始只有一个Region，后来不断分裂。Region拆分操作非常快，接近瞬间，因为拆分之后的Region读取的仍然是原存储文件，直到“合并”过程把存储文件异步地写到独立的文件之后，才会读取新文件

<img src="img/image-20230302165638571.png" alt="image-20230302165638571" style="zoom:50%;" />

<img src="img/image-20230302165704734.png" alt="image-20230302165704734" style="zoom:50%;" />

每个Region默认大小是100MB到200MB（2006年以前的硬件配置）

每个Region的最佳大小取决于单台服务器的有效处理能力

目前每个Region最佳大小建议1GB-2GB（2013年以后的硬件配置）

同一个Region不会被分拆到多个Region服务器

每个Region服务器存储10-1000个Region

<img src="img/image-20230302175216536.png" alt="image-20230302175216536" style="zoom:50%;" />

元数据表，.META.表，存储Region和Region服务器的映射关系

HBase表很大时， .META.表会被分裂成多个Region

根数据表，又名-ROOT-表，记录所有元数据的具体位置

-ROOT-表只有唯一一个Region

Zookeeper文件记录了-ROOT-表的位置

<img src="img/image-20230302175511827.png" alt="image-20230302175511827" style="zoom:50%;" />

| **层次** | **名称**      | **作用**                                                     |
| -------- | ------------- | ------------------------------------------------------------ |
| 第一层   | Zookeeper文件 | 记录了-ROOT-表的位置信息                                     |
| 第二层   | -ROOT-表      | 记录了.META.表的Region位置信息  -ROOT-表只能有一个Region。通过-ROOT-表，就可以访问.META.表中的数据 |
| 第三层   | .META.表      | 记录了用户数据表的Region位置信息，.META.表可以有多个Region，保存了HBase中所有用户数据表的Region位置信息 |

为了加快访问速度，.META.表的全部Region都会被保存在内存中

假设.META.表的每行（一个映射条目）在内存中大约占用1KB，并且每个Region限制为128MB，那么，上面的三层结构可以保存的用户数据表的Region数目的计算方法是：

（-ROOT-表能够寻址的.META.表的Region个数）×（每个.META.表的 Region可以寻址的用户数据表的Region个数）

一个-ROOT-表最多只能有一个Region，也就是最多只能有128MB，按照每行（一个映射条目）占用1KB内存计算，128MB空间可以容纳128MB/1KB=$2^{17}$行，也就是说，一个-ROOT-表可以寻址这么多个.META.表的Region。

同理，每个.META.表的 Region可以寻址的用户数据表的Region个数是128MB/1KB。

最终，三层结构可以保存的Region数目是(128MB/1KB) × (128MB/1KB) = $2^{34}$个Region

•为了加速寻址，客户端会缓存位置信息，同时，需要解决缓存失效问题

寻址过程客户端只需要询问Zookeeper服务器，不需要连接Master服务器

<img src="img/image-20230302175929565.png" alt="image-20230302175929565" style="zoom:50%;" />

客户端包含访问HBase的接口，同时在缓存中维护着已经访问过的Region位置信息，用来加快后续数据访问过程

Zookeeper可以帮助选举出一个Master作为集群的总管，并保证在任何时刻总有唯一一个Master在运行，这就避免了Master的“单点失效”问题

Zookeeper是一个很好的集群管理工具，被大量用于分布式计算，提供配置维护、域名服务、分布式同步、组服务等。

<img src="img/image-20230302180034139.png" alt="image-20230302180034139" style="zoom: 50%;" />

主服务器Master主要负责表和Region的管理工作：

–管理用户对表的增加、删除、修改、查询等操作

–实现不同Region服务器之间的负载均衡

–在Region分裂或合并后，负责重新调整Region的分布

–对发生故障失效的Region服务器上的Region进行迁移

. Region服务器

–Region服务器是HBase中最核心的模块，负责维护分配给自己的Region，并响应用户的读写请求

行键是按照字典序存储，因此，设计行键时，要充分利用这个排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。

例：如果最近写入HBase表中的数据是最可能被访问的，可以考虑将时间戳作为行键的一部分，由于是字典序排序，所以可以使用Long.MAX_VALUE - timestamp作为行键，这样能保证新写入的数据在读取时可以被快速命中



#### 配置理论

HBase有三种运行模式，单机模式、伪分布式模式、分布式模式。
 以下先决条件很重要，比如没有配置JAVA_HOME环境变量，就会报错。
 – JDK
 – Hadoop( 单机模式不需要，伪分布式模式和分布式模式需要)
 – SSH

启动关闭Hadoop和HBase的顺序一定是：
 启动Hadoop—>启动HBase—>关闭HBase—>关闭Hadoop

HBASE_MANAGES_ZK=true，则由HBase自己管理Zookeeper

否则，启动独立的Zookeeper

建议：单机版HBase，使用自带Zookeeper；集群安装HBase则采用单独Zookeeper集群



•[HBase](http://c.biancheng.net/hbase/) 解决不了所有的问题，但是针对某些特点的数据可以使用 HBase 高效地解决，如以下的应用场景。数据模式是动态的或者可变的，且支持半结构化和非结构化的数据。

•数据库中的很多列都包含了很多空字段，在 HBase 中的空字段不会像在关系型数据库中占用空间。

•需要很高的吞吐量，瞬间写入量很大。

•数据有很多版本需要维护，HBase 利用时间戳来区分不同版本的数据。

•具有高可扩展性，能动态地扩展整个存储系统。

•在实际应用中，有很多公司使用 Hbase。

Facebook 公司的 Social Inbox 系统，使用 HBase 作为消息服务的基础存储设施，每月可处理几千亿条的消息；



### Hive

#### 概述

Hive本质上是一个SQL解析引擎，它将SQL语句转译成MapReduce作业并在Hadoop上执行。Hive执行过程如下，其工作原理如图

（1）用户通过用户接口连接Hive，发布Hive QL。
（2）Hive解析查询并制定查询计划。
（3）Hive将查询转换成MapReduce作业。
（4）Hive在Hadoop上执行MapReduce作业

<img src="img/image-20230308145817830.png" alt="image-20230308145817830" style="zoom: 50%;" />

Hive的体系架构如图所示，按功能主要分5大模块：用户接口、Thrift服务器、解析器、MetaStore元数据和Hadoop集群

<img src="img/image-20230308150059030.png" alt="image-20230308150059030" style="zoom:50%;" />

Hive包含以下四种数据模型：内部表（Managed Table）、外部表（External Table）、分区（Partition）和桶（Bucket）。Hive中的数据存储如下图所示

<img src="img/image-20230308150111724.png" alt="image-20230308150111724" style="zoom:50%;" />

（1）内部表（Managed Table）。
每个Hive内部表在HDFS中都有对应目录用来存储表的数据。
内部表的创建过程和数据加载过程可以分别独立完成，也可以在同一个语句中完成。
删除内部表时，表中的数据和元数据会被同时删除。
（2）Hive外部表（External Table）
Hive外部表和内部表在元数据组织上是一样的，但在实际数据存储上有较大差异。
外部表数据不存储在自己表所属目录中，存储在LOCATION指定的HDFS路径中。
外部表仅有一个过程，创建表和数据加载过程同时进行和完成。
删除外部表仅仅是删除外部表对应的元数据，外部表所指向的数据不会被删除。
创建外部表使用EXTERNAL关键字。

（3）分区（Partition）
分区是表的部分列的集合。
一般为频繁使用的数据建立分区，在查找分区中数据时不用扫描全表，有利于提高查找效率。
Hive每个表有一个相应的目录存储数据，表中的的每一个分区对应表目录下的一个子目录，每个分区中的数据存储在对应子目录下的文件中。例如，表member（假定包含分区字段gender）在HDFS的路径为/user/hive/warehouse/member，分区gender=F对应的HDFS路径为/user/hive/warehouse/ member/gender=F，分区gender=M对应的HDFS路径为/user/hive/warehouse/member/gender=M，当导入数据到分区gender=F，则数据存储在/user/hive/warehouse/member/gender=F/000000_0文件中，当导入数据到分区gender=M，则数据存储在/user/hive/warehouse/member/gender=M/000000_0文件中

（4） 桶（Bucket）
桶是将表的列通过Hash算法进一步分解成不同的文件存储。
对指定列计算hash值，根据hash值切分数据，目的是为了并行。
每一个桶对应一个文件（注意和分区的区别）。分区是粗粒度的划分，桶是细粒度的划分，这样可以让查询发生在小范围的数据上，提高查询效率，适合进行表连接查询，适合用于采样分析。比如，要将member表的id列分散至32个桶中，首先对id列的值进行Hash值计算，其中对应Hash值是0的数据存储在/hive/warehouse/member/000000_0文件中，对应Hash值是1的数据存储在/hive/warehouse/ member/000001_0文件中，依次类推。

#### 部署

Hive将metastore元数据存储在RDBMS中，如MySQL、Derby。按metastore存储位置的不同，其部署模式分为内嵌模式、本地模式和完全远程模式三种

##### 内嵌模式

内嵌模式是安装时默认部署模式
元数据信息被存储在Hive自带的数据库Derby中
所示所有组件（如数据库、元数据服务）运行在同一个进程内。
只允许建立一个连接，意味着同一时刻只支持一个用户访问和操作Hive。

<img src="img/image-20230308150909041.png" alt="image-20230308150909041" style="zoom:50%;" />

Hive是基于Hadoop的数据仓库技术，因此部署Hive前要预先部署完成Hadoop稳定版本的集群环境。hive-2.1.1二进制包的安装步骤如下。
（1）下载Hive安装包到/root/tools目录。
（2）解压二进制包到安装目录/usr/cstor，解压后生成子目录apache-hive-2.1.1-bin。
（3）将目录apache-hive-2.1.1-bin重命名为hive，方便后续操作。
（4）为Hive配置Hadoop安装路径。 
（5）在HDFS里新建Hive存储目录。
（6）初始化元数据库，启动Hive，进入Hive运行时环境。
内嵌模式下，启动Hive指的是启动Hive运行时环境。出现“hive>”提示表示Hive正常启动，否则部署失败。

##### 本地模式

本地模式的元数据服务运行在Hive服务主进程中。
当涉及元数据操作时，Hive服务中的元数据服务模块通过JDBC和存储于DB里的元数据数据库进行交互。
本地模式下MySQL数据库与Hive运行在同一台物理机器上
可提供多用户并发访问metastore服务。

<img src="img/image-20230308151038984.png" alt="image-20230308151038984" style="zoom:50%;" />

Hive本地模式安装步骤。
（1）完成Hive基本安装。
（2）安装MySQL程序。
（3）启动MySQL。
（4）创建存储Hive元数据的数据库HiveDB。
（5）退出MySQL，切换到hive的bin目录编辑hive-config.sh文件。

（6）切换到hive的conf目录创建和编辑hive-site.xml文件。
（7）把Java连接MySQL的驱动程序文件mysql-connector-java-x.y.z-bin.jar复制到hive的lib目录。
（8）初始化元数据库，启动Hive，进入Hive运行时环境。
注意：要确保Hadoop集群已经启动和MySQL服务运行正常才可以启动Hive。

##### 远程模式

远程模式的元数据信息是被存储在独立数据库中
元数据可能存储在另一台物理机上，甚至另一种操作系统上。
元数据服务以独立进程运行。
远程模式允许创建多个连接，提供多用户同时访问并操作Hive。

<img src="img/image-20230308151057972.png" alt="image-20230308151057972" style="zoom:50%;" />

远程模式的安装与本地模式类似，最主要的区别是在hive-site.xml配置文件中修改localhost为远程MySQL数据库的IP地址。

#### 配置

Hive启动时会读取相关配置信息，Hive配置可以调优HQL代码执行效率，Hive配置文件在conf子目录下，经常需要根据需求进行修改。Hive配置文件主要有两个。

1. 文件hive-env.sh。指定Hadoop安装路径。
2. 文件hive-site.xml。保存Hive运行时所需要的相关配置参数。
  HIVE_HOME/conf目录中的文件hive-default.xml.template保存着各个配置参数的默认值，用户可以通过在conf目录中创建hive-site.xml并新增特定参数的值来覆盖默认值。
  更改hive-site.xml文件中的配置属性的方法。
  （1）直接用命令vim hive-site.xml编辑hive-site.xml文件。
  （2）使用带-hiveconf选项的Hive命令。
  （3）使用set命令。

#### 接口

Hive主要提供Hive Shell接口、Hive Web接口、Hive API接口、Hcatalog接口、Pig接口和Beeline接口等。

##### Shell

Hive Shell接口运行在Hadoop集群环境上，提供在Hive Shell下执行类SQL命令相关HiveQL操作环境，是Hive提供的标准接口，也是开发者最常用命令行接口。
在Hive提示符后输入HiveQL命令，Hive Shell把HiveQL查询转换为一系列MapReduce作业对任务进行并行处理，然后返回处理结果。通过Shell接口，程序员和分析师很容易编写HiveQL来实现新建表和查询表操作。
安装完Hive后，输入“<HIVE_HOME>/bin/hive”或者“hive --service cli”回车即可启动Hive Shell。初次在Hive Shell下执行命令，会在执行命令操作的机器上创建metastore数据库。

Hive Shell操作有如下特点。
（1）命令必须以分号“;”结束，通知Hive开始执行相应的操作。
（2）命令不要求大小写敏感（除了进行字符串比较相关操作），因此，命令“show table;”将会产生与“SHOW TABLE;”相同的输出结果。
（3）支持Tab键命令自动补全功能，如在hive>提示符后输入SH或SHO按下Tab键会自动补齐为show，输入show ta按下Tab键会显示所有可能的命令。
（4）默认会输出执行过程信息，如执行查询操作所用时间，通过指定-S选项可以禁止输出此类信息，只输出HiveQL执行结果。
Hive Shell的常用命令如下。
（1）清屏。                                            （4）查看表结构。
（2）查看数据库中的表。                        （5）查看HDFS上的文件
（3）查看数据库中的内置函数。              （6）执行操作系统的命令

##### Web

Hive Web接口简称HWI（Hive Web Interface），是Hive Shell接口的一个替代方案（图形化实现），提供了更直观的Web界面，适合数据分析或数据运营人员做即席查询。用户通过浏览器来访问和操作Hive服务端，可以查看Hive数据库模式，执行HiveQL相关操作命令。在浏览器的地址栏输入http://<IP地址>:9999/hwi/回车实现访问Hive Web管理接口。
（1）配置HWI的步骤
打包war文件。
copy相关jar文件到$HIVE_HOME/lib目录下，
修改hive-site.xml配置文件
（2）启动HWI服务，在命令行下输入“hive --service hwi”启动Hive Web管理方式

#### SQL

Hive定义了简单的类SQL查询语句，称HiveQL，与大部分SQL语法兼容，方便熟悉SQL的开发者使用Hive开发和处理复杂的分析工作，还可以用HQL进行查询。

Hive支持基本类型和复杂类型，基本类型包括如下几种。
（1）整数类型：tinyint/smallint/int/bigint。
（2）浮点数类型：float/double/dicemal。
（3）布尔类型：boolean。
（4）字符串类型：string/varchar/char。

复杂类型包括如下几种。
（1）Array：数组类型，由一系列相同数据类型的元素组成。
（2）Map：集合类型，包括Key-Value键值对，可以通过Key来访问元素。
（3）Struct：结构类型，可以包含不同数据类型的元素，这些元素可以通过“点语法”的方式来得到所需要的元素。

DDL操作（Data Definition Language，数据定义语言），常用DDL语句。
（1） create/drop/alter数据库
（2） create/drop/truncate表
（3） alter表/分区/列
（4） Create/Drop/Alter视图
（5） Hive函数
Hive中内置了许多函数，比如日期操作函数day()、year()和month()等，数值操作函数sum()、avg()、max()、min()和count()等。但在某些特殊场景下，可能还是需要自定义函数满足特定功能，这时要用用户自定义函数UDF。

DML（Data Manipulation Language，数据操作语言），常见的操作包括下面几个方面。
（1）将文件中的数据导入（load）到Hive表中
（2） select和filters
（3）数据表连接join操作
（4）将select查询结果导出到Hive的另一个表中
（5）将select查询结果写入文件
写入本地文件系统
写入分布式文件系统HDFS

## 命令

### hadoop

#### 用户命令

| **命令**               | **功能**           |
| ---------------------- | ------------------ |
| bin/Hadoop             | 显示帮助           |
| bin/hadoop fs          | 文件操作           |
| bin/hadoop jar         | 运行MapReduce程序  |
| bin/hadoop version     | 查看Hadoop版本     |
| bin/hadoop checknative | 检查Hadoop的本地库 |

fs

| 命令                             | 功能                               |
| -------------------------------- | ---------------------------------- |
| -cat path/file                   | 输出文本文件的内容                 |
| -appendToFile 本地文件 集群文件  | 将本地文件的内容追加到集群文件结尾 |
| -copyFromLocal 本地文件 集群文件 | 将本地文件复制到集群               |
| -copyToLocal集群文件  本地文件   | 将集群文件复制到本地               |
| -cp 集群原文件 集群目标文件      | 复制集群文件                       |
| -mv 集群原文件 集群目标文件      | 移动或重命名文件                   |
| -ls 路径                         | 列出集群文件或者目录               |
| -mkdir 路径                      | 在集群中创建目录                   |
| -setrep [参数] [副本数] [路径]   | 设置文件副本数                     |



| 命令                                      | 功能                                                         |
| ----------------------------------------- | ------------------------------------------------------------ |
| hadoop fs  -mkdir /park                   | 在hdfs 的根目录下，创建 park目录                             |
| hadoop   fs -ls /                         | 查看hdfs根目录下有哪些目录                                   |
| hadoop fs  -put /root/1.txt /park         | 将linux操作系统root目录下的1.txt放在hdfs的park目录下         |
| hadoop fs -get   /park/jdk /home          | 把hdfs文件系统下park目录的文件下载到linux的home目录下        |
| hadoop fs -rm  /park/文件名               | 删除hdfs  的park目录的指定文件                               |
| hadoop fs  -rmdir /park                   | 删除park目录，但是前提目录里没有文件                         |
| hadoop fs  -rmr /park                     | 删除park目录，即使目录里有文件                               |
| hadoop fs  -cat /park/a.txt               | 查看park目录下的a.txt文件                                    |
| hadoop fs  -tail /park/a.txt              | 查看park目录下a.txt文件末尾的数据                            |
| haddop  jar xxx.jar                       | 执行jar包                                                    |
| hadoop fs  -cat /park/result/part-r-00000 | 查看  /park/result/part-r-00000文件的内容                    |
| hadoop   fs  –mv  /park02 /park01         | 将HDFS上的park02目录重名为park01命令。                       |
| hadoop fs -mv /park02/1.txt /park01       | 将park02目录下的1.txt移动到/park01目录下                     |
| hadoop fs  -touchz /park/2.txt            | 创建一个空文件                                               |
| hadoop fs  -getmerge /park /root/tmp      | 将park目录下的所有文件合并成一个文件，并下载到linux的root目录下的tmp目录 |



#### 管理命令

|                                                           |                                                              |
| --------------------------------------------------------- | ------------------------------------------------------------ |
| hadoop  dfsadmin -safemode leave                          | 离开hadoop安全模式                                           |
| hadoop  dfsadmin -safemode enter                          | 进入安全模式                                                 |
| hadoop  dfsadmin -rollEdits                               | 手动执行fsimage文件和Edis文件合并元数据                      |
| hadoop  dfsadmin -report                                  | 查看存活的datanode节点信息                                   |
| hadoop fsck  /park                                        | 汇报/park目录  健康状况                                      |
| hadoop fsck  /park/1.txt -files -blocks -locations -racks | 查看1.txt  这个文件block信息以及机架信息      ![IMG_256](file:///C:/Users/lr580/AppData/Local/Temp/msohtmlclip1/01/clip_image002.jpg) |
| hadoop fs  -expunge                                       | 手动清空hdfs回收站                                           |

#### 日志

调整日志级别：

```sh
bin/hadoop daemonlog -getlevel <host:httpport> <classname>
bin/hadoop daemonlog -setlevel <host:httpport> <classname> <level>
```

`DEBUG < INFO < WARN < ERROR < FATAL`

`http://<hostname>:50070/logLevel`

#### 启动关闭

| **命令**                                     | **功能**                             |
| -------------------------------------------- | ------------------------------------ |
| \$sbin/start-all.sh  $sbin/stop-all.sh       | 启动集群所有服务/关闭集群所有服务    |
| \$sbin/start-dfs.sh $sbin/stop-dfs.sh        | 启动dfs;关闭dfs                      |
| \$sbin/start-yarn.sh  $sbin/stop-yarn.sh     | 启动YARN;关闭YARN                    |
| $sbin/hadoop-daemon.sh  [start\|stop] 服务名 | 单个Hadoop服务启动或者关闭           |
| $sbin/hadoop-daemons.sh [start\|stop] 服务名 | 全部slaves上的Hadoop服务启动或者关闭 |
| $sbin/yarn-daemon.sh [start\|stop] 服务名    | 单个yarn服务的启动或者关闭           |
| $sbin/yarn-daemons.sh [start\|stop] 服务名   | 全部slaves上的yarn服务启动或者关闭   |
| $bin/hdfs secondarynamenode                  | 以控制台的方式启动SecondaryNameNode  |
| $bin/hdfs namenode                           | 以控制台的方式启动NameNode           |
| $bin/hdfs datanode                           | 以控制台的方式启动DataNode           |



#### 其他

|                                              |                                                              |
| -------------------------------------------- | ------------------------------------------------------------ |
| hadoop fs -cp   /park01/1.txt /park02        | 将HDFS上 /park01下的1.txt拷贝一份到 /park02目录下。   目标路径可以有多个，用空格隔开，比如：hadoop fs -cp /park01/1.txt /park02   /park03…… |
| hadoop fs -du  /park/1.txt                   | 查看HDFS上某个文件的大小。也可以查看指定目录，如果是目录的话，则列出目录下所有的文件及其大小，比如：hadoop fs -du /park |
| hadoop fs -copyFromLocal /home/1.txt /park01 | 将本地文件1.txt上传到/park01目录下                           |
| hadoop fs -lsr   /                           | 递归查看指定目录下的所有内容                                 |
| hadoop fs  -copyToLocal /park01/1.txt /home  | 将HDFS上的1.txt 拷贝到本地文件系统                           |

### hdfs

#### 用户命令

| **命令**                     | **功能**                                 |
| ---------------------------- | ---------------------------------------- |
| $bin/hdfs                    | 显示帮助                                 |
| $bin/hdfs dfs [参数]         | 文件操作，与$bin/hadoop fs的参数完全一样 |
| $bin/hdfs namenode  -format  | 格式化NameNode                           |
| $bin/hdfs getconf            | 从配置文件中获取配置信息                 |
| $bin/hdfs fsck [路径] [参数] | 处理损坏的文件                           |

fsck 参数：

| 参数                      | 功能                                             |
| ------------------------- | ------------------------------------------------ |
| -list-corruptfileblocks   | 输出损坏的文件及丢失的块                         |
| -move                     | 将文件移动到/lost+found目录                      |
| -delete                   | 删除损坏的文件                                   |
| -openforwrite             | 输出以写方式打开的文件                           |
| -files                    | 输出该目录及子目录下所有文件的状态               |
| -files -blocks            | 输出该目录及子目录下所有文件的块信息             |
| -files –blocks -locations | 输出该目录及子目录下所有文件在DataNode的存储信息 |
| -files -blocks -racks     | 输出该目录及子目录下所有文件机架感知信息         |



#### 管理命令

| **命令**                                                     | **功能**                              |
| ------------------------------------------------------------ | ------------------------------------- |
| $bin/hdfs dfsadmin -report                                   | 查看HDFS的基本统计信息                |
| $bin/hdfs dfsadmin -safemode <enter \| leave \| get \| wait> | 配置安全模式                          |
| $bin/hdfs dfsadmin -saveNamespace                            | 将内存信息保存到磁盘，并重置edits文件 |
| $bin/hdfs dfsadmin - refreshNodes                            | 刷新节点和排除文件                    |
| $bin/hdfs dfsadmin – setBalancerBandwidth [byte per second]  | 设置负载均衡带宽                      |
| $bin/hdfs secondarynamenode [参数]                           | 操作SecondaryNameNode                 |
| $bin/hdfs balancer                                           | 平衡集群中DataNode的数据              |



### hbase

#### shell命令

进入：`hbase shell`。退出 `exit`。

查看帮助 `help '命令名'`，如 `help 'create'`。

列出所有表 `list`

> 列出所有命名空间 `list_namespace`

查看表的字段：`describe '表名'`

查看表是否存在 `exists '表名'`

查询表的信息 `scan '表名'`

查询数据 `get '表名', '行名', '列族:列名'`

创建表 `create`

- 创建一个表，只有一个列族：`create 'test', 'cf'`

添加数据 `put`。在添加数据时，HBase会自动为添加的数据添加一个时间戳，当然，也可以在添加数据时人工指定时间戳的值。

- `put 表名, 行id, 列族:列名, 值`。如 `put 't1', 'r1', 'f1:a', 'baicha'`

  新的 put 会覆盖掉旧的，所以 put 可以增改

- 为了存多个时间戳版本，先设置只保留最新的两个版本数据：

  `alter 't1', NAME=>'f1', VERSIONS=>2`

  再增加带时间戳的数据 `put 't1', 'r1', 'f1:a', 'guodong', 1677840899646`

  查询 `get 't1','r1', COLUMN=>'f1:a',VERSIONS=>2`

- 删除，put 改 delete

使表有效或无效 `enable/disable '表名'`

删除表 `drop '表名'`。删除前必须 disable。



### hive

#### 部署

hive不需要所有节点都安装

加载指令：

Load data local inpath '/home/stu.txt'  into table student;

Navicat连接到mysql：

对应数据库名称：HIveDB； hive-site.xml中设置

表DBS: 存放hive中有哪些数据库

#### 基础

| **命令**                                                     | **作用**                                     | **额外说明**                                                 |
| ------------------------------------------------------------ | -------------------------------------------- | ------------------------------------------------------------ |
| show databases;                                              | 查看都有哪些数据库                           |                                                              |
| create  database park;                                       | 创建park数据库                               | 创建的数据库，实际是在Hadoop的HDFS文件系统里创建一个目录节点，统一存在： /user/hive/warehouse 目录下 |
| use park;                                                    | 进入park数据库                               |                                                              |
| show  tables;                                                | 查看当前数据库下所有表                       |                                                              |
| create  table stu   (id int,name string);                    | 创建stu表，以及相关的两个字段                | 1. hive里，表示字符串用的是string，不用char和varchar  2. 所创建的表，也是HDFS里的一个目录节点 |
| insert into  stu values(1,'zhang')                           | 向stu表插入数据                              | 1.      HDFS不支持数据的修改和删除，因此已经插入的数据不能够再进行任何的改动  2.      在Hadoop2.0版本后支持了数据追加。实际上，insert into 语句执行的是追加操作  3.      hive支持查询，行级别的插入。不支持行级别的删除和修改  4.      hive的操作实际是执行一个job任务，调用的是Hadoop的MR  5.      插入完数据之后，发现HDFS stu目录节点下多了一个文件，文件里存了插入的数据，因此，hive存储的数据，是通过HDFS的文件来存储的。 |
| select *  from stu                                           | 查看表数据                                   | 也可以根据字段来查询，比如select id from stu                 |
| drop table  stu                                              | 删除表                                       |                                                              |
| select * from stu                                            | 查询stu表数据                                |                                                              |
| load data  local inpath '/home//stu.txt' into table stu;     | 通过加载文件数据到指定的表里                 | 1.      在执行完这个指令之后，发现hdfs stu目录下多了一个txt文件。由此可见，hive的工作原理实际上就是在管理hdfs上的文件，把文件里数据抽象成二维表结构，然后提供hql语句供程序员查询文件数据  2.      可以做这样的实验：不通过load 指令，而通过插件向stu目录下再上传一个文件，看下hive是否能将数据管理到stu表里。 |
| create  table stu1(id int,name string) row format delimited fields terminated by  ' '; | 创建stu1表，并指定分割符 空格。              |                                                              |
| desc stu                                                     | 查看 stu表结构                               |                                                              |
| create  table stu2 like stu                                  | 创建一张stu2表，表结构和stu表结构相同        | like只复制表结构，不复制数据                                 |
| insert overwrite   table stu2 select * from stu              | 把stu表数据插入到stu2表中                    |                                                              |
| insert  overwrite local directory '/home/stu' row format delimited fields terminated  by '   ' select * from stu; | 将stu表中查询的数据写到本地的/home/stu目录下 |                                                              |
| insert  overwrite directory '/stu' row format delimited fields terminated by '   ' select * from stu; | 将stu表中查询的数据写到HDFS的stu目录下       |                                                              |
| from stu  insert overwrite table stu1 select * insert overwrite table stu2 select *; | 将stu表中查询的数据写到stu1以及stu2两张表中  |                                                              |
| alter  table stu rename to stu2                              | 为表stu重命名为stu2                          |                                                              |
| alter table  stu add columns (age int);                      | 为表stu增加一个列字段age，类型为int          |                                                              |
| exit  或 quit                                                | 退出hive                                     |                                                              |

#### 表结构

##### 内外部表

\1)    内部表：先在hive里建一张表，然后向这个表插入数据（用insert可以插入数据，也可以通过加载外部文件方式来插入数据），这样的表称之为hive的内部表

\2)    外部表：HDFS里已经有数据了，然后，通过hive创建一张表来管理这个文件数据。则这样表称之为外部表。需要注意的是，hive外部表管理的是HDFS里的某一个目录下的文件数据

外部表创建

```hive
create external table stu (id int,name string) row format delimited fields terminated by  '  '  location   '/目录路径'
```

\1)    对于内部表，在删除该表的时候，HDFS对应的目录节点会被删除

\2)    对于外部表，在删除该表的时候，HDFS对应的目录节点不会删除

##### 分区表

\1)    分区表可以通过添加指定的字段来提高Hive的查询效率

\2)    在数据量较大的情况下，往往会添加分区表来避免**全表**查询

| 指令                                                         | 作用                                         | 额外说明                                                     |
| ------------------------------------------------------------ | -------------------------------------------- | ------------------------------------------------------------ |
| create table book (id int, name string) **partitioned** by (category  string)   row format delimited fields terminated by '\t'; | 创建book表，以category作为分区               | 在创建分区表时，partitioned字段可以不在字段列表中。生成的表中自动就会具有该字段。 |
| load data local inpath '/home/cn.txt' overwrite into table book  partition (category='cn'); | 将本地文件cn.txt添加到book表中，分区字段为cn | 在HDFS下生成category=cn目录                                  |
| select * from book where category='cn';                      | 查看分区为cn的数据                           |                                                              |
| ALTER TABLE book add PARTITION  (category = 'jp') location '/user/hive/warehouse/park.db/book/category=jp'; | 将指定的目录添加为分区字段                   |                                                              |
| show partitions iteblog;                                     | 查看分区                                     |                                                              |
| msck repair table book;                                      | 修复分区                                     |                                                              |
| alter table book drop partition(category='cn');              | 删除分区                                     |                                                              |
| alter table book partition(category='french') rename to partition  (category='hh'); | 修改分区的名字                               |                                                              |

##### 分桶表

\1)    分桶表是一种更细粒度的数据分配方式

\2)    一个表既可以分区也可以分桶

\3)    分桶的主要作用是实现数据的抽样，方便进行数据测试

\4)    分桶表通过hash分桶算法，将数据分放在不同的桶(hdfs中的文件)中，方便后续获取

\5)    分桶表机制默认是不开启的，需要手动开启：set hive.enforce.bucketing=true;

\6)    分桶表不允许以外部文件方式导入数据，只能从另外一张表数据导入

| 指令                                                         | 作用                                         | 额外说明                                                     |
| ------------------------------------------------------------ | -------------------------------------------- | ------------------------------------------------------------ |
| create table teacher(name string) clustered by (name) into 3 buckets **row format delimited  fields terminated by ' '**; | 创建teacher表，以name作为分桶机制，分为3个桶 |                                                              |
| insert overwrite table teacher select * from tmp;            | 将tmp表中的数据添加到teacher表中             | 实际上是产生了3个文件用于存储不分桶的数据                    |
| select * from teacher tablesample(bucket 1 out of 3 on name); | 进行抽样                                     | 抽样格式为：bucket x out of y on XXX  1. x表示抽样的起始桶，例如bucket 1 out of 3表示从第1 个桶开始抽取数据  2. y决定抽样的比例，要求必须是桶数的因子或者整数倍  3. 如果桶数为6，y为2，则表示抽取6/2=3个桶中的数据  4. 如果桶数为6，y为3，则表示抽取6/3=2个桶中的数据  5. 如果桶数为6，y为12，则表示抽取6/12=0.5个桶中的数据  6. 如果桶数为6，抽样为bucket 1 out of 3 on  id表示从第1个桶开始抽样，抽取2个桶的数据，所以抽取的样本为1和4桶中的数据 |

<img src="img/image-20230308151906975.png" alt="image-20230308151906975" style="zoom:50%;" />

#### 数据类型

| Hive中的类型 | Java中的类型 |
| ------------ | ------------ |
| tinyint      | byte         |
| smallint     | short        |
| int          | int          |
| bigint       | long         |
| boolean      | boolean      |
| float        | float        |
| double       | double       |
| string       | String       |
| timestamp    | TimeStamp    |
| binary       | byte[]       |

##### array

```
100,200,300 
200,300,500
```

```hive
create external table ex(vals array<int>) row format delimited fields terminated by '\t' collection items terminated by ',' location '/ex';
```

查询每行数组的个数，查询语句：

```hive
select size(vals) from ex;
```

注：hive 内置函数不具备查询某个具体行的数组元素。需要自定义函数来实现，但这样的需求在实际开发里很少，所以不需要在意。



```
100,200,300      tom,jary
200,300,500      rose,jack
```

```hive
create external table ex1(info1 array<int>,info2 array<string>) row format delimited fields terminated by '\t' collection items terminated by ',' location '/ex';
```



##### map

```
tom,23
rose,25
jary,28
```

```hive
create external table m1 (vals map<string,int>) row format delimited fields terminated by '\t' map keys terminated by ',' location '/map';
```

```hive
select vals['tom'] from m1;
```



```
tom 192.168.234.21
rose 192.168.234.21
tom 192.168.234.22
jary 192.168.234.21
tom 192.168.234.24
tom 192.168.234.21
rose 192.168.234.21
tom 192.168.234.22
jary 192.168.234.21
tom 192.168.234.22
tom 192.168.234.23
```

map类型，列的分割符必须是\t

```hive
create external table ex (vals map<string,string>) row format delimited fields terminated by '\t' map keys terminated by ' ' location '/ex';
```

```hive
select vals['tom'] from ex where vals['tom'] is not null;
```

去重：

```hive
select distinct(ip) from (select vals['tom'] ip from ex where vals['tom'] is not null)ex1;
```

```hive
select distinct(vals['tom']) from m2 where vals['tom'] is not null;
```



##### struct

```
tom 23
rose 22
jary 26
```

```hive
create external table ex (vals struct<name:string,age:int>)row format delimited collection items terminated by ' '  location '/ex';
```

```hive
select vals.age from ex where vals.name='tom';
```



#### 运算符

##### 关系

| 运算符        | 类型         | 说明                                                         |
| ------------- | ------------ | ------------------------------------------------------------ |
| A = B         | 所有原始类型 | 如果A与B相等，返回true，否则返回false                        |
| A == B        | 无           | 失败，因为无效的语法。 SQL使用”=”，不使用”==”。              |
| A <> B        | 所有原始类型 | 如果A不等于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。 |
| A < B         | 所有原始类型 | 如果A小于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。 |
| A <= B        | 所有原始类型 | 如果A小于等于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。 |
| A > B         | 所有原始类型 | 如果A大于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。 |
| A >= B        | 所有原始类型 | 如果A大于等于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。 |
| A IS NULL     | 所有类型     | 如果A值为”NULL”，返回TRUE,否则返回FALSE                      |
| A IS NOT NULL | 所有类型     | 如果A值不为”NULL”，返回TRUE,否则返回FALSE                    |
| A LIKE B      | 字符串       | 如果A或B值为”NULL”，结果返回”NULL”。字符串A与B通过sql进行匹配，如果相符返回TRUE，不符返回FALSE。B字符串中  的”_”代表任一字符，”%”则代表多个任意字符。例如： (‘foobar’ like ‘foo’)返回FALSE，（  ‘foobar’ like ‘foo_ _ _’或者 ‘foobar’ like ‘foo%’)则返回TURE |
| A RLIKE B     | 字符串       | 如果A或B值为”NULL”，结果返回”NULL”。字符串A与B通过java进行匹配，如果相符返回TRUE，不符返回FALSE。例如：（ ‘foobar’ rlike ‘foo’）返回FALSE，（’foobar’  rlike ‘^f.*r$’ ）返回TRUE。 |
| A REGEXP B    | 字符串       | 与RLIKE相同。                                                |

##### 算术

| 运算符 | 类型         | 说明                                                         |
| ------ | ------------ | ------------------------------------------------------------ |
| A + B  | 所有数字类型 | A和B相加。结果的与操作数值有共同类型。例如每一个整数是一个浮点数，浮点数包含整数。所以，一个浮点数和一个整数相加结果也是一个浮点数。 |
| A – B  | 所有数字类型 | A和B相减。结果的与操作数值有共同类型。                       |
| A * B  | 所有数字类型 | A和B相乘，结果的与操作数值有共同类型。需要说明的是，如果乘法造成溢出，将选择更高的类型。 |
| A / B  | 所有数字类型 | A和B相除，结果是一个double（双精度）类型的结果。             |
| A % B  | 所有数字类型 | A除以B余数与操作数值有共同类型。                             |
| A & B  | 所有数字类型 | 运算符查看两个参数的二进制表示法的值，并执行按位”与”操作。两个表达式的一位均为1时，则结果的该位为 1。否则，结果的该位为 0。 |
| A\|B   | 所有数字类型 | 运算符查看两个参数的二进制表示法的值，并执行按位”或”操作。只要任一表达式的一位为 1，则结果的该位为 1。否则，结果的该位为 0。 |
| A ^ B  | 所有数字类型 | 运算符查看两个参数的二进制表示法的值，并执行按位”异或”操作。当且仅当只有一个表达式的某位上为 1 时，结果的该位才为 1。否则结果的该位为 0。 |
| ~A     | 所有数字类型 | 对一个表达式执行按位”非”（取反）。                           |

##### 逻辑

| 运算符  | 类型   | 说明                                                         |
| ------- | ------ | ------------------------------------------------------------ |
| A AND B | 布尔值 | A和B同时正确时,返回TRUE,否则FALSE。如果A或B值为NULL，返回NULL。 |
| A && B  | 布尔值 | 与”A AND B”相同                                              |
| A OR B  | 布尔值 | A或B正确,或两者同时正确返返回TRUE,否则FALSE。如果A和B值同时为NULL，返回NULL。 |
| A \| B  | 布尔值 | 与”A OR B”相同                                               |
| NOT A   | 布尔值 | 如果A为NULL或错误的时候返回TURE，否则返回FALSE。             |
| ! A     | 布尔值 | 与”NOT A”相同                                                |



#### 函数

\1.   Hive实现了标准的sql，但在这之外，为了提升hive处理数据的能力，还额外提供了很多内置的函数，这些内置函数非常丰富，且可以直接使用，虽然不属于sql原生的语法，但大大的增强了hive处理数据的能力，是hive功能的重要组成部分

\2.   Hive除了有内置函数，还允许用户自定义函数

##### 数学

| 返回类型   | 函数                                               | 说明                                                         |
| ---------- | -------------------------------------------------- | ------------------------------------------------------------ |
| BIGINT     | round(double a)                                    | 四舍五入                                                     |
| DOUBLE     | round(double a, int  d)                            | 小数部分d位之后数字四舍五入，例如round(21.263,2),返回21.26   |
| BIGINT     | floor(double a)                                    | 对给定数据进行向下舍入最接近的整数。例如floor(21.2),返回21。 |
| BIGINT     | ceil(double a),  ceiling(double a)                 | 将参数向上舍入为最接近的整数。例如ceil(21.2),返回23.         |
| double     | rand(), rand(int  seed)                            | 返回大于或等于0且小于1的平均分布随机数（依重新计算而变）     |
| double     | exp(double a)                                      | 返回e的n次方                                                 |
| double     | ln(double a)                                       | 返回给定数值的自然对数                                       |
| double     | log10(double a)                                    | 返回给定数值的以10为底自然对数                               |
| double     | log2(double a)                                     | 返回给定数值的以2为底自然对数                                |
| double     | log(double base,  double a)                        | 返回给定底数及指数返回自然对数                               |
| double     | pow(double a, double  p) power(double a, double p) | 返回某数的乘幂                                               |
| double     | sqrt(double a)                                     | 返回数值的平方根                                             |
| string     | bin(BIGINT a)                                      | 返回二进制格式                                               |
| string     | hex(BIGINT a)  hex(string a)                       | 将整数或字符转换为十六进制格式                               |
| string     | unhex(string a)                                    | 十六进制字符转换由数字表示的字符。                           |
| string     | conv(BIGINT num, int  from_base, int to_base)      | 将指定数值，由原来的度量体系转换为指定的试题体系。例如CONV(‘a’,16,2),返回 |
| double     | abs(double a)                                      | 取绝对值                                                     |
| int double | pmod(int a, int b)  pmod(double a, double b)       | 返回a除b的余数的绝对值                                       |
| double     | sin(double a)                                      | 返回给定角度的正弦值                                         |
| double     | asin(double a)                                     | 返回x的反正弦，即是X。如果X是在-1到1的正弦值，返回NULL。     |
| double     | cos(double a)                                      | 返回余弦                                                     |
| double     | acos(double a)                                     | 返回X的反余弦，即余弦是X，，如果-1<= A <= 1，否则返回null.   |
| int double | positive(int a)  positive(double a)                | 返回A的值，例如positive(2)，返回2。                          |
| int double | negative(int a)  negative(double a)                | 返回A的相反数，例如negative(2),返回-2。                      |

##### 类型转换

| 返回类型    | 函数                  | 说明                                                         |
| ----------- | --------------------- | ------------------------------------------------------------ |
| 指定 “type” | cast(expr as  <type>) | 类型转换。例如将字符”1″转换为整数:cast(’1′ as  bigint)，如果转换失败返回NULL。 |

##### 日期

| 返回类型 | 函数                                             | 说明                                                         |
| -------- | ------------------------------------------------ | ------------------------------------------------------------ |
| string   | from_unixtime(bigint  unixtime[, string format]) | UNIX_TIMESTAMP参数表示返回一个值’YYYY- MM – DD HH：MM：SS’或YYYYMMDDHHMMSS.uuuuuu格式，这取决于是否是在一个字符串或数字语境中使用的功能。该值表示在当前的时区。 |
| bigint   | unix_timestamp()                                 | 如果不带参数的调用，返回一个Unix时间戳（从’1970- 01 – 0100:00:00′到现在的UTC秒数）为无符号整数。 |
| bigint   | unix_timestamp(string  date)                     | 指定日期参数调用UNIX_TIMESTAMP（），它返回参数值’1970- 01 – 0100:00:00′到指定日期的秒数。 |
| bigint   | unix_timestamp(string  date, string pattern)     | 指定时间输入格式，返回到1970年秒数：unix_timestamp(’2009-03-20′, ‘yyyy-MM-dd’) = 1237532400 |
| string   | to_date(string  timestamp)                       | 返回时间中的年月日：  to_date(“1970-01-01 00:00:00″)  = “1970-01-01″ |
| string   | to_dates(string  date)                           | 给定一个日期date，返回一个天数（0年以来的天数）              |
| int      | year(string date)                                | 返回指定时间的年份，范围在1000到9999，或为”零”日期的0。      |
| int      | month(string date)                               | 返回指定时间的月份，范围为1至12月，或0一个月的一部分，如’0000-00-00′或’2008-00-00′的日期。 |
| int      | day(string date)  dayofmonth(date)               | 返回指定时间的日期                                           |
| int      | hour(string date)                                | 返回指定时间的小时，范围为0到23。                            |
| int      | minute(string date)                              | 返回指定时间的分钟，范围为0到59。                            |
| int      | second(string date)                              | 返回指定时间的秒，范围为0到59。                              |
| int      | weekofyear(string  date)                         | 返回指定日期所在一年中的星期号，范围为0到53。                |
| int      | datediff(string  enddate, string startdate)      | 两个时间参数的日期之差。                                     |
| int      | date_add(string  startdate, int days)            | 给定时间，在此基础上加上指定的时间段。                       |
| int      | date_sub(string  startdate, int days)            | 给定时间，在此基础上减去指定的时间段。                       |

##### 条件

| 返回类型 | 函数                                                        | 说明                                                         |
| -------- | ----------------------------------------------------------- | ------------------------------------------------------------ |
| T        | if(boolean  testCondition, T valueTrue, T valueFalseOrNull) | 判断是否满足条件，如果满足返回一个值，如果不满足则返回另一个值。 |
| T        | COALESCE(T v1, T v2,  …)                                    | 返回一组数据中，第一个不为NULL的值，如果均为NULL,返回NULL。  |
| T        | CASE a WHEN b THEN c  [WHEN d THEN e]* [ELSE f] END         | 当a=b时,返回c；当a=d时，返回e，否则返回f。                   |
| T        | CASE WHEN a THEN b  [WHEN c THEN d]* [ELSE e] END           | 当值为a时返回b,当值为c时返回d。否则返回e。                   |

##### 字符串

| 返回类型                     | 函数                                                         | 说明                                                         |
| ---------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| int                          | length(string A)                                             | 返回字符串的长度                                             |
| string                       | reverse(string A)                                            | 返回倒序字符串                                               |
| string                       | concat(string A,  string B…)                                 | 连接多个字符串，合并为一个字符串，可以接受任意数量的输入字符串 |
| string                       | concat_ws(string  SEP, string A, string B…)                  | 链接多个字符串，字符串之间以指定的分隔符分开。               |
| string                       | substr(string A, int  start) substring(string A, int start)  | 从文本字符串中指定的起始位置后的字符。                       |
| string                       | substr(string A, int  start, int len) substring(string A, int start, int len) | 从文本字符串中指定的位置指定长度的字符。                     |
| string                       | upper(string A)  ucase(string A)                             | 将文本字符串转换成字母全部大写形式                           |
| string                       | lower(string A)  lcase(string A)                             | 将文本字符串转换成字母全部小写形式                           |
| string                       | trim(string A)                                               | 删除字符串两端的空格，字符之间的空格保留                     |
| string                       | ltrim(string A)                                              | 删除字符串左边的空格，其他的空格保留                         |
| string                       | rtrim(string A)                                              | 删除字符串右边的空格，其他的空格保留                         |
| string                       | regexp_replace(string  A, string B, string C)                | 字符串A中的B字符被C字符替代                                  |
| string                       | regexp_extract(string  subject, string pattern, int index)   | 通过下标返回正则表达式指定的部分。regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’,  2) returns ‘bar.’ |
| string                       | parse_url(string  urlString, string partToExtract [, string keyToExtract]) | 返回URL指定的部分。parse_url(‘http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1′, ‘HOST’) 返回：’facebook.com’ |
| string                       | get_json_object(string  json_string, string path)            | select a.timestamp,  get_json_object(a.appevents, ‘$.eventid’), get_json_object(a.appenvets, ‘$.eventname’) from log a; |
| string                       | space(int n)                                                 | 返回指定数量的空格                                           |
| string                       | repeat(string str,  int n)                                   | 重复N次字符串                                                |
| int                          | ascii(string str)                                            | 返回字符串中首字符的数字值                                   |
| string                       | lpad(string str, int  len, string pad)                       | 返回指定长度的字符串，给定字符串长度小于指定长度时，由指定字符从左侧填补。 |
| string                       | rpad(string str, int  len, string pad)                       | 返回指定长度的字符串，给定字符串长度小于指定长度时，由指定字符从右侧填补。 |
| array                        | split(string str,  string pat)                               | 将字符串转换为数组。                                         |
| int                          | find_in_set(string  str, string strList)                     | 返回字符串str第一次在strlist出现的位置。如果任一参数为NULL,返回NULL；如果第一个参数包含逗号，返回0。 |
| array<array<string>>         | sentences(string  str, string lang, string locale)           | 将字符串中内容按语句分组，每个单词间以逗号分隔，最后返回数组。 例如sentences(‘Hello there! How are you?’) 返回：( (“Hello”, “there”), (“How”, “are”, “you”) ) |
| array<struct<string,double>> | ngrams(array<array<string>>,  int N, int K, int pf)          | SELECT  ngrams(sentences(lower(tweet)), 2, 100 [, 1000]) FROM twitter; |
| array<struct<string,double>> | context_ngrams(array<array<string>>,  array<string>, int K, int pf) | SELECT context_ngrams(sentences(lower(tweet)),  array(null,null), 100, [, 1000]) FROM twitter; |

##### 聚合

| 返回类型                 | 函数                                                         | 说明                                                         |
| ------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| bigint                   | count(*) ,  count(expr), count(DISTINCT expr[, expr_., expr_.]) | 返回记录条数。                                               |
| double                   | sum(col),  sum(DISTINCT col)                                 | 求和                                                         |
| double                   | avg(col), avg(DISTINCT  col)                                 | 求平均值                                                     |
| double                   | min(col)                                                     | 返回指定列中最小值                                           |
| double                   | max(col)                                                     | 返回指定列中最大值                                           |
| double                   | var_pop(col)                                                 | 返回指定列的方差                                             |
| double                   | var_samp(col)                                                | 返回指定列的样本方差                                         |
| double                   | stddev_pop(col)                                              | 返回指定列的偏差                                             |
| double                   | stddev_samp(col)                                             | 返回指定列的样本偏差                                         |
| double                   | covar_pop(col1,  col2)                                       | 两列数值协方差                                               |
| double                   | covar_samp(col1,  col2)                                      | 两列数值样本协方差                                           |
| double                   | corr(col1, col2)                                             | 返回两列数值的相关系数                                       |
| double                   | percentile(col, p)                                           | 返回数值区域的百分比数值点。0<=P<=1,否则返回NULL,不支持浮点型数值。 |
| array<double>            | percentile(col,  array(p~1,,\ [, p,,2,,]…))                  | 返回数值区域的一组百分比值分别对应的数值点。0<=P<=1,否则返回NULL,不支持浮点型数值。 |
| double                   | percentile_approx(col,  p[, B])                              | Returns an  approximate p^th^ percentile of a numeric column (including floating point  types) in the group. The B parameter controls approximation accuracy at the  cost of memory. Higher values yield better approximations, and the default is  10,000. When the number of distinct values in col is smaller than B, this  gives an exact percentile value. |
| array<double>            | percentile_approx(col,  array(p~1,, [, p,,2_]…) [, B])       | Same as above, but  accepts and returns an array of percentile values instead of a single one. |
| array<struct\{‘x’,'y’\}> | histogram_numeric(col,  b)                                   | Computes a histogram  of a numeric column in the group using b non-uniformly spaced bins. The  output is an array of size b of double-valued (x,y) coordinates that  represent the bin centers and heights |
| array                    | collect_set(col)                                             | 返回无重复记录                                               |

##### explode

explode函数可以将一个array或者map展开，其中explode(array)使得结果中将array列表里的每个元素生成一行；explode(map)使得结果中将map里的每一对元素作为一行，key为一列，value为一列.

1．explode 命令可以将行数据，按指定规则切分出多行

2．用explode做行切分，注意表里只有一列，并且行数据是string类型，因为只有字符类型才能做切分

```
100,200,300
200,300,500
```

要将上面两行数据根据逗号拆分成多行（每个数字占一行）

上传HDFS，并创建对应的外部表，执行：

```hive
create external table ex1 (num string) location '/ex';
```

切分：

```hive
select explode(split(num,',')) from ex1;
```



##### UDF

自定义函数

\1.    如果hive的内置函数不够用，用户也可以自己定义函数来使用，这样的函数称为hive的用户自定义函数，简称UDF

\2.    UDF使得Hive的可扩展性增强



\1.    新建java工程，导入hive相关包，导入hive相关的lib

\2.    创建类继承UDF

\3.    自己编写一个evaluate方法，返回值和参数任意。

```java
import org.apache.hadoop.hive.ql.exec.UDF;
public class ToUpper extends UDF{
    public String evaluate(String str){
        return str.toUpperCase();
    }
}
```

\4.    为了能让mapreduce处理，String要用Text处理。

\5.    将写好的类打成jar包，上传到linux中

\6.    在hive命令行下，向hive注册UDF：add jar /xxxx/xxxx.jar

\7.    在hive命令行下，为当前udf起一个名字：create temporary function fname as '类的全路径名';

\8.    之后就可以在hql中使用该自定义函数了。`select fname('asdasdsa');`
