MySQL 见 `后端.md`

# 教材理论

## 概论

> ### 导学
>
> > 大数据, 或称巨量资料，是指需要新处理模式才能具有更强的决策力、洞察力和流程优化能力的海量、高增长率和多样化的信息资产
> >
> > 大数据的5V特点：Volume(大量)、Velocity(高速)、Variety(多样)、Value(价值密度)、Veracity(真实性)
>
> 数据库管理系统(database management system, DBMS)是由一个相互关联的数据的集合和一组用以访问、管理和控制这些数据的程序(建立在操作系统之上的系统软件)组成
>
> 这个相互关联的数据集合通常称为数据库(database, DB)，其中包含了关于某个企业信息系统的所有信息。 
>
> 设计DBMS的目的是为了有效地管理大量的数据，并解决操作系统的文件处理系统中存在的问题。
>
> 数据的有效管理，包括定义数据存储结构、提供数据操作机制。
> 不仅需要解决数据的共享性、独立性和数据之间的联系问题；
> 还需要解决数据的完整性、原子性、并发控制和安全性问题。
>
> 数据库系统(database system, DBS)，是指在计算机系统中引入数据库后的系统，一般由数据库、数据库管理系统(及其应用开发工具)、应用系统、数据库管理员和最终用户构成。——人机系统  
>
> > 服务外包：信息技术外包服务(ITO)，业务流程外包服务(BPO)

### 数据库系统的作用

数据：描述事物的符号记录。分为：

- 无结构的文本描述形式
- 结构化数据（表格，表中一行构成一条记录）
- 半结构化数据（XML，一般自描述，结构和内容混在一起)

数据处理：从大量的、可能是杂乱无章的、难以理解的数据中抽取并推导出对于某些特定的人们来说有价值、有意义的数据。

数据管理：对数据进行有效的分类、组织、编码、存储、检索、维护和应用——数据处理的中心问题。

**数据库管理技术的产生与发展**：

- 人工管理阶段(20c50s中以前)，数据面向应用程序，一个数据集对应一个程序，由应用程序定义和管理，无法共享利用，存在大量冗余数据

- 文件系统阶段(20c50s后-60s中)，计算机开始应用于数据管理，按文件名访问，按记录进行存取，数据放在多个数据文件

  ![image-20220531142235142](img/image-20220531142235142.png)

  主要特点：文件内的结构性，即一个文件内的数据是按记录进行组织的，这样的数据是有结构的(语义的) ，语义明确；整体上还是无结构的，即多个文件之间是相互独立的，无法建立全局的结构化数据管理模式

  弊端：数据共享性差 即数据冗余和不一致（不同应用程序难共享），数据独立性差（不易扩充，数据与应用程序之间缺乏逻辑独立性），数据孤立，数据获取困难（数据间的联系缺乏有效管理），完整性问题，安全性问题，原子性问题，并发访问异常

- 数据库管理系统阶段

  主要特点：(**数据库系统特点**)

  - 数据结构化。与文件系统的本质区别。不仅内部结构化，而是将数据以及数据之间的联系统一管理起来，使之结构化。不是仅仅针对某一个应用，而是面向全组织的所有应用。

  - 数据的共享度高，冗余度低，易扩充

  - 数据独立性高

  - 数据由数据库管理系统（DBMS）统一管理和控制 

    （数据的安全性保护，数据的完整性检查，并发控制，数据库恢复

  <u>助记：独统共结(独(自)统(治)共(\*党)解(\*军))</u>



数据库管理系统(DBMS)是由一个相互关联的数据的集合(数据库, DB)和一组用以访问、管理和控制这些数据的程序组成，是系统软件。

数据的完整性是指数据的正确性、有效性和相容性，也称为一致性约束

数据独立是指数据的使用(即应用程序)与数据的说明(即数据的组织结构与存储方式)分离，使得应用程序只需要考虑如何使用数据，而无须关心数据库中的数据是如何构造和存储的(表现为不会因为系统数据存储结构与数据逻辑结构的变化而影响应用程序)

- 数据独立性用来描述应用程序与数据结构之间的依赖程度，包括数据的物理独立性和数据的逻辑独立性
  - 物理独立性是指用户的应用程序与数据库中数据的物理结构是相互独立的；应用程序只关心数据的逻辑结构
  - 逻辑独立性是指用户的应用程序与数据库中数据的逻辑结构是相互独立的；应用程序只关心数据的局部逻辑结构(即应用视图) 




### 数据模型

数据库结构的基础是数据模型(data model)，是一个描述数据结构、数据操作以及数据约束的数学形式体系(即概念及其符号表示系统)(**三要素**)

- 数据结构：描述数据库的组成对象以及对象之间的联系 

- 数据操作：指对数据库中各种对象(型)的实例(值)允许执行的操作集合，包括操作及有关的操作规则 

- 数据完整性约束：一组数据完整性规则，是数据、数据语义和数据联系所具有的制约和依存规则，以保证数据库中数据的正确、有效和相容

  > 数据约束是对数据结构和数据操作的一致性、完整性约束，亦称为数据完整性约束 

<u>助记：完结操(玩节操)</u>

根据数据抽象的不同级别，将数据模型划分为3类：(**数据模型的分层**)

- 概念模型：概念层次的数据模型，也称为信息模型
- 逻辑模型：用于描述数据库数据的逻辑结构
- 物理模型：用来描述数据的物理存储结构和存取方法 



概念模型有实体-联系模型(E-R模型)和面向对象模型(OO模型) 

OO模型是用面向对象观点来描述现实世界实体(对象)的逻辑组织、对象间限制和联系等的模型

E-R模型基于对现实世界的如下认识：现实世界是由一组称作实体的基本对象以及这些对象间的联系构成

实体是现实世界中可区别于其他对象的一件“事情”或一个“物体”



逻辑模型是是用户通过数据库管理系统看到的现实世界，是按计算机系统的观点对数据建模，即数据的计算机实现形式，主要用于DBMS的实现。不同的DBMS提供不同的逻辑数据模型

- 层次模型(hierarchical model)

  优点：数据结构比较简单清晰，查询效率高，提供了良好的数据完整性支持

  缺点：不能很好地表达多对多联系，解决的办法：一是通过引入冗余数据(易产生不一致性)，二是创建非自然的数据结构(引入虚拟结点) ；查询孩子结点必须通过双亲结点；由于结构严密，层次命令趋于程序化 

- 网状模型(network model)

  条件：允许一个以上的结点无双亲，一个结点可以有多个双亲

  优点：能够更为直接地描述现实世界；具有良好的性能，存取效率较高

  缺点：结构比较复杂，而且随着应用规模的扩大，数据库的结构会变得越来越复杂，不利于最终用户掌握；操作语言比较复杂 

- 关系模型(relational model)

  优点：

  - 严格的数学基础：有关系代数作为语言模型，有关系数据理论作为理论基础
  - 概念单一：无论实体还是实体之间的联系都是用关系来表示，对数据(关系)的操作(查询和更新)结果还是关系。所以其数据结构简单、清晰，用户易懂易用
  - 存取路径透明：具有更高的数据独立性、更好的安全保密性，简化了程序员的工作，提高了软件的开发和维护效率

  缺点：

  - 由于存取路径对用户透明，查询效率往往不如非关系数据模型
  - 为了提高性能，DBMS必须对用户的查询请求进行查询优化，这样就增加了DBMS的开发难度

- 面向对象模型(即OO模型)

- XML模型

- 对象关系模型(object relational model)



物理模型：物理层是数据抽象的最低层。

> 从现实世界到概念模型的转换是由数据库设计人员完成
>
> 从概念模型到逻辑模型的转换可以由数据库设计人员完成，也可以用数据库设计工具协助设计人员完成
>
> 从逻辑模型到物理模型的转换一般由DBMS来完成 
>
> 概念模型：从现实世界到信息世界
> 逻辑模型：信息世界到逻辑机器世界
> 物理模型：逻辑机器世界到物理机器世界

三层模型是数据库设计的工具和方法(要满足DBMS体系结构的要求)，目的

1. 较真实地模拟现实世界、容易被人理解、便于计算机实现。
2. 提供全局逻辑模型：支持整体结构化，从而实现数据共享度高、冗余度低。
3. 同时满足DBMS三级模式结构要求。



关系模型：

- 关系(relation)：一个关系对应一张二维表，每一个关系有一个名称，即关系名

- 元组(tuple)：表中的一行称为一个元组

- 属性(attribute)：表中的一列称为一个属性，每一个属性有一个名称，即属性名

- 码(key)：也称为码键或键。表中的某个属性或属性组，它可以唯一地确定关系中的一个元组

- 域(domain)：属性的取值范围

- 分量(component)：元组中的一个属性值

- 外码(foreign key)：表中的某个属性或属性组，用来描述本关系中的元组(实体)与另一关系中的元组(实体)之间的联系

  外码的取值范围对应于另一个关系的码的取值范围的子集

- 关系模式(relational schema)：通过关系名和属性名列表对关系进行描述，即二维表的表头部分(表格的描述部分)

  如：关系名(属性名1，属性名2，…，属性名n)

  带下画线的属性为码属性，斜体的属性为外码属性 

  要求：关系必须是规范化：

  - 关系的每一个元组必须是可分区的，即存在码属性 
  - 关系的每一个属性(即元组的分量)必须是一个不可分的数据项，即不允许表中有表 

  完整性约束：实体完整性、参照完整性和用户自定义完整性 
  
  <u>助记：自实参(自(己)食铲)</u>



### 数据抽象与数据库的三级模式

隐藏关于数据存储和维护的某些细节，为用户提供数据在不同层次上的视图，即数据抽象

- 物理层抽象：最低层次的抽象，描述数据实际上是怎样存储的
- 逻辑层抽象：描述数据库中存储什么数据以及这些数据之间存在什么关联。数据库管理员和数据库应用开发人员使用
- 视图层抽象：只描述整个数据库的某个部分，即局部逻辑结构；同一数据库提供多个视图，每一个视图对应一个具体的应用，亦称为应用视图

提供观察数据库的3个不同角度。这就是数据库的三级模式结构，是DBMS的体系结构：(**三级模式结构**)

- 内模式：存储模式。对应于物理层数据抽象，它是数据的物理结构和存储方式的描述，是数据在数据库内部的表示方式
- 模式：逻辑模式。对应于逻辑层数据抽象，是数据库中全体数据的逻辑结构和特征的描述，是所有用户的公共数据视图。模式的一个具体值称为模式的一个实例(instance)。是DBMS模式结构的中间层
- 外模式：用户模式，子模式。对应视图层数据抽象。是数据库用户能够看见和使用的局部数据的逻辑结构和特征的描述，是数据库用户的数据视图，是与某一具体应用有关的数据的逻辑表示。外模式是保证数据库安全性的一个有力措施

目的：

1. 隐藏数据的存储和维护的细节，为用户提供数据在不同层次上的视图，方便不同的使用者可以从不同的角度去观察和利用数据库中的数据
2. 支持数据独立性的实现
3. 提供全局逻辑视图(模式)：支持整体结构化，从而实现数据共享度高、冗余度低、易扩充
4. 部分支持安全性的实现



**两层映像**功能：(**数据库独立性**)

- 模式/内模式映像。数据库中只有一个模式，也只有一个内模式，模式/内模式映像是唯一的，它定义了数据全局逻辑结构与存储结构之间的对应关系。在模式描述中定义模式/内模式映像。保证了数据与应用程序的物理独立性，简称为数据的物理独立性 
- 外模式/模式映像。对应于一个模式可以有多个外模式。对于每一个外模式，数据库管理系统都有一个模式/外模式映像，它定义了该外模式与模式之间的对应关系。在各自的外模式描述中定义外模式/模式映像。保证了数据与应用程序的逻辑独立性，简称为数据的逻辑独立性

模式即全局逻辑结构是数据库的核心和关键。设计数据库模式结构时，应首先确定数据库的逻辑模式

> 要保证数据库的数据独立性，应该修改的是两种映射



### 数据库系统

数据库系统(database system, DBS)，是指在计算机系统中引入数据库后的系统，一般由数据库、数据库管理系统(及其应用开发工具)、应用系统、数据库管理员和最终用户构成(**组成**)

数据库管理员(database administrator, DBA)，是指数据库的建立、使用和维护等的工作人员

- 决定数据库中的信息内容和结构
- 决定数据库的存储结构和存取策略
- 定义数据的安全性要求和完整性约束条件
- 监控数据库的使用和运行
- 数据库的改进和重组重构

在不引起混淆的情况下，常常把数据库系统简称为数据库，数据库管理系统简称为数据库系统 



数据库管理系统(DBMS)是一组软件，负责数据库的访问、管理和控制。功能：

- 数据定义：DBMS提供数据定义语言(DDL)
- 数据组织、存储和管理：DBMS要分类组织、存储和管理各种数据，包括数据字典、用户数据、数据的存取路径等 
- 数据操纵：DBMS还提供数据操纵语言(DML) 
- 数据库的事务管理和运行管理：数据库在建立、运行和维护时由DBMS统一管理、统一控制，以保证数据的安全性、完整性(一致性)，以及多用户对数据并发操作时的数据库正确性(称为并发控制)和系统发生故障后的数据库正确性(称为恢复与备份) 
- 数据库的建立和维护
- 其他功能 

组成：

- 查询处理器：对用户请求的SQL操作进行查询优化，从而找到一个最优的执行策略，然后向存储管理器发出命令，使其执行
- 存储管理器：根据执行策略，从数据库中获取相应的数据,或更新数据库中相应的数据
- 事务管理器：负责保证系统的完整性，保证多个同时运行的事务不发生冲突操作，以及保证当系统发生故障时数据不会丢失。

![image-20220531150428769](img/image-20220531150428769.png)





## 关系模型与关系代数

### 关系模型

> 关系模型的数据结构是二维表，亦称为关系
>
> 关系数据库是表的集合，即关系的集合
>
> 表是一个实体集，一行就是一个实体，它由有关联的若干属性的值所构成
>
> 关系模型中，现实世界的实体以及实体间的各种联系都是用关系来表示
>
> 基本特征有：与行的次序、列的次序无关

**关系数据结构**的形式化定义：(**关系模型的相关概念**)

域：是一组具有相同数据类型的值的集合($D$)

笛卡尔积：$D_1\times \cdots D_n=\{(d_1,\cdots ,d_n)|d_i\in D_i,i=1,\cdots n\}$

给定一组域 $D_i$，集合中每一个元素 $(d_1,\cdots ,d_n)$ 是 $n$ 元组，简称元组，每一个值 $d_i$ 是一个分量。$D_i$ 为有限集且基数为 $m_i$，则笛卡尔积基数为 $\prod_{i=1}^nm_i$。

关系：一组域的笛卡尔积的有限子集是该组域上的关系，即笛卡尔积的有限子集，表示为 $r(D_1,\cdots, D_n)$。$r$ 是关系名，$n$ 是关系的目或度(degree)。$n=1$ 单元关系，$n=2$ 二元关系。 表的一行对应关系一个元祖，一列对应一个域。列名是属性（因不同列的域可以相同）。$n$ 目关系共有 $n$ 个属性

关系的要求：

- 关系中的每个属性的域必须是原子的，即域中的每个值都是不可再分的一个完整单元。
- 关系中的每个元组都是可区分的，即存在唯一标识不同元组的属性(集)——码

空值(null)是所有可能的域的一个取值。

关系模式(relation schema)是关系的描述。表示为 $r(U,D,DOM,F)$，$U$ 是属性名集合，$DOM$ 是属性向域的映像集合，$F$ 是属性间数据依赖关系集合。简记 $r(U)$ 或 $r(A_1,\cdots,A_n)$，后者即属性名集合。 表头部分对应关系模式，是型。表体部分对应关系，是元祖集合，实例，是值。

> 人们经常把关系数据库模式和关系数据库实例都笼统地称为关系数据库，即型+值。

超码(super key)：关系 $r$ 的一个或多个属性的集合 $A$ 满足可以唯一地标识关系 $r$ 中的一个元组

候选码(candidate key)：关系 $r$ 的超码 $A$，满足任意真子集不是超码

主码(primary key, PK)：人为选定的若干候选码的一个

外码(foreign key FK)：$F$ 是关系 $r$ 的一个属性(集)满足对 $K_s$ 是关系 $s$ 的主码，$F$ 与 $K_s$ 相对应(关系 $r$ 中属性 $F$ 取值范围对应 $s$ 中 $K_s$ 取值范围的子集)，那么 $F$ 是关系 $r$ 参照关系 $s$ 的外码，简称 $F$ 是关系 $r$ 的外码



**关系完整性约束**条件：(**完整性规则**)

- 实体完整性：主码每个属性都非空
- 参照完整性：外码要么空，要么为对应的主码值
- 用户自定义完整性。如取值范围，取值间逻辑关系，唯一性等。



数据库模式导航图：一个含有主码和外码依赖的数据库模式的图例

![image-20220531152359187](img/image-20220531152359187.png)

- 下划线：主码
- 斜体：外码



**关系操作**特点：集合为单位。非关系数据模型操作方式：记录为单位。分为查询（选择select，投影project，连接join，除divide，并union，交intersection，差except，笛卡尔积，更名等）和更新（增删改）。查询是最主要操作。

关系操作表示方式：代数方式、逻辑方式。

- 关系代数是用代数方式表达的关系查询语言。
- 关系演算是用逻辑方式表达的关系查询语言。

对于关系代数、关系演算均是抽象的查询语言，在表达能力上是完全等价的。



### 关系代数

[表格/在线SQL转换代数参见](https://dbis-uibk.github.io/relax/help)

传统集合运算，(前三者)要求具有相同数量属性，域对应相同，即两关系的模式/结构相同。

- 并运算 $r\cup s=\{t|t\in r\vee t\in s\}$ 
- 差 $r-s=\{t|t\in r\wedge t\notin s\}$
- 交 $r\cap s=\{t|t\in r\wedge t\in s\}=r-(r-s)$ 
- 笛卡尔积 $r\times s=\{t_r\cdot t_s|t_r\in r,t_s\in s\}$ 得到 $n+m$ 目元组

关系运算：(**基本关系代数运算**)

选择：$\sigma_P(r)=\{t|t\in r\wedge P(t)\}$，$P$ 是谓词，逻辑表达式

投影：$\prod_A(r)=\{t[A]|t\in r\}$，选出若干列组成子关系

连接：$r\bowtie_\theta s=\{t_r\cdot t_s|t_t\in r\wedge t_s\in s\wedge(r.A\ op\ s.B)\}$，记作 $A\ op\ B$，两者为属性个数相等且可比的连接属性集，$op$ 是谓词，等效 $\sigma_\theta(r\times s)$。

$\theta$ 为等值比较谓词时，称为等值连接(equijoin)。自然连接(natural join)是特殊等值连接，要求具有公共属性集，并进行去重。

除：$r\div s=\{t_r[R-S]|t_r\in r\wedge s\subseteq S_x\}$，是 $R-S$ 上投影，条件为 $r$ 中 $R-S$ 取值象集包含 $s$。也就是先找到 $r$ 中元组，按 $R-S$ 分组，使得每组能取遍属性集 $R\wedge S$ 的所有取值。

![image-20220627011719759](img/image-20220627011719759.png)

> 如：
> $$
> (\prod_{stuNo,cNo}(Score))\div(\prod_{cNo}(\sigma_{name=语文 \vee name=数学}(Course)))
> $$

> 构造关系代数表达式的步骤：
>
> - 明确该查询涉及到哪些属性
> - 明确该查询涉及到哪些关系
> - 根据数据库模式导航图，通过多对一联系(或一对多联系)把所有涉及的关系连接起来，每一个多对一联系(或一对多联系)都可以表示为外码属性的自然连接

五种基本关系代数运算：$\cup,-,\times,\sigma,\pi$。(和更名)

其他运算：$\cap,\bowtie,\div,=$(赋值)。



## SQL查询语句

### SQL概述

包括：

- 数据操纵语言 **DML**： select, insert, update, delete (M: manipulation)

- 数据定义语言 **DDL**： create, drop, alter (D: definition)

- 数据控制语言 DCL： grant, revoke (C: control)

- 其他（嵌入式和动态，重新组织，备份，恢复）

**SQL特点**：

- 综合统一 (DDL,DML,DCL于一体)
- 高度非过程化
- 面向集合的操作方式
- 同一种语法结构提供两种使用方式 (自含式、嵌入式)
- 语言简洁，易学易用 (动词少)

非过程化。

SQL语言支持三级模式结构，外模式对应视图和部分基本表，模式对应基本表，内模式对应存储文件 

基本表：数据库中独立存在的表称为基本表，在SQL中一个关系对应一个基本表，一个(或多个)基本表对应一个存储文件，一个表可以带若干索引，索引存放在存储文件中

视图：指从一个或几个基本表(或视图)导出的表，是虚表，只存放视图的定义而不存放对应数据

查询表：指查询结果对应的表

存储文件：指数据库中存放关系的物理文件



**单表查询、连接查询、嵌套子查询**都是重点

单表查询：(各子句用法)

1. 投影运算：查询列、消除重复元组、别名、计算列
2. 选择运算：比较运算、范围查询、集合查询、空值查询、字符匹配、逻辑查询
3. 排序运算、聚合查询

连接查询：等值/非等值连接

> Theta join($\theta$ join)使用等于，不等于，大于小于等链接

嵌套子查询：IN/NOT IN，比较运算符(=ANY,!ALL等价于IN, NOT IN)

### 单表查询

> SQL Server函数：`lower,getdate,year,month` 等

>  举例：年龄大于或等于19岁 (==da:周岁==)
>
> ```sql
> SELECT studentNo, studentName, birthday
> FROM Student
> WHERE year(getdate()) - year(birthday)>=19
> ```
>
> 不含 `福` ：
>
> ```mysql
> SELECT studentNo, studentName
> FROM Student WHERE studentName NOT LIKE '%福%'
> ```
>
> with 的使用：
>
> ```sql
> with a as(
> SELECT * 
> FROM Student 
> WHERE sex='女')
> SELECT studentNo, studentName, convert(varchar(10),birthday, 111) as birthday
>    FROM  a
>    WHERE year(birthday)=1999 # convert 是 SQL Server
> ```



### 连接查询

> 同时选修两门课：
>
> ```mysql
> SELECT a.studentNo, studentName, b.courseNo, b.score, c.courseNo, c.score
> FROM Student a, Score b, 
> (SELECT * FROM Score WHERE courseNo='002') c
> WHERE b.courseNo='001' 
>       AND a.studentNo=b.studentNo # 表a与表b的连接条件
>       AND a.studentNo=c.studentNo # 表a与表c的连接条件
> ```
>
> ```sql
> SELECT a.studentNo, studentName, b.courseNo, b.score, c.courseNo, c.score
>      FROM Student a, Score b, Score c
>      WHERE a.studentNo=b.studentNo # 表a与表b的连接条件
>        AND a.studentNo=c.studentNo # 表a与表c的连接条件
>            AND b.courseNo='001' # 表b上的选择条件
>            AND c.courseNo='002' # 表c上的选择条件
>      ORDER BY a.studentNo
> ```
>
> 
>
> SQL不直接支持自然连接，完成自然连接的方法是在等值连接的基础上消除重复列
>
> 若某个表与自己进行连接，称为自表连接 
>



### 嵌套子查询

> 常用：
>
> - 元素与集合间的属于关系
> - 集合之间的包含和相等关系
> - 集合的存在关系
> - 元素与集合元素之间的比较关系

> 子查询中，不允许使用ORDER BY子句，该子句仅用于最后结果排序
>
> 分为非相关子查询，相关子查询。
>
> 选修过包含`系统`二字的课程的学生信息：
>
> ```mysql
> SELECT studentNo, studentName, classNo
> FROM Student
> WHERE studentNo IN 
>               ( SELECT studentNo FROM Score
>                WHERE courseNo IN 
>                ( SELECT courseNo FROM Course
>                 WHERE courseName LIKE '%系统%' ))
> ```
>
> ```mysql
> SELECT studentNo, studentName, classNo
> FROM Student
> WHERE studentNo IN 
>               ( SELECT studentNo 
>                FROM Score b, Course c
>                WHERE b.courseNo=c.courseNo 
>                AND courseName LIKE '%系统%' )
> ```
>
> ```mysql
> SELECT DISTINCT a.studentNo, studentName, classNo
> FROM Student a, Score b, Course c
> WHERE a.studentNo=b.studentNo
>     AND b.courseNo=c.courseNo 
>     AND courseName LIKE '%系统%' 
> ```
>
> 并不是每一个IN子查询都可以转化为连接运算来实现
>
> 
>
> 除法运算，同时选修两门课的人选的所有课：(也可以用 in 三层)
>
> ```mysql
> SELECT  a.studentNo, studentName, courseName, score
> FROM Student a, Course b, Score c
> WHERE  a.studentNo=c.studentNo AND b.courseNo=c.courseNo
>    AND   a.studentNo IN 
>     ( SELECT studentNo FROM Score x, Course y
>                 WHERE x.courseNo=y.courseNo 
>                 AND courseName='计算机原理' )
>    AND   a.studentNo IN 
>      ( SELECT studentNo FROM Score x, Course y
>                 WHERE x.courseNo=y.courseNo 
>                 AND courseName='高等数学' )
> ORDER BY a.studentNo, score DESC
> ```
>
> 
>
> 查询最高分：
>
> ```mysql
> SELECT studentNo, courseNo, score
> FROM Score
> WHERE score=( SELECT max(score) FROM Score )
> ```
>
> 年龄小于班上某个人(即出生日期晚于)
>
> ```mysql
> SELECT studentNo, studentName, year(getdate())-year(birthday) AS age
> FROM Student
> WHERE birthday>ANY
> ( 	SELECT birthday
> 	FROM Student a, Class b
> 	WHERE className='计算机科学与技术16-01班' 
> 	AND a.classNo=b.classNo )
> ```
>
> 
>
> exists是逻辑表达式，若子句非空条件为真
>
> exists 的使用：(将元组扔到 exists 子句里判断)(选修了课程的人)
>
> ```mysql
> SELECT studentName, classNo 
> FROM Student x
> WHERE EXISTS 
>         ( SELECT * FROM Score a, Course b
>          WHERE a.courseNo=b.courseNo 
>          AND a.studentNo=x.studentNo 
>          AND courseName='计算机原理' )
> ```
>
> 
>
> 全称量词转存在量词：
> $$
> (\forall c)P(x,c)\Leftrightarrow\lnot(\exists c(\lnot P(x,c)))
> $$
> 如选修了所有课程的学生：
>
> ```mysql
> SELECT studentName 
> FROM Student x
> WHERE NOT EXISTS 
>   ( SELECT * FROM Course c
>    WHERE NOT EXISTS   
>    --判断学生x.studentNo没有选修课程c.courseNo
>        ( SELECT * FROM Score
>         WHERE studentNo=x.studentNo 
>         AND courseNo=c.courseNo )
>   )
> ```
>
> 解析：
>
> - 整体逻辑：先找一个集合，集合里的人没选修全部课程，然后找出不属于这个集合的人；对于这个集合，等价于这些人不在选修关系里
> - 如果一个人选修了所有课程，那么每门课它都修了即最内层select结果非空，所以exists(非空)为真，not exists为假，那么中间层select没有结果，是空集，那么最外层成立
> - 如果一个人有一门课没选修，那么这门课not exists为真，中间层不空，所以not exists为假
>
> 如：至少选修了特定学号学生所选修的所有课程的学生：
>
> 即：不存在一门课程，特定学号学生选修了，且该学生没选修
>
> ```mysql
> SELECT studentName
> FROM Student x
> WHERE NOT EXISTS 
>     ( SELECT * FROM Score y      // 不能用Course表
>      WHERE studentNo='1600002'  
>      --查询学生'1600002'所选修课程的情况
>      AND NOT EXISTS    
>      --判断学生x.studentNo没有选修课程y.courseNo
>          ( SELECT * FROM Score
>           WHERE studentNo=x.studentNo 
>           AND courseNo=y.courseNo )
>     )
> ```
>
> 至少选修了学号为1600002学生所选修的所有课程的学生学号、姓名以及该学生所选修的1600002学生选修过的所有课程的课程名和成绩：
>
> ```mysql
> SELECT  x.studentNo,  studentName,  courseName,  score 
> FROM  Student x, Course y, Score z
> WHERE  x.studentNo=z.studentNo AND y.courseNo=z.courseNo 
>       AND   NOT EXISTS
>    ( SELECT * FROM Score b
>     WHERE studentNo='1600002'   --查询学生'1600002'所选修课程的情况
>     AND NOT EXISTS    --判断学生x.studentNo没有选修课程b.courseNo
>         ( SELECT * FROM Score
>          WHERE studentNo=x.studentNo AND courseNo=b.courseNo )
>    )
>     AND  y.courseNo IN ( SELECT courseNo FROM Score WHERE studentNo='1600002')
> ```
>
> 
>
> 至少拿了 28 学分的人：
>
> ```mysql
> SELECT a.studentNo, studentName, courseName, score, creditHour
> FROM Student a, Course b, 
>     ( SELECT studentNo, courseNo, max(score) score
>      FROM Score
>      WHERE score>=60    -- 仅列示已经获得学分(即及格了)的课程
>      GROUP BY studentNo, courseNo ) AS c      -- 查询表c
> WHERE a.studentNo=c.studentNo AND c.courseNo=b.courseNo 
>        AND a.studentNo IN 
>    ( SELECT studentNo     -- 子查询Q
>      FROM Course x, 
>     ( SELECT studentNo, courseNo, max(score) score
>      FROM Score
>      WHERE score>=60    -- 只有及格才能获得学分
>      GROUP BY studentNo, courseNo ) AS y
>     WHERE y.courseNo=x.courseNo
>     GROUP BY studentNo
>     HAVING sum(creditHour)>=28 )
> ORDER BY a.studentNo
> ```
>
> 
>
> 查询至少选修了5门课程且课程平均分最高的同学的学号和课程平均分。如果一个学生选修同一门课程多次，则选取最高成绩：
>
> ```mysql
> SELECT studentNo, avg(score) avgScore
> FROM  ( SELECT studentNo, courseNo, max(score) score
>        FROM Score
>        GROUP BY studentNo, courseNo ) AS a
> GROUP BY studentNo
> HAVING count(*)>=5 
>  AND avg(score)=
>     ( SELECT max(avgScore)    -- 子查询Q2
>      FROM ( SELECT studentNo, avg(score) avgScore    -- 子查询Q1
>            FROM  ( SELECT studentNo, courseNo, max(score) score
>                   FROM Score
>                   GROUP BY studentNo, courseNo ) AS b
>            GROUP BY studentNo
>            HAVING count(*)>=5 ) AS x)
> ```
>
> 非相关子查询指子查询的结果不依赖于上层查询
> 相关子查询指当上层查询的元组发生变化时，其子查询必须重新执行
>
> 
>
> 查询平均分最高的课程：
>
> ```mysql
> SELECT a.courseNo, courseName, avg(score) 最高平均分
> FROM Course a, Score b
> WHERE a.courseNo=b.courseNo
> GROUP BY a.courseNo, courseName
> HAVING avg(score)=
>    ( SELECT max(avgScore)
>     FROM ( SELECT avg(score) avgScore
>           FROM Score
>           GROUP BY courseNo ) x)
> ```
>
> 
>
> 集合运算 union(并), intersect(交), except(减)可以用其他语句改写
>
> order by表达式可以是列名或表达式

SQL 语句一般格式：

```mysql
SELECT [ALL | DISTINCT] <目标列表达式> [AS] [<别名>]
                 [, <目标列表达式> [AS] [<别名>] ... ]
FROM {<表名> | <视图名> | <查询表>} [AS] [<别名>]
             [, {<表名> | <视图名> | <查询表>} [AS] [<别名>] ... ]
[ WHERE <条件表达式> ]
[ GROUP BY <列名1> [, <列名2> ... ]
   [ HAVING <条件表达式> ] ]
[ ORDER BY <列名表达式> [ASC | DESC] 
                [, <列名表达式> [ASC | DESC] ... ] ]
```



## 数据库建模

### 数据库设计过程

**全节重点**

数据库设计过程概述：

数据库设计就是根据各种应用处理的要求、硬件环境及操作系统的特性等，将现实世界中的数据进行合理组织，并利用已有的数据库管理系统(DBMS)来建立数据库系统的过程。
具体地说，对于一个给定的应用环境，构造出最优的数据库逻辑模式和物理模式，并建立数据库及其应用系统，使之能够有效地存储和管理数据，满足用户的信息要求和处理要求。 

```mermaid
graph LR
	a[需求分析]-->b[概念设计]
	b-->c[逻辑设计]
	c-->d[模式求精]
	d-->e[物理设计]
	e-->f[应用与安全统计]
```

- 需求分析：基础，最困难和耗时

  功能需求、数据需求、数据使用业务规则、数据联系及约束、性能需求等；分析用户行为、需求分析、建立数据流图

- 概念设计：得到概念模型(E-R图)(E-R模型是语义模型，能映射到数据库概念模型)(使用到的抽象方法：聚合、概括、分类；不包括合并)(概念模型独立于硬件设备和DBMS)

- 逻辑设计：逻辑数据模型(数据库(关系)模式) 考虑概念模式、处理要求、DBMS特性，不考虑存取方式

- 模式求精：分析并发现数据库逻辑模式存在的问题，并进行改进和优化(减少数据冗余，消除更新、插入与删除异常)(基于关系理论)

- 物理结构：数据库文件组织格式、文件内部存储结构、索引、表的聚集；关注存取方式、索引与入口设计、安全性完整性一致性问题

- 应用与安全设计：访问权限(角色)，存储过程

E-R图合并可能会出现属性、结构和命名冲突(不会出现语法冲突)、

实施阶段的工作：建立数据库、加载数据、系统调试(无扩充功能)



### E-R模型基本概念及表示

**全节重点**

实体是客观世界中可区别于其他事物的“事物”或“对象”，特征：独立存在、可区别于其他实体。可以是有形、实在或抽象、概念(课程、单据)

实体集是具有相同类型即相同性质(或属性)的实体组成的集合

属性是实体集中每个实体都具有的特征描述。实体集具有相同属性，有属性值和属性的域

- 简单属性：不能分为更小部分
- 复合属性：可以进一步划分为更小部分 (如住址拆成省市街道)
- 单值属性：特定实体任何时候只能取一个值
- 多值属性：0到任意多个(如电话号码) (转为弱实体集/多个单值)
- 派生属性：值可以从其它相关属性或实体(集)派生出来 (如年龄)
- 空值NULL：在某个属性没有值，表示未知/不存在

联系：多个实体相互关联

联系集：同类联系的集合。是 $n(n\ge 2)$ 个实体集 $E$ 上的数学关系，实体集不必互异。联系集是 $\{(e_1,\cdots,e_n)|e_i\in E_i\}$ 的子集。可具有自身的描述属性

- 度：参与联系集的实体集的数目。划分为 $x$ 元联系集

多联系：在相同的实体集上可能存在多个不同的联系集

多值联系：是指在同一个给定的联系集中，相关联的相同实体之间可能存在多个联系 (如客户与银行可能发生多次贷款)

角色：实体在联系中的作用



### 约束

主要有：

1. **映射约束**：

   - 实体集A中的一个实体通过某联系集R能与实体集B中的实体相联系的数目，称为实体集A到实体集B之间的联系集R的映射基数(mapping cardinality)，简称为联系集R的映射基数

     共有一对一，一对多和多对多三种映射基数

   - 一对一: A中的一个实体至多(允许不)同B中的一个实体相联系，B中的一个实体也至多(允许不)同A中的一个实体相联系。如发票与销货单(可以存在实体与另一方无联系(即如不开发票))

   - 一对多:A中一个实体可以同B中任意数目(可以为0)的实体相联系，而B中一个实体至多(允许不)同A中一个实体相联系。如班级和学生一对多(学生可以不加入班级，也可以有空班)

   - 多对多:A中的一个实体可以同B中任意数目(可以为0)的实体相联系，而B中的一个实体也可以同A中任意数目(可以为0)的实体相联系。如学生与选课

2. **码约束**：

   - 超码：一个或多个属性的集合，且这些属性的集合在一实体集中能够唯一标识一个实体。超码的任意超集是超码
   - 候选码和超码是实体集客观存在的特性，它们表示实体集中任意两个实体不允许在码属性上有相同的值
   - 主码选择原则：
     选择属性长度最短的候选码；
     选择包含单个属性的码，而不是复合候选码；
     选择在数据库系统生命周期内属性值最少变化的候选码；
     选择在数据库系统生命周期内更可能包含唯一值的候选码
   - 一个联系集中的每一个联系也要求是可区分的，也必须存在候选码，并选择其中的一个候选码作为主码，用于唯一标识该联系集中的一个联系。联系集的超码是与它联系的所有实体集主码的并
   - 二元联系集的主码选择，依赖于联系集的映射基数：
     - 一对一：主码可以使用参与联系集中的任何一方实体集的主码
     - 一对多(多对一)：主码由“多”的一方实体集的主码组成
     - 多对多：主码由参与联系集中所有实体集的主码组成
   - 二元联系集**联系属性安置**：
     - 一对一联系集的属性：安置于联系集或任一边的实体集上
     - 一对多联系集的属性：安置于联系集或多的一方实体集上
     - 多对多联系集的属性：它是描述相关联实体集间的交互性语义，因此，联系属性只能安置于联系集上，不能放到相关联的实体集上去

3. **依赖约束**：

   - 指联系中一种实体的存在依赖于该联系集中联系或其他实体集中实体的存在
     - 联系中一种实体的存在依赖于该联系集中联系的存在，称为实体集与联系集之间的依赖约束，并将依赖于联系集而存在的实体集称为**依赖实体集**
     - 联系中一种实体的存在依赖于其他实体集中实体的存在，称为实体集之间的依赖约束，并将依赖于其他实体集而存在的实体集称为**弱实体集**

4. 参与约束：

   - 如果实体集A中的每个实体都参与到联系集R中至少一个联系中，则称实体集A全部参与联系集R
   - 如果实体集A中只有部分实体参与到联系集R的联系中，则称实体集A部分参与联系集R



### 弱实体集

**全节重点**

与之相对是强实体集

弱实体集所依赖的强实体集称为标识实体集 (identifying entity set)

弱实体集必须与一标识实体集相关联才有意义，该联系集称为标识联系集 (identifying relationship set)

一个弱实体集中用来标识弱实体的属性(集) 称为该弱实体集的部分码 (partial key)

弱实体集中的实体是由其标识实体集中的主码与其部分码共同标识

限制：

- 标识实体集和弱实体集必须是“一对多”联系集 
- 弱实体在标识联系集中是全部参与

应用：

1. 多值属性建模为弱实体集

   (依赖约束和多值联系可能会导致将联系集建模为依赖实体集或弱实体集)

2. 语义上存在包含关系的实体 (如学院与系)

   (没有必要在系统全局范围内唯一标识的实体集可建模为弱实体集)

弱实体集可以同时依赖于多个标识实体集



### 扩展E-R特征

使用实体集的继承和ISA联系(超类/子类联系)描述子集。子类会继承父类的所有属性

联系集之间的联系：使用聚合

聚合是一种抽象，它将一个联系集及其相关联的实体集抽象为一个高层实体集(称为联系实体集)对待，然后建立该高层实体集与其它实体集之间的联系集

![image-20220625211125740](img\image-20220625211125740.png)



### E-R建模问题

基本原则：[(属性冲突、命名冲突、结构冲突)]

1. 忠实性 (首要，最重要) (忠于需求)
2. 简单性 (不要添加非必要成分)
3. 避免冗余 (一个对象只存放在一个地方)

通常满足下述两条规则，均可作为属性对待：

- 作为属性，不能再具有要描述的性质；
- 属性不能和其它实体相联系

注意：

- 不能将实体集主码表示为另一实体集属性 (要使用联系)
- 不能将相关实体集的主码属性作为联系集属性 (已经隐含了)

一事物是描述为实体集还是联系集并没有一个绝对的标准。通常原则：

- 实体对应于现实世界中实际存在的事物，是名词
- 联系对应的概念一般为一种动作，即描述实体间的一种行为

三元联系转二元联系：

- 通过聚合将二元联系集建模成一个联系实体集，再加上它与原来联系的实体集之间的二元联系，如图(b)所示
- 或者建立一个依赖实体集或弱实体集，再与原实体集之间建立二元联系，如图(c)、图(d)所示

![image-20220625211620612](img\image-20220625211620612.png)

下图 b,c 是错误示范(语义不对)，下下图是对的

![image-20220625211741675](img\image-20220625211741675.png)

![image-20220625211938484](img/image-20220625211938484.png)

依赖约束：依赖于联系集而存在的实体集一般是指伴随着业务发生而形成的单据。如员工、客户、商品之间发生销售/购买商品等业务时，伴随着会产生销货单/购货单。
在E-R建模时，一般将依赖于业务的发生而产生的销货单/购货单等直接建模为依赖实体集(而不是联系集)，并将它直接与所依赖的联系集关联起来

> 类似的业务有：
> 领料员/采购员、仓库保管员、材料之间发生的出库/入库业务会伴随着产生出库单/入库单；
> 读者、图书管理员、图书之间发生的借书业务会伴随着产生借书单；
> 客户、员工、现金之间发生的存款/取款业务会伴随着产生存款单/取款单;
> 病人、医生、药品之间发生的诊断业务会伴随着产生病历记录-处方单；
> 旅客、员工、客房之间发生的入住业务会伴随着产生入住单；
> 司机、警察、违章处罚目录之间发生的违章处罚业务会伴随着产生违章处罚单；
> 员工、游客、景点之间发生的旅游业务会伴随着产生旅游安排单；
> 公交车、车站之间发生的运行安排业务会伴随着产生公交线路。 



概念设计任务：

- 定义实体集及属性，实体集的主码，并用数据字典描述实体集；
- 定义联系集及属性，联系集的主码，联系的映射基数及参与约束,联系中实体的角色，并用E-R图描述被建模的联系集；
- 分析初步E-R图中是否存在依赖约束、多值联系，并将其建模为依赖实体集或弱实体集；
- 利用扩展E-R特征对对象进行分类及聚合(建模为联系实体集) ；
- 将多元联系转化为二元联系进行建模(联系实体集、依赖实体集或弱实体集)；
- 去除冗余数据，并保证满足所有数据需求不冲突；
- 对照需求分析规格说明书，检查E-R模型，看其是否包含了所有数据、能否满足所有功能需求等



### E-R图图例

> 下面展示的是陈氏标记法 (Chen)

E-R图图例：

- 实体集：矩形
- 属性：椭圆 (主码：下划线)
- 多值属性：双椭圆
- 派生属性：虚线椭圆
- 属性与实体之间：连线
- 联系：菱形
- 联系与实体之间：连线 (箭头指向一，横线指向多，双横线是全部参与)
- 依赖实体集：带填充背景的矩形 
- 依赖实体集所依赖的联系集：带填充背景的菱形
- 弱实体集：双矩形
- 标识实体集：双菱形
- 部分码：虚下划线
- ISA联系：三角形
- 联系实体集：内部包含无填充背景虚线菱形的填充背景矩形
- 聚合：虚线矩形

![image-20220625202842323](img/image-20220625202842323-17094578940531.png)

> Crow's foot (鸦爪形)方法 [参考](https://blog.csdn.net/aopstudio/article/details/122772545) [PPT](https://s3.us-west-2.amazonaws.com/ucsd.dsc100/homework/homework_5_4wd7ss.pdf)
>
> 解决了上文的缺点之属性用椭圆表示：当一个实体存在很多属性的时候，就是产生非常多的椭圆，导致图片非常混乱
>
> ![image-20240303172333747](img/image-20240303172333747.png)
>
> ![在这里插入图片描述](img/329dc24a9a580210749264df35a02a30.png)

> ![image-20240303172547794](img/image-20240303172547794.png)

> ![image-20240303172637979](img/image-20240303172637979.png)
>
> ![image-20240303172711640](img/image-20240303172711640.png)

### 逻辑设计

**全节重点**

逻辑模型(数据库模式，关系模式的集合)

- 强实体集：直接转

- 弱实体集：用外码加部分码当主码

- 联系集：

  - 一对多：多方增加一方的主码作为外码和联系属性(如公司与员工的聘用，把公司名和聘期放到员工去)
  - 一对一：把任意一方看成多，同上处理
  - 多对多：按映射基数确定主码，外码是各实体集主码并

- 标识联系集：不需要转化

- 复合属性：拆成多个简单属性

- 多值属性：创建关系模型(建模为弱实体集)，属性为所在实体集(/联系集)主码(是外码)和本来多值属性对应的属性

- 类层次：

  - 父类是公共属性，子类是特殊属性和父类主码，子类主码与父类主码相同
  - 只转换子类，删掉父类

- 聚合：

  - 内层联系集、外层联系集都是按其映射基数决定是否需要单独转化为一个独立的关系模式(多对多联系集才需要)
  - 外层联系集的主码根据映射基数不同分别由内层联系集(即联系实体集)的主码、外层实体集的主码按不同方式产生

  说人话就对被联系的练习集，跳过聚合这玩意本身，就是拆掉聚合，用它对应的实体集去做，如：

  ![image-20220625214734892](img/image-20220625214734892.png)

  ![image-20220625214909078](img/image-20220625214909078.png)

  ![image-20220625214922154](img/image-20220625214922154.png)



## 关系数据理论与模式求精

### 问题提出

数据冗余是指同一信息在数据库中存储了多个副本。它可能引起下列问题：

- 冗余存储：信息被重复存储，导致浪费大量存储空间
- 更新异常：当重复信息的一个副本被修改，所有副本都必须进行同样的修改。因此当更新数据时，系统要付出很大的代价来维护数据库的完整性，否则会面临数据不一致的危险
- 插入异常：只有当一些信息事先已经存放在数据库中时，另外一些信息才能存入数据库中
- 删除异常：删除某些信息时可能丢失其它信息

模式分解存在的问题：

- 有损分解：两个分解后的关系通过连接运算还原得到的信息与原来关系的信息不一致

  反之称为无损分解(lossless decomposition)

- 丢失依赖关系

  反之，如果被分解关系模式上的所有依赖关系都在分解得到的关系模式上保留，称该分解为依赖保持 (dependency preserving)分解

好的关系模式：

- 数据冗余尽可能少(即数据共享尽可能高)
- 不发生插入异常、删除异常、更新异常等问题。
- 模式分解时，分解后的模式应具有无损连接、保持依赖等特性

理论依据：关系规范化理论

候选关键字的属性可以有 1 个或多个。候选关键字可能有 1 个或多个，但主关键字只有一个

关系模式的任何属性不可再分



### 函数依赖定义

**全节重点**

函数依赖(functional dependency, 简称FD)是一种完整性约束,  是现实世界事物属性之间的一种制约关系

函数依赖定义：设 $r(R)$ 为关系模式，$\alpha\subseteq R,\beta\subseteq R$，对任意合法关系 $r$ 及其中任两个元组 $t_i,t_j(i\neq j)$，若 $t_i[\alpha]=t_j[\alpha]$ 则 $t_i[\beta]=t_j[\beta]$，则称 $\alpha$ 函数确定 $\beta$，或 $\beta$ 函数依赖于 $\alpha$，记作 $\alpha\to\beta$。

函数依赖那么 $\alpha,\beta$ 可能是多对一关系

函数依赖是语义范畴的，只能根据数据语义确定，不能被证明

码约束是函数依赖特例，码属性(集)是 $\alpha$，关系中所有属性是 $\beta$

非平凡函数依赖：$\alpha\to\beta,\beta\not\subseteq\alpha$；否则是平凡函数依赖($\beta\subseteq\alpha$)。对于任一关系模式，平凡函数依赖都是必然成立的，它不反映新的语义

完全函数依赖：对非平凡函数依赖，$\forall\gamma\subset\alpha,\gamma\to\beta$ 都不成立。简称函数依赖。否则是部分函数依赖，简称部分依赖

对完全函数依赖 $\alpha\neq \beta$，则 $\alpha\to\beta$ 一定是非平凡函数依赖

显然 $\alpha$ 是单属性时函数依赖一定是完全函数依赖

若 $\gamma\subseteq R,\alpha\to\beta,\beta\to\gamma$ 则存在函数依赖 $\alpha\to\gamma$，若 $\alpha\to\beta,\beta\to\gamma,\alpha\to\gamma$ 非平凡函数依赖，且 $\beta\not\to\alpha$，则 $\alpha\to\gamma$ 是传递函数依赖，简称传递依赖

因为是非平凡函数依赖，所以共四个条件：$\beta\not\subseteq\alpha,\gamma\not\subseteq\beta,\gamma\not\subseteq\alpha,\beta\not\to\alpha$

因为最后一个条件，所以 $\beta$ 一定不是候选码

传递依赖也可能会导致数据冗余及产生各种异常

函数依赖是指关系模式中属性之间存在的一种约束关系。这种约束关系既可以是现实世界事物或联系的属性之间客观存在的约束，也可以是数据库设计者根据应用需求或设计需要强加给数据的一种约束
但不论是哪种约束，一旦确定，进入数据库中的所有数据都必须严格遵守



### 范式

**全节重点**

第一范式 $\mathbf{1NF}$：关系模式每个属性值域都不可分(原子的)，则 $r(R)\in1NF$

目标：将基本数据划分成称为实体集或表的逻辑单元，当设计好每个实体后，需要为其指定主码

主属性：$\alpha\subseteq R,\alpha$ 包含在 $r(R)$ 的某个候选码中；否则是非主属性

第二范式 $\mathbf{2NF}$：$1NF$ 的所有非主属性完全函数依赖于 $r(R)$ 的候选码(不存在非主属性对候选码的部分依赖)

目标：将只部分依赖于候选码（即依赖于候选码的部分属性）的非主属性移到其他表中(全部是码)

第三范式 $\mathbf{3NF}$：$2NF$ 的所有非主属性直接函数依赖于 $r(R)$ 候选码(不存在非主属性传递依赖于候选码)

目标：去掉表中不直接依赖于候选码的非主属性(仅仅是码)

> 如：$r(R)=r(A,B,C),F=\{AB\to C,C\to A\}$，计算得候选码为 $AB$ 或 $BC$，故 $r(R)\in 3NF$；理由：都是主属性，没有非主属性

Boyce-Codd 范式 $\mathbf{BCNF}$：$1NF$ 的 $F^+$ 所有函数依赖 $\alpha\to\beta$ 要么是平凡函数依赖，要么 $\alpha$ 是一个超码(即包含 $r(R)$ 的候选码) (即 $F^+$ 的每个非平凡函数依赖的决定属性集 $\alpha$ 包含候选码)

BCNF 排除了：任何属性(包括主属性和非主属性)对候选码的部分依赖和传递依赖；主属性之间的传递依赖

> 如：$r(R)=r(A,B,C)$，则
>
> - $F=\{A\to B,B\to C\}$ 不是 $BCNF$
> - $F=\{AB\to C,C\to A\}$ 不是 $BCNF$
> - $F=\{AB\to C,BC\to A\}$ 是 $BCNF$

$BCNF\subset 3NF\sub 2NF\sub 1NF$

BCNF 与 3NF 的区别在于是否 $\beta-\alpha$ 的每个属性都是候选码的一部分(3NF 不要求都在一个候选码里，多个候选码时不一定都在)，即 3NF 允许存在主属性对候选码的传递依赖和部分依赖

若 $\beta-\alpha$ 每个属性都是候选码的一部分，那么 $\alpha$ 也一定包含候选码的一部分

> 满足BCNF范式的模式分解，可能不是保持依赖分解。3NF分解既是无损分解，又是保持依赖分解

3NF存在数据冗余和异常问题，而BCNF是基于函数依赖理论能够达到的最好关系模式(任何二元关系模式的最高范式)

总结：

- 2NF：不存在非主属性对候选码的部分依赖

- 3NF：不存在非主属性对候选码的传递依赖

  ​		允许存在主属性对候选码的部分和传递依赖

- BCNF：每个非平凡函数依赖的左部是超码 

  (不存在对候选码的部分依赖和传递依赖和主属性传递依赖)

任何一个关系模式都一定有码



### 函数依赖理论

给定函数依赖集 $F$，可以证明其他成立的函数依赖是被 $F$ 逻辑蕴涵的函数依赖。 

**函数依赖集闭包**：$F$ 逻辑蕴涵的所有函数依赖组成的集合 $F^+$

Armstrong 公理：

- 自反律(reflexivity rule)  $\beta\subseteq\alpha\Rightarrow\alpha\to\beta$ 
- 增补律(augmentation rule) $\alpha\to\beta\Rightarrow\gamma\alpha\to\gamma\beta$
- 传递律(transitivity rule) $\alpha\to\beta,\beta\to\gamma\Rightarrow\alpha\to\gamma$

推论：

- 合并律(union rule) $\alpha\to\beta,\alpha\to\gamma\Rightarrow\alpha\to\beta\gamma$
- 分解律(decomposition rule) $\alpha\to\beta\gamma\Rightarrow\alpha\to\beta,\alpha\to\gamma$
- 伪传递律(pseudo-transitivity rule) $\alpha\to\beta,\beta\gamma\to\delta\Rightarrow\alpha\gamma\to\delta$ 

> 求已知部分属性时能通过函数依赖得到的所有属性集：可以检验是否存在函数依赖 $A\to B$，即求 $A$ 能否推出 $B$ 的全部属性
>
> ```python
> # Define the given functional dependencies as a dictionary for easier manipulation
> fd = {
>     'A': ['B'],
>     'CH': ['A'],
>     'B': ['E'],
>     'BD': ['C'],
>     'EG': ['H'],
>     'DE': ['F']
> }
> 
> # Define a function to compute the closure of a set of attributes
> def closure(attributes, fd):
>     # Initialize the closure with the given set of attributes
>     closure = set(attributes)
>     
>     # Keep track of whether we added new attributes to the closure in the last iteration
>     added = True
>     
>     # Continue until no new attributes can be added
>     while added:
>         added = False
>         for lhs, rhs in fd.items():
>             # If the left-hand side of the functional dependency is a subset of the current closure
>             # and the right-hand side is not already in the closure, add it
>             if set(lhs).issubset(closure) and not set(rhs).issubset(closure):
>                 closure.update(rhs)
>                 added = True
>                 
>     return closure
> 
> # Compute the closure for each option's left-hand side
> closure_bcd = closure('BCD', fd)
> closure_adg = closure('ADG', fd)
> closure_cde = closure('CDE', fd)
> closure_acg = closure('ACG', fd)
> print(closure_bcd, closure_adg, closure_cde, closure_acg)
> ```



**属性集闭包**：$A\subseteq R,F$ 下由 $A$ 函数确定的所有属性的集合为 $F$ 下属性集 $A$ 的闭包 $A^+$

作用：

- 判断 $\alpha\to\beta$ 是否在 $F^+$ 等价于 $\beta\subseteq\alpha_F^+$ 是否成立
- 判断 $\alpha$ 是否是超码，计算 $\alpha^+$ 是否包含 $R$ 所有属性
- 判断 $\alpha$ 是否是候选码，检验 $\alpha$ 所有子集闭包是否不存在包含 $R$ 所有属性的
- 求函数是否在 $F^+$ 。$\forall\gamma\subseteq R$，求 $\gamma^+$，若 $\forall S\subseteq\gamma^+$，可知 $\gamma\to S$

**候选码计算** 

1. 将属性分类：
   - $L$ 类：只在函数依赖左部出现
   - $R$ 类：只在函数依赖右部出现
   - $N$ 类：在函数依赖左右都未出现
   - $LR$ 类：在函数依赖左右都出现
2. 结论：
   - $L$ 类属性一定是任一候选码成员；若闭包为 $U$ 就是唯一候选码
   - $R$ 类属性不在任何候选码中
   - $N$ 类属性在任一候选码中
3. 步骤：
   1. 找出 $X=L\cup N$，若 $X^+=R$ 就是唯一候选码
   2. 否则，尝试增加一个属性(在 $LR$ 找)，增加后闭包全了就是一个候选码，这样的结果可能有零个或多个
   3. 在找到的之外，继续尝试增加两个、三个……属性，看看还有没有别的候选码

> 如：
>
> -  $r(R)=(A,B,C,D),F=\{B\to C,D\to A\}$，找到 $L=BD$ 为候选码一部分，且 $(BD)^+=R$，所以是唯一候选码
> - $r(R)=(A,B,C,D,E),F=\{A\to B,BC\to E,ED\to A\}$，$L=CD,(CD)^+=CD$；所以判断，发现 $(ACD)^+=(BCD)^+=(CDE)^+=R$，所以存在三个候选码；由于 $R-CD-ABE=\varnothing$，所以不用继续判断了
> - $r(R)=(A,B,C,D,E,G)$, $F=\{B\to ADE,A\to BE,AC\to G,BC\to D\}$；得 $L=C,C^+=C$，那么在 $AB$ 里抽单属性，发现 $(AC)^+=(BC)^+=R$，之后不用往下判断了
> - $r(R)=(A,B,C,D,E,F),F=\{AEF\to C,BD\to C,EF\to D,ACDE\to F\}$ 得 $L=ABE,LR=CDF$，抽 $F$，得 $R$；抽 $C,D$ 不得 $R$，但抽 $CD$ 得 $R$，所以两个候选码为 $ABEF,ABCDE$；一共有 $5$ 个超码

> 正则覆盖略

无损连接：$r_1(R_1),r_2(R_2)$ 子模式，对任意一个满足 $F$ 的关系实例 $r$ 都有 $\prod_{R_1(r)}\bowtie\prod_{R_2(r)}=r$，则该分解对于 $F$ 是无损连接的

**无损连接分解** [lossless factorization] 当且仅当 $F^+$ 包含 $R_1\cap R_2\to R_1$ 或 $R_1\cap R_2\to R_2$，即 $F$ 下 $R_1\subseteq(R_1\cap R_2)^+$ 或  $R_2\subseteq(R_1\cap R_2)^+$。充要条件：公共属性包含 $r_1(R_1)$ 或 $r_2(R_2)$ 的码，即 $R_1\cap R_2$ 是 $r_1(R_1)$ 或 $r_2(R_2)$ 的超码

> 如：$r(R)=r(A,B,C,D,E)$, $F=\{A\to BC,CD\to E,B\to D,E\to A\}$ 
>
> - $r_1(R_1)=r_1(A,B,C),r_2(R_2)=r_2(A,D,E)$，由于 $R_1\cap R_2=A,A\to R_1$ 所以是
> - $r_1(R_1)=r_1(A,B,C),r_2(R_2)=r_2(C,D,E)$ 因为 $C\not\to R_1,C\not\to R_2$ 所以不是

若多个子关系模式，判断是否为无损连接分解

1. 逐步连接分解法：选择两个满足无损连接条件的子关系模式进行连接，不断重复上述操作，直到连成一个则是无损分解；如果剩下两个子模式不能无损连接就是有损

2. 表格判断法：

   1. 建立行为子关系模式，列为属性的表格，一开始哪个子关系模式有哪些属性就填到表上方格，填 $a_x$ 代表有第 $x$ 个属性
   2. 对 $F$ 每个函数依赖，当一个依赖在某一行左右都在表里时，对另外的所有行若左边所有属性在一个表格列都有，那么把右边那些属性也填到该表格对应列上
   3. 重复上述操作，若某一表格行填满了就是无损连接分解；若无论如何都填不满(遍历 $F$ 后表格不变)，那么有损

   ![image-20220626150927814](img/image-20220626150927814.png)

保持依赖分解：若 $F$ 在 $R_i$ 投影为 $F^+$ 中所有只包含 $R_i$ 属性依赖的集合 $F_i$ 满足 $\alpha\to\beta$ 在 $F_i$ 中，则 $\alpha,\beta$ 的所有属性都在 $R_i$ 中。

保持依赖分解：$(F_1\cup F_2\cup\cdots\cup F_n)^+=F^+$

> 例如 $r(R)=r(A,B,C),F=\{A\to B,B\to C\}$，则：
>
> - $r_1(R_1)=r_1(A,B),r_2(R_2)=r_2(B,C)$ 是
> - $r_1(R_1)=r_1(A,B),r_2(R_2)=r_2(A,C)$ 不是



### 模式分解算法

分解的基本原则：无损分解、保持依赖

数据库设计目标(基于函数依赖)：BCNF，无损连接，保持依赖

若不能同时达到，需要在 BCNF, 3NF 选一个

**BCNF 分解**：若 $\alpha\to\beta$ 的 $\alpha$ 不是超码，将 $r(R)$ 分解为：

- $R_1=\alpha\beta,F_1=\{\alpha\to\beta\}$，若 $\alpha\cap\beta=\varnothing$ 则 $\alpha$ 为候选码 
- $R_2=R-(\beta-\alpha)$ 

直到 $R_2$ 属于 $BCNF$ 为止，否则递归执行上述操作

> 如：
>
> - $r(R)=r(A,B,C),F=\{AB\to C,C\to A\}$，计算得候选码为 $AB$ 或 $BC$，因为 $C\to A$ 不符合 $BCNF$，故拆分为 $r_1(R_1)=r_1(A,C),r_2(R_2)=r_2(B,C)$。分解后丢失了函数依赖 $AB\to C$
> - $r(R)=(A,B,C,D,G,H)$, $F=\{A\to BC,DG\to H,D\to A\}$，求得候选码 $DG$，所以拆分为 $r_1(A,B,C),r_{21}(D,A),r_{22}(D,G,H)$ 
> - $r(R)=r(A,B,C,D,G,H)$, $F=\{AB\to GH,CD\to GH,B\to A,D\to B\}$，求得 $CD$ 候选码，拆分为 $r_1(A,B,G,H),r_{21}={BA},r_{221}={D,B},r_{222}={C,D}$ 

上述算法得到的分解不仅是BCNF分解，而且是无损分解（但可能不是保持函数依赖分解），用该算法生成的BCNF分解不是唯一的

> 3NF 分解算法：略



### 数据库模式求精

模式求精是运用关系理论(如函数依赖理论、多值依赖理论等)对已有关系模式进行结构调整、分解、合并和优化的过程，以满足应用系统的功能及性能等需求

模式分解。根据范式要求(是选择BCNF还是3NF)，运用规范化方法将关系模式分解成所要求的关系模式
模式合并。在分解过程中可能进行模式合并。如当查询经常涉及到多个关系模式的属性时，系统将经常进行连接操作，而连接运算的代价是相当高的。此时，可考虑将这几个关系合并为一个关系



## SQL数据定义、更新及数据库编程

### SQL数据定义语言

包括对数据库、基本表、视图、索引的增删改(后两个不能改)

包括：数据库、关系模式、每个属性的值域、完整性约束、每个关系的索引集合和关系的物理存储结构等

数据库保存了企业所有的数据，以及相关的一些控制信息，如安全性和完整性约束、关系的存储路径等

数据库包含了基本表、视图、索引以及约束等对象

![image-20220626163833868](img/image-20220626163833868.png)

数据库作为一个整体存放在外存的物理文件中
物理文件(即磁盘文件)有两种：
一是数据文件，存放数据库中的对象数据
二是日志文件，存放用于恢复数据库的企业冗余数据
物理文件可以是多个，可以将一个或若干个物理文件设置为一个逻辑设备
数据库可以有多个逻辑设备，必须在定义数据库时进行定义
数据库的对象存放在逻辑设备上，由逻辑设备与物理文件进行联系，从而实现数据库的逻辑模式与存储模式的独立

数据库定义(SQL server)：长度 1 到 30 之丰富，首必须字母、下划线或@，之后可以有数字，不能有空格。数据库的大小可以被扩展或者收缩

> 如：
>
> ```sql
> CREATE DATABASE ScoreDB 
> ON    -- 定义第一个逻辑设备(默认为主逻辑设备)及其数据文件
>       (  NAME=ScoreDB,     -- 数据文件的逻辑文件名(即别名)
>          FILENAME='e:\SQLDatabase\ScoreDB.mdf',   -- 物理(磁盘)文件名 
>          SIZE=2, 
>          MAXSIZE=10, 
>          FILEGROWTH=1 ) 
> LOG ON         -- 定义日志逻辑设备及其日志文件
>       (  NAME=ScoreLog,     -- 日志文件的逻辑文件名(即别名)
>          FILENAME='e:\SQLDatabase\ScoreLog.ldf',   -- 日志(磁盘)文件名 
>          SIZE=1, 
>          MAXSIZE=5, 
>          FILEGROWTH=1 )
> ```
>
> SQL Server 可以 `alter database 数据库名 动词 名词……` 

通过将基本表与逻辑设备相关联，使得一个基本表可以放在一个数据文件(磁盘文件)上，也可以放在多个数据文件上

> SQL中的基本数据类型是：
> 整型：int (4B)，smallint (2B)，tinyint (1B)；
> 实型：float，real (4B)，decimal(p, n)，numeric(p, n)；
> 字符型：char(n)，varchar(n)，text；
> 2进制型：binary(n)，varbinary(n)，image；
> 逻辑型：bit，只能取0和1，不允许为空；
> 货币型：money (8B, 4位小数)，small money (4B, 2位小数)；
> 时间型：datetime (4B, 从1753.1.1开始)，
>                      smalldatetime (4B, 从1900.1.1开始)
> 其中：image为存储图象的数据类型，text存放大文本数据

基本**表的定义**：(基本表的名称，最多可包含 128 个字符) (**约束定义**)

```sql
 CREATE TABLE <tableName>
      (  <columnName1> <dataType>
                  [DEFAULT <defaultValue>] [null | NOT null], 
         [ <columnName2> <dataType>
                  [DEFAULT <defaultValue>] [null | NOT null],  … ]
        [ [CONSTRAINT <constraintName1>] {UNIQUE | PRIMARY KEY}
                  (<columName> [, <columName>…]) [ON <filegroupName>], ... ] 
        [ [CONSTRAINT <constraintName2>] 
                  FOREIGN KEY (<columName1> [, <columName2>…] ) 
                  REFERENCE [<dbName>.owner.]<refTable>
                         (<refColumn1> [, <refColumn2>… ]) [ON <filegroupName>], ... ] 
     ) [ON <filegroupName>] 
```

> 如：
>
> ```sql
> CREATE TABLE Course (          -- 创建课程表Course
>     courseNo	      char(3) 	                             NOT NULL,     --课程号
>     courseName   varchar(30)   UNIQUE            NOT NULL,     --课程名
>     creditHour     numeric(1)     DEFAULT  0     NOT NULL,     --学分
>     courseHour    tinyint            DEFAULT  0      NOT NULL,     --课时数
>     priorCourse   char(3)		                NULL,               --先修课程
>     /* 建立命名的主码约束和匿名的外码约束 */
>     CONSTRAINT CoursePK PRIMARY KEY (courseNo),
>     FOREIGN KEY (priorCourse)  REFERENCES  Course(courseNo)  
> )              -- 外码约束是匿名的
> ```
>
> ```sql
> CHECK (studentNo LIKE '[0-9][0-9][0-9][0-9][0-9][0-9][0-9]'),    --学号
> ```

修改：

增加列（新增一列的值为空值）：
ALTER TABLE <tableName>
     ADD <columnName> <dataType>
增加约束：
ALTER TABLE <tableName>
     ADD CONSTRAINT <constraintName>
删除约束：
ALTER TABLE <tableName>
     DROP <constraintName>
修改列的数据类型：
ALTER TABLE <tableName>
     ALTER COLUMN <columnName> <newDataType>

> ```sql
> [例7.6]  在MyTempDB数据库中为TempTable表增加一列。
> ALTER TABLE TempTable
> ADD xsex int DEFAULT 0
> [例7.7]  在MyTempDB数据库中为TempTable表的xname 列修改数据类型。
> ALTER TABLE TempTable
> ALTER COLUMN xname char(10)
> [例7.8]  在MyTempDB数据库中为TempTable表的xname列增加唯一约束
> ALTER TABLE TempTable
> ADD CONSTRAINT UniqueXname UNIQUE (xname)
>     注意：基本表在修改过程中，不可以删除列，一次仅执行一种操作。
> ```

删除：DROP TABLE <tableName> [RESTRICT | CASCADE]

若选择RESTRICT，则该基本表的删除有限制条件
即该基本表不能有视图、触发器以及被其它表所引用(如检查约束CHECK，外码约束FOREIGN KEY)，该项为缺省项。
若选择CASCADE，则该基本表的删除没有限制条件
在删除基本表的同时，也删除建立在该基本表上的所有索引、完整性规则、触发器和视图等。

> SQL Server不支持 [RESTRICT | CASCADE]选项

索引：一个基本表可以建立多个索引。由<搜索码值，指针>的记录构成
索引中的记录(称为索引项)按照搜索码值的顺序进行排列，但不改变基本表中记录的物理顺序；索引和基本表分别存储。

索引的类型有：聚集或非聚集，非聚集索引就是普通索引，一个基本表可以建立多个普通索引。
每个基本表仅能建立一个聚集索引

聚集索引按搜索码值的某种顺序(升/降序)来重新组织基本表中的记录
即索引的顺序就是基本表记录存放的顺序
聚集索引可以极大地提高查询速度，但是给数据的修改带来困难
建立了聚集索引的基本表一般仅执行查询操作，很少进行更新操作，这在数据仓库中使用得较多。

索引建立：

```sql
CREATE [UNIQUE] [CLUSTERED | NONCLUSTERED] INDEX <indexName>
 ON <tableName> ( <columnName1> [ASC | DESC]
                                  [, <columnName2> [ASC | DESC] … ] )
 [ON <filegroupName>]
```

UNIQUE：建立唯一索引(要求索引属性或属性组上的取值必须唯一)；
CLUSTERED | NONCLUSTERED ：表示建立聚集或非聚集索引，默认为非聚集索引；

> 如：
>
> ```mysql
> CREATE NONCLUSTERED INDEX InstituteIdx 
> ON Class(institute)
> CREATE INDEX ClassBirthIdx 
> ON Student(classNo, birthday DESC)
> DROP INDEX <indexName> ON <tableName>
> ```



### SQL数据更新语言

**全节重点**

包括插入、删除和修改数据

插入：(没有列出的属性列自动取空值NULL或默认值)

```sql
INSERT INTO <tableName> [ (<columnName1> [, <columnName2> ... ] ) ] 
VALUES (<value1> [, <value2> ... ] ) 
```

> 如：
>
> ```sql
> insert into student(sno,sname,ssex,sdept,sage) 
> select fields from table
> insert into student(sno,sname,ssex,sdept,sage)
> select '200215128','陈冬','男','IS','18'
> union all
> select '200215129','陈冬','男','IS','18'
> ```
>
> ```sql
> DELETE FROM Score 
> WHERE studentNo IN (
>     SELECT studentNo 
>     FROM Score 
>     GROUP BY studentNo
>     HAVING avg(score) BETWEEN 60 AND 70 )
> ```
>
> ```sql
> UPDATE Score 
> SET score=88
> WHERE courseNo='002' AND termNo='151' 
>       AND studentNo IN 
>       (  SELECT studentNo FROM Student 
>                     WHERE studentName='王红敏' )
> ```
>
> ```sql
> UPDATE Score 
> SET score=88
> FROM Score a, Student b
> WHERE a.studentNo=b.studentNo AND courseNo='002' 
>       AND termNo='151' AND studentName='王红敏'
> ```
>
> ```sql
> UPDATE Class 
> SET classNum=sCount 
> FROM Class a, 
>   ( SELECT classNo, count(*) sCount
>    FROM Student
>    GROUP BY classNo ) b
> WHERE a.classNo=b.classNo
> ```

插入、删除和修改操作会破坏数据的完整性，如果违反了完整性约束条件，其操作会失败



### 视图

**全节重点**

视图是虚表，是从一个或几个基本表(或视图)中导出的表。(**概念**)

基于视图的操作包括：查询、删除、受限更新和创建基于该视图的新视图

视图的主要作用是：(**特点**)

- 简化用户的操作；
- 使用户能以多种角度看待同一数据库模式；
- 对重构数据库模式提供了一定程度的逻辑独立性；
- 能够对数据库中的机密数据提供一定程度的安全保护；
- 适当的利用视图可以更清晰的表达查询

```sql
CREATE VIEW <viewName> [(<columnName1> [, <columnName2> ... ]) ]
AS <subquery>
[WITH CHECK OPTION] -- 有的话增删改必须满足
```

> ```sql
> CREATE VIEW StudentView1999
> AS SELECT * FROM Student WHERE year(birthday)=1999      -- 创建视图中的谓词条件
> WITH CHECK OPTION
> ```
>
> ```sql
> CREATE VIEW StudentAgeView
> AS SELECT studentNo, studentName, year(getdate())-year(birthday) age FROM Student
> ```
>
> ```mysql
> SELECT * FROM SourceView WHERE courseAvg>=80
> -- 对聚合函数的限制转化为 having
> ```

当视图是基于一个基本表创建的，且保留了主码属性，这样的视图称为行列子集视图

从系统的角度来看，查询视图的过程是：

- 有效性检查：检查查询中涉及的基本表和视图是否存在？
- 从数据字典中取出创建视图的语句，将创建视图的子查询与用户的查询结合起来，转换成等价的对基本表的查询；
- 执行改写后的查询

**更新限制条件**

如果创建视图的语句中包含了表达式，或聚合运算，或消除重复值运算，则不能对视图进行更新操作

一般来讲，如果是行列子集视图，则可以对该视图进行更新操作；其它类型的视图，具体的数据库系统有具体的定义，一般不对其进行更新操作

```sql
DROP VIEW <viewName> [CASCADE]
```

如果该视图上还导出了其他视图，使用CASCADE级联删除语句，把该视图和由它导出的所有视图一起删除

删除基本表时，由该基本表导出的所有视图都必须显式地使用DROP VIEW语句删除



### T-SQL语言简介

```sql
局部变量：变量名前加1个@符号
全局变量：变量名前加2个@@符号。如:
@@ERROR：当事务成功时为0，否则为最近一次的错误号
@@ROWCOUNT：返回受上一语句影响的行数
@@FETCH_STATUS：返回最近的FETCH语句执行后的游标状态
变量的声明与赋值
声明变量的语法：
    DECLARE <@variableName> <datatype> [, <@variableName> <datatype> … ]
单个变量赋值的语法：SET <@variableName> = <expr>
变量列表赋值(或显示表达式的值)的语法：
  SELECT <@variableName> [= <expr | columnName>] 
   [, <@variableName> [= <expr | columnName>] … ]
```

![image-20220626182518593](img/image-20220626182518593.png)

![image-20220626182526072](img/image-20220626182526072.png)

![image-20220626182538681](img/image-20220626182538681.png)

![image-20220626182548079](img/image-20220626182548079.png)

程序块范围用 BEGIN-END



### 游标

对SELECT语句的结果集进行逐行处理，需使用游标。
游标(cursor)是系统为用户开设的一个数据缓冲区，用于存放SQL语句的执行结果(元组集合)。每个游标都有一个名字，用户可以用SQL提供的语句从游标中逐一获取元组(记录)，并赋给主变量，交由主语言进一步处理。
可对游标的当前位置进行更新、查询和删除，使用游标需要经历5个步骤：
定义游标：DECLARE
打开游标：OPEN
逐行提取游标集中的行：FETCH
关闭游标：CLOSE
释放游标：DEALLOCATE

```sql
DECLARE <cursorName> CURSOR 
FOR <SQL-Statements>
 [FOR{ READ ONLY | UPDATE [OF <columnName_list>]}] 
```



### 存储过程

存储过程是为了完成特定功能汇集而成的一组命名了的SQL语句集合 (**概念**)

使用存储过程具有如下优点：(**特点**)

- 将业务操作封装
- 便于事务管理
- 实现一定程度的安全性保护
- 减少网络通信量，特别适合统计和查询操作

使用存储过程时，必须执行命令EXECUTE

注意：由于存储过程是在服务器端执行，程序中不需要有输出命令SELECT，由SELECT引出的输出不会在客户端出现。预编译好的。是数据库对象



### 触发器

触发器(trigger)是用户定义在关系表上的一类由事件驱动的存储过程，由服务器自动激活。(**概念**)

是一种特殊的存储过程，不管什么原因造成的数据变化都能自动响应。常用于保证完整性，一定程度上实现完全性 (**特点**)

可以同步数据库的相关表进行级联更新，SQL Server 支持 DDL 触发器

特殊的触发器表：

在SQL Server中使用deleted表和inserted表；
Oracle数据库使用old表和new表

触发器仅在当前数据库中被创建
触发器有3种类型，即插入、删除和修改；
插入、删除或修改也可组合起来作为一种类型的触发器

> ```mysql
> CREATE TRIGGER sexIns    -- 创建插入类型的触发器
> ON Student                                -- 触发器作用的基本表
> FOR INSERT,UPDATE          -- 触发器的类型，即触发该触发器的事件
> AS
>     IF EXISTS (SELECT * FROM inserted WHERE sex NOT IN ('男', '女'))
>           ROLLBACK    -- 事务的回滚操作，即终止触发该触发器的插入操作
> ```



## 数据库存储结构与查询处理

### 索引与散列

**索引的基本概念、作用、优缺点**

索引可以提升查询、分组和排序的性能；全表扫描不一定比使用索引的执行效率低；索引应该建在选择性高的字段上；针对某些字段建立索引,能够有效的减少相关数据库表的磁盘空间占用

顺序索引(ordered index)：索引中的记录(索引项)基于搜索码值顺序排列

组织结构：在索引中按搜索码值的顺序存储索引项，并将索引项与包含该索引项中搜索码值的文件记录关联起来(通过指针).

用于支持快速地对文件中的记录进行顺序或随机地访问

![image-20220626193916032](img/image-20220626193916032.png)

散列索引(hash index)：索引中的记录(索引项)基于搜索码值的散列函数(即哈希函数)的值平均、随机地分布到若干个散列桶中

建立了索引的文件称为索引文件(指数据文件)。索引文件中的记录自身可以按照某种排序顺序存储。一个索引文件可以有多个索引，分别对应于不同的搜索码。
如果索引文件中的记录按照某个搜索码值指定的顺序物理存储，那么该搜索码对应的索引就称为主索引(primary index)，也叫聚集索引(clustering index)。 (存储了实际数据)
与此相反，搜索码值顺序与索引文件中记录的物理顺序不同的那些索引称为辅助索引(secondary index)或非聚集索引(non-clustering index)

对于索引顺序文件，顺序索引有两类：稠密索引和稀疏索引。稀疏索引就是没有每个搜索码都建立索引

![image-20220626194201541](img/image-20220626194201541.png)

通过多级索引技术能够较好地解决上述问题。所谓多级索引就是在索引之上再建立索引

辅助索引必须是稠密索引，即对于每个搜索码值都必须有一个索引项，而且该索引项要存放指向数据文件中具有该搜索码值的所有记录的指针

可以通过指针桶的方式实现，即将数据文件中具有该搜索码值的所有记录的指针存放在一个指针桶中，索引项中的指针域再存放指向指针桶的指针(可以理解为指向指针数组的指针)

![image-20220626194433626](img/image-20220626194433626.png)



**B+树索引**

B+树索引是一个多级索引，但其结构不同于多级顺序索引

B+树索引采用平衡树结构，即每个叶结点到根结点的路径长度相同

B+树索引中的所有结点的结构都相同，它最多包含n-1个搜索码值和n个指针，每个节点搜索码值升序存放

每个非叶子节点有 $[\lceil\dfrac n2\rceil,n]$ 个儿子节点

指针指向具有搜索码值的一条文件记录或一个指针桶，且指针桶中的每个指针指向具有搜索码值的一条文件记录

![image-20220626194605145](img/image-20220626194605145.png)

![image-20220626194736964](img/image-20220626194736964.png)

![image-20220626194837620](img/image-20220626194837620.png)

B+树索引中的非叶结点形成叶结点上的一个多级(稀疏)索引

非叶结点的结构与叶结点相同，只不过非叶结点中的所有指针都是指向B+树中下一层结点的指针

根结点与其他非叶结点不同，它包含的指针数可以小于 $\lceil\dfrac n2\rceil$，但是除了整棵B+树只有一个结点之外，根结点中必须至少包含两个指针

插入和删除比查找更加复杂，因为结点可能因为插入而变得过大需要分裂，或因为删除而变得过小(指针数少于 $\lceil\dfrac n2\rceil$ )而需要合并

此外，当一个结点分裂或一对结点合并时，必须保证B+树能保持平衡

B+树文件组织是通过在B+树的叶结点层直接包含真实的数据记录，以解决索引顺序文件组织中随着文件的增大而性能下降的缺点

在B+树文件组织中，B+树结构不仅用做索引，同时也是文件中记录的组织者，树叶结点中存储的是记录

基于B+树的索引技术能有效地支持范围检索，并且它的等值检索效果也很好



散列索引(hash index)将搜索码值及其相应的文件记录指针组织成一个散列索引项

将散列函数作用于一条文件记录的搜索码值，以确定该文件记录所对应的散列索引项的散列桶；
将由该搜索码值以及相应文件记录指针组成的散列索引项存入散列桶(或溢出桶)中

![image-20220626200352878](img/image-20220626200352878.png)

散列其实就是一种不通过值的比较，而通过值的含义来确定存储位置的方法，它是为有效地实现等值查询而设计的；基于散列技术不支持范围检索



### 查询处理

查询处理(query processing)是指从数据库中提取数据时所涉及的一系列活动 (**概念**)

包括：(**查询处理过程**)

- 语法分析与翻译
- 查询优化
- 查询执行

<u>助记：分翻优执(纷繁幼稚)</u>

![image-20220626202024684](img/image-20220626202024684.png)

- 语法分析与翻译：检查用户查询的语法，并利用数据字典验证查询中出现的关系名、属性名等是否正确；构造该查询语句的语法分析树，并将其翻译成关系代数表达式

- 查询执行计划与查询优化器：一个给定的查询任务，一般都会有多种计算结果的方法 (最主要的模块)

  执行一个查询，不仅需要提供关系代数表达式，还要对该关系代数表达式加上注释来说明如何执行每个关系运算。生成查询执行计划

  不同的查询执行计划会有不同的代价。构造具有最小查询执行代价的查询执行计划称为查询优化，由查询优化器来完成

  查询优化是影响RDBMS性能的关键因素

- 查询执行引擎：根据输入的查询执行计划，调用相关算法实现查询计算，并将计算结果返回给用户

  有效地对内存缓冲区进行管理是影响查询执行性能的非常重要的方面

查询处理的代价：主要包括磁盘存取时间、执行一个查询所用的CPU时间、以及在并行/分布式数据库系统中的通信开销等

对于大型数据库系统而言，在磁盘上存取数据的代价通常是最重要的代价 ，可以通过传输磁盘块数以及搜索磁盘次数来度量

在代价估算时，通常假定是最坏的情形

用于选择运算的搜索算法有：
不用索引的搜索算法——文件扫描，包括

线性搜索算法A1
二分搜索算法A2 

使用索引的搜索算法——索引扫描，包括

在主索引的码属性上的等值比较算法A3
在主索引的非码属性上的等值比较算法A4
在辅助索引上的等值比较算法A5
在主索引上的范围比较算法A6
在辅助索引上的范围比较算法

数据库的很多查询都涉及到连接运算，因此连接运算的效率就成为衡量RDBMS(关系数据库管理系统)性能的一个主要指标

实现连接运算的主要算法有： (全表扫描和索引扫描)

- (块)嵌套循环连接算法(nested-loop join, NLJ) ——最基本的、通用的算法 
- 索引嵌套循环连接算法(index nested-loop join, INLJ) 
- 归并连接算法
- 散列连接算法

其他运算：

- 排序
- 去除重复元组
- 投影
- 集合运算
- 聚集运算 



### 查询优化

处理一个给定的查询，尤其是复杂的查询，通常会有许多种执行策略。
查询优化(query optimization)就是从这许多策略中找出最有效的查询执行计划的处理过程。 
期望RDBMS能够构造并选择出一个具有最小查询执行代价的查询执行计划

查询优化三步：（1,3交叉进行，为主要步骤）

1. 逻辑优化，即产生逻辑上与给定关系代数表达式等价的关系代数表达式；

2. 代价估计，即估计每个查询执行计划的代价；

3. 物理优化，即对所产生的表达式以不同方式作注释，产生不同的查询执行计划 (根据系统所提供的存取路径,选择合理的存取策略)

等价规则：(**关系表达式转换**)

![image-20220626202930549](img/image-20220626202930549.png)

如何为一个给定查询选择最佳查询执行计划？(**查询优化策略**)
一种方法是搜索所有的查询执行计划，基于代价估计选择最佳的查询执行计划
第二种是使用启发式策略选择查询执行计划
实际中的查询优化器将两种方法结合起来使用

基于代价的优化器(cost-based optimizer)通过使用等价规划从给定的查询语句产生一系列查询执行计划，并选择代价最小的一个。对于一个复杂的查询，等价于给定查询的不同查询执行计划可能很多。

基于代价的优化器在实际应用中，不可能也没必要对所有可能的查询执行计划进行穷举搜索，通常寻找接近最优的查询执行计划

**启发式优化准则** 启发式(heuristics)规则

1. 尽早执行选择操作
2. 尽早执行投影运算(以1为前提)

(而不是先要对文件进行预处理)

将频繁访问的视图物化并进行维护,有利于提高查询效率；将频繁地向数据库发送的某条查询语句用存储过程来代替,可以提高查询效率

笛卡尔积和连接运算最费时间



### 物理数据库设计

**全节重点**

数据库在物理设备上的存储结构与存取方法称为数据库的物理结构，它依赖于给定的计算机系统。
为一个给定的逻辑数据模型选取一个最适合应用环境的物理结构的过程，就是数据库的物理设计。

目标：
提高数据库性能，以满足应用的性能需求；
有效利用存储空间；
在性能和空间之间做出最优平衡。

内容：
确定数据库的存储结构；
为数据选择合适的存取路径，即索引的设计；
对物理结构进行评价，重点是评价时间和空间效率

确定数据存放位置 ：为了提高系统性能，数据应该根据应用情况将易变部分和稳定部分、经常存取部分和存取频率较低部分分开来存放。

确定数据库存储结构：确定数据库存储结构时要综合考虑存取时间、存储空间利用率和维护代价三个方面的因素。这三个方面常常是相互矛盾的。例如，消除一切冗余数据虽然能够节约存储空间，但往往会导致检索代价的增加，因此必须进行权衡，选择一个折衷方案

在关系数据库中，选择存取路径主要是指确定如何建立索引。例如:
应选择哪些属性作为搜索码建立索引
建立多少个索引
建立聚集索引(主索引)，还是非聚集索引(辅助索引)
建立单码索引还是组合索引，等等

常用的文件存储方式有三种：索引方法、聚集方法和散列(Hash, 哈希)方法。目前使用最普遍的是B+树索引 

通常情况下，系统配置参数包括：同时使用数据库的用户数，同时打开数据库对象数，使用的缓冲区长度、个数，时间片大小，数据库的大小，装填因子，锁的数目等。这些参数值影响存取时间和存储空间的分配，在数据库物理设计时要根据应用环境确定这些参数值，以使系统性能最优

注意，在数据库物理设计时对系统配置参数的调整只是初步的，在系统运行时还要根据系统实际运行情况做进一步的调整，以期切实改进系统性能

数据库物理设计过程中，需要对时间效率、空间效率、维护代价和各种用户要求进行权衡，其结果可以产生多种方案

数据库设计人员必须对这些方案进行细致的评价，从中选择一个较优的方案作为数据库的物理结构

影响物理设计的主要因素：

- 应用处理需求。在进行数据库物理设计前，应先弄清应用的处理需求，如吞吐量、平均响应时间、系统负荷、事务类型及发生频率等，这些需求直接影响着设计方案的选择，而且它们还会随应用环境的变化而变化。
- 数据特征。数据本身的特性对数据库物理设计也会有较大影响。如关系中每个属性值的分布、记录的长度与个数等，这些特性都影响到数据库的物理存储结构和存取路径的选择。
- 运行环境。数据库物理设计与运行环境有关，因此在设计时还要充分考虑DBMS、操作系统、网络、计算机硬件等运行环境的特征和限制。
- 物理设计的调整。数据库物理设计是基于数据库的当前状况选择的一个合适的设计方案。数据库的状态和特性也会发生变化，因此可能导致以前的物理设计不能再满足目前的应用需求，因此，需对物理设计不断调整，甚至是重新设计

<u>助记：应运特调</u>



## 数据库安全性与完整性

### 数据库安全性

**概述**

数据库的安全性：保护数据库以防止不合法使用所造成的数据泄密、更改或破坏

数据库的完整性：防止数据库中存在不符合语义的数据，其防范对象是不合语义的、不正确的数据 （正确性和相容性）

安全性、完整性、并发控制是必须的，可移植性不是

数据库系统中大量数据集中存放，且为许多最终用户直接共享

数据库安全保护目标是确保只有授权用户才能访问数据库，未被授权的人员则无法接近数据

安全措施是指计算机系统中用户直接或通过应用程序访问数据库所要经过的安全认证过程

![image-20220626205816440](img/image-20220626205816440.png)

- 用户标识与鉴别(identification & authentication)

  当用户访问数据库时，要先将其用户名(user name)与密码(password)提交给数据库管理系统进行认证；
  只有在确定其身份合法后，才能进入数据库进行数据存取操作

- 数据库安全保护

  通过身份认证的用户，拥有了进入数据库的“凭证”；
  用户在数据库中执行什么操作，需通过“存取控制”或视图进行权限分配

存取控制：决定用户对数据库中的哪些对象进行操作，进行何种操作。存取控制机制主要包括两部分：

- 定义用户权限及将用户权限登记到数据字典中；

- 合法权限检查：当用户发出操作请求后，DBMS查找数据字典并根据安全规则进行合法权限检查，若操作请求超出了定义的权限，系统将拒绝执行此操作

- 操作系统安全保护
  通过操作系统提供的安全措施来保证数据库的安全性 

- 数据密码存储

  访问控制和存取控制可将用户的应用系统访问范围最小化和数据对象操作权限最低化，但对一些敏感数据进行“加密存储”也是系统提供的安全策略；
  数据加密(data encryption)：防止数据库中数据存储和传输失密的有效手段；
  加密的基本思想：先根据一定的算法将原始数据(即明文, plaintext)加密为不可直接识别的格式(即密文, ciphertext)，然后数据以密文的方式存储和传输

视图：通过为不同的用户定义不同的视图，达到限制用户访问范围的目的。(提高了安全性，不提高隔离性、完整性)

视图机制能隐藏用户无权存取的数据，从而自动地对数据库提供一定程度的安全保护；
视图的主要功能在于提供数据库的逻辑独立性，其安全性保护不太精细，往往不能达到应用系统的要求；
在实际应用中，通常将视图与存取控制机制结合起来使用，如先通过视图屏蔽一部分保密数据，然后进一步定义存取权限

审计：是一种监视措施，用于跟踪并记录有关数据的访问活动

审计追踪把用户对数据库的所有操作自动记录下来，存放在审计日志(audit log)中；
审计日志的内容一般包括：
操作类型(如修改、查询、删除)；
操作终端标识与操作者标识；
操作日期和时间；
操作所涉及到的相关数据(如基本表、视图、记录、属性)；
数据库的前映像(即修改前的值)和后映像(即修改后的值)。
利用这些信息，可找出非法存取数据库的人、时间和内容等；
数据库管理系统往往将审计作为可选特征，允许操作者打开或关闭审计功能

定义审计功能，必然会带来系统的开销，DBMS一般不启用审计功能

制定审计方针的一些参考意见：
最小化审计选项来降低审计跟踪记录个数，如仅跟踪user01用户。
监视或定期删除审计跟踪记录。
避免审计跟踪记录被非法用户删除。定义的审计表只能DBA操作，任何其它用户没有操作权限

主体：指数据库的访问者，包括用户、进程和线程等；
客体：指数据库中的数据和载体，如基本表、视图、存储过程和数据文件等；
主体与客体是独立的，一个主体可以在一定条件下访问某些客体。

自主存取控制(Discretionary Access Control, 简记为DAC)
DAC是一种基于存取矩阵的存取控制模型；
此模型由三种元素组成，即主体、客体和存取操作。三种元素构成一个矩阵，列表示主体，行表示客体，矩阵中的元素表示存取操作，如读(R)、写(W)、删除(D)和修改(U)操作
在DAC中，主体按存取矩阵的要求访问客体，存取矩阵中的元素可通过授权方式进行修改

强制存取控制(Mandatory Access Control, 简记为MAC)
MAC不是用户能直接感知或进行控制的；
MAC适用于对数据有严格而固定密级分类的部门，如军事部门或政府部门；
对于主体和客体，DBMS为每个实例(值)指派了一个敏感度标记(label)；
敏感度标记被分成若干级别，例如绝密(top secret)、机密(secret)、可信(confidential)和公开(public)等；
主体的敏感度标记称为许可证级别(clearance level)，客体的敏感度标记称为密级(classification level)；
MAC对比主体和客体的敏感度标记，确定主体是否能够存取客体
规则1：仅当主体的许可证级别大于或等于客体的密级时，该主体才能读取相应的客体；
规则2：仅当主体的许可证级别等于客体的密级时，该主体才能写相应的客体
仅当主体的许可证级别小于或等于客体的密级时，才能写相应的客体；
即用户可以为写数据对象赋予高于自己的许可证级别的密级，这样一旦数据被写入，该用户自己也不能再读该数据对象了
两种规则的共同点在于：
禁止拥有高许可证级别的主体更新低密级的数据对象，从而防止敏感数据泄漏.
强制存取控制是对数据本身进行密级标记，标记与数据是不可分的整体；只有符合密级标记要求的用户才可操纵数据，从而提供更高级别的安全性

隐蔽通道
在主体访问客体时，一般通过正常路径访问；但在网络数据流中利用隐蔽通道进行非法通信已成为威胁网络数据库安全的一种重要手段；
在数据库安全中，一定要寻找和防止隐蔽通道的出现，一旦发现要采取措施加以阻塞。
数据库安全的形式化
由于数据库的安全在整个系统中的重要性，必须建立一套有效的形式化体系用于保证其自身正确性，发现并填补安全漏洞，防止隐蔽通道，为数据库安全的研究提供理论依据；
目前数据库安全的形式化已成为高级数据库安全的必要条件.
访问监控器
访问监视器是一个独立的物理机构，由一定的软件和硬件共同组成，它能够监视主体对客体的全部访问活动

最有影响的标准为TCSEC（Trusted Computer System Evaluation Criteria，美国国防部标准，1985年）和          CC（Common Criteria for IT Security Evaluation，ISO标准，1999年）。 
TCSEC将系统划分为４组共七个级别，依次为D，C(C1, C2)，B(B1, B2, B3)和A(A1)：
D：最低级，DOS属于该级别，几乎没有专门的安全机制;
C1：初级的自主安全保护。将用户和数据分离，实现自主存取控制，限制用户权限的传播；
C2：安全产品的最低档，提供受控的存取保护，实施审计和资源隔离：
如Windows 2008、Oracle11g、SQL Server 2014等；
在SQL中，通过授权语句GRANT和REVOKE来实现
B1：标记安全保护。对数据加以标记，对主体和客体实施强制存取控制：
这类产品一般加上安全或信任字样，如Trusted Oracle7；
该标准的核心是强制存取控制。
B2：隐蔽通道和形式化，适合于网络工作方式，目前国内外尚无符合该标准的数据库系统。
B3：访问监控器，适合于网络工作方式，目前国内外尚无符合该标准的数据库系统。
A1：验证设计，较高的形式化要求，仅仅是一种理想化的等级，目前没有相应的系统

国际标准化组织提出的CC文本由三部分组成：简介及一般模型、安全功能要求、安全保证要求。
我国于1999年颁布了国家标准，其标准与TCSEC标准相似



SQL支持受控的存取保护：
在自主存取控制中，用户对不同的数据对象有不同的存取权限；
不同的用户对同一对象有不同的权限；
用户可将其拥有的存取权限转授给其他用户。
自主存取控制通过SQL的**GRANT**和**REVOKE**语句实现

用户权限：是指用户可以在哪些数据对象上进行哪些类型的操作。它由两个要素组成：数据对象和操作类型。
定义存取权限称为授权(authorization)；
授权粒度可以精细到字段级，也可以粗到关系级；
授权粒度越细，授权子系统就越灵活，但是系统的开销也会相应地增大
自主存取控制：用户可自由地决定将数据的存取权限授予何人，决定是否将“授权”的权限授予别人

授权分为数据库级、表级和列级权限。
在SQL Server中权限只能由担任不同角色的用户来分配；
不同类型的用户有不同的等级；
下图给出了授权等级图

![image-20220626210640325](img/image-20220626210640325.png)

对数据库模式的授权则由DBA在创建用户时实现

GRANT和REVOKE有两种权限：目标权限和命令权限

命令权限的授予与收回
主要指DDL操作权限，语法分别为：

```sql
GRANT {all | <command_list>} TO {public | <username_list>} 
REVOKE {all | <command_list>} FROM {public | <username_list>}
<command_list>可以是create database、create default、create function、create procedure、create rule、create table、create view、create index、backup database和backup log等
```

如果具有创建对象的create权限，则自动具有其创建对象的修改alter权限和删除drop权限

对于基本表，自动具有在所创建表上创建、删除和修改触发器的权限；
修改alter和删除drop权限不额外授权；

all：表示上述所有权限

public：表示所有的用户



目标权限(DML)：

```sql
 GRANT {all | <command_list>} ON <objectName> [(<columnName_list>)] 
TO {public | <username_list>} [WITH GRANT OPTION] 
 REVOKE {all | <command_list>} ON <objectName> [(<columnName_list>)] 
FROM {public | <username_list>} [CASCADE | RESTRICT]
```

<command_list>可以是update、select、insert、delete、execute和all
execute针对存储过程授予执行权限；
update、select、insert、delete针对基本表和视图授权

对象的创建者自动拥有该对象的插入、删除、修改和查询操作权限；存储过程的创建者自动拥有所创建过程的执行权限

CASCADE：级联收回；
RESTRICT：缺省值，若转赋了权限，则不能收回；
WITH GRANT OPTION：允许将指定对象上的目标权限授予其它安全用户

不允许循环授权，即不允许将得到的权限授予其祖先

> ```sql
> GRANT create table, create view TO u1, u2
> GRANT all ON Score TO u1 WITH GRANT OPTION
> GRANT select, insert ON Score TO u5
> REVOKE select ON Score FROM u6 
> ```

数据库角色是指被命名的一组与数据库操作相关的权限；
角色是权限的集合，可以为一组具有相同权限的用户创建一个角色



### 数据库完整性

**概述**

为维护数据库的完整性，数据库管理系统提供如下功能:
完整性约束条件定义机制
完整性约束条件也称为完整性规则，是数据库中的数据必须满足的语义约束条件；
由SQL的DDL实现，作为模式的一部分存入数据库中。
完整性检查方法
检查数据是否满足已定义的完整性约束条件称为完整性检查；
一般在insert、delete、update执行后开始检查，或事务提交时进行检查。
违约处理措施
若发现用户操作违背了完整性约束条件，应采取一定的措施，如拒绝操作等

完整性约束条件作用的对象可以是关系、元组、列三种：
列约束：在定义属性的同时定义该属性应满足的约束条件，主要是定义属性的数据类型、取值范围和精度、默认值、是否允许空值、是否唯一、单一属性主码等约束条件；
元组约束：定义元组中属性间的联系的约束条件；在定义属性之后单独定义；
关系约束：定义若干元组间、关系集合上以及关系之间的联系的约束条件，亦称为表约束；在定义属性之后单独定义。

完整性约束，其状态可是静态的，也可是动态的。

静态约束：指数据库每一确定状态时的数据对象所应满足的约束条件。
反映数据库状态合理性的约束；

静态约束主要表现在：

静态列约束：对列的取值域的说明，包括以下几方面：
对数据类型的约束，包括数据的类型、长度、单位、精度等；
对数据格式的约束；
对取值范围或取值集合的约束；
对空值的约束；
其他约束。
静态元组约束：规定元组的各个列之间的约束关系。
静态关系约束：在一个关系的各个元组之间或者若干关系之间常存在各种联系或约束。常见的静态关系约束有：
实体完整性约束；
参照完整性约束；
函数依赖约束，大部分函数依赖约束都在关系模式中定义；
统计约束，字段值与关系中多个元组的统计值之间的约束关系。

动态约束：指数据库从一种状态转变为另一种状态时的新、旧值之间所应满足的约束条件。
反映数据库状态变迁的约束；
动态约束主要表现在：
动态列约束：修改列定义或列值时应满足的约束条件。
修改列定义时的约束。如，将允许空值的列改为不允许空值时，如果该列已存在空值，则拒绝这种修改。
修改列值时的约束。修改列值有时需要参照其旧值，并且新旧值之间需要满足某种约束条件。
例如，职工工资调整不得低于其原来工资，学生年龄只能增长等。
动态元组约束：指修改元组的值时元组中各个字段间需要满足某种约束条件。
例如，职工工资调整时，新工资不得低于原工资+工龄*1.5。
动态关系约束：动态关系约束是加在关系变化前后状态上的     限制条件。
例如，事务一致性、原子性等约束条件。

完整性约束又分为立即执行的约束和延迟执行的约束：
立即执行约束(immediate CONSTRAINTS)：检查是否违背完整性约束的时机是在一条语句执行完后立即检查。
延迟执行约束(deferred CONSTRAINTS)：需要延迟到整个事务执行结束后再进行检查。



**SQL Server 完整性** **实现方法**

用户可以为完整性约束命名，命名格式如下：

一是便于理解约束的含义；
二是修改约束方便，不必查询数据字典

使用ALTER TABLE语句修改基本表中的完整性约束。
要修改约束，必须先删除约束，然后加入新的约束

```sql
[ CONSTRAINT <constraintName> ]
  PRIMARY KEY (<constraintExpr>)
[ CONSTRAINT <constraintName> ]
  FOREIGN KEY (<constraintExpr>) REFERENCES <refTable>(<constraintExpr>) 
[ CONSTRAINT <constraintName> ] 
  CHECK (<constraintExpr>)
```



实体完整性要求基本表的主码值唯一且不允许为空值

- 用 `primary key` 或 `alter` 里 `add primary key` 

- 对单属性构成的主码可定义为列约束, 也可定义为元组约束;
  对多个属性构成的主码，只能定义为元组约束(也有的书将其划归关系约束，即表约束) 

  > ```sql
  > CONSTRAINT ClassPK PRIMARY KEY (classNo)
  > CONSTRAINT ScorePK PRIMARY KEY (studentNo, courseNo, termNo)
  > PRIMARY KEY (studentNo, courseNo, termNo)
  > ```

- 检查主码值是否唯一，如果不唯一则拒绝插入或修改；
  检查主码的各个属性是否为空，只要有一个为空则拒绝插入或修改

- unique 关键字(唯一性约束)

参照完整性为若干个基本表中的相应元组建立联系

- `foreign key` `references` 或 `alter` 的 `add`

- 给出FOREIGN KEY定义的关系称为参照关系；
  由REFERENCES指明的基本表称为被参照关系

- > ```sql
  > CONSTRAINT ScoreFK1 FOREIGN KEY (studentNo)
  > REFERENCES Student(studentNo), -- 表约束
  > FOREIGN KEY (studentNo) REFERENCES Term (studentNo)
  > ```

- 如果外码是主码的一部分，则外码不允许为空值

  (外码是某个关系的码，体现了关系间的联系手段，是一个关系的码同时是另一个关系的属性)

- 当在被参照关系中删除元组时，仅当参照关系中没有任何元组的外码值与被参照关系中要删除元组的主码值相同时，系统才执行删除操作，否则拒绝此操作

- 参照关系中可以随意删除元组

- 级联(CASCADE)操作。当删除或修改被参照关系的某些元组造成了与参照关系的不一致时，则自动级联删除或修改参照关系中所有不一致的元组

  > ```sql
  > CONSTRAINT ScoreFK1 FOREIGN KEY (studentNo)
  >    REFERENCES Student (studentNo)
  >    ON DELETE CASCADE     /* 级联删除参照关系Score中相应的元组 */
  >    ON UPDATE CASCADE,    /* 级联修改参照关系Score中相应的元组 */
  > CONSTRAINT ScoreFK2 FOREIGN KEY (courseNo)  
  >       REFERENCES Course (courseNo)
  >       ON DELETE NO ACTION 	  -- 该定义为默认值，可以不定义 
  >       -- 当修改被参照表Course中的courseNo时, 级联修改参照关系Score中相应的元组 
  >       ON UPDATE CASCADE
  > ```

用户自定义完整性就是定义某一具体应用中数据必须满足的语义要求，由RDBMS提供，而不必由应用程序承担。
用户自定义完整性包括属性上的约束和元组上的约束两种

- 列约束

  - 属性上的约束包括：数据类型、列值非空、列值唯一、设置默认值、满足CHECK (<predicate>)定义等；
    属性上的约束：当向基本表中插入或修改属性值时，系统检查是否满足约束条件，若不满足，则拒绝相应的操作

  - > ```sql
    > sex char(2) NULL CHECK (sex IN ('男', '女')),
    > age	tinyint DEFAULT 16 NULL 
    > CONSTRAINT ageCK CHECK ( age > 0 AND age < 60 ),
    > CHECK ( studNo LIKE '[0-9][0-9][0-9][0-9][0-9]' ) -- 五位数
    > CHECK ( studNo LIKE '[S, M, D, s, m, d, 1, 2, 3][0-9][0-9][0-9][0-9]' )
    > ```

- 元组约束

  - 元组上的约束可以设置不同属性之间的取值的相互约束条件；
    用短语CHECK (<predicate>)引出的约束；
    插入元组或修改属性的值时，RDBMS检查元组上的约束条件是否被满足，若不满足，则拒绝相应的操作

  - > ```sql
    > CONSTRAINT SexCK CHECK (sex='女' OR studName NOT LIKE '刘%'),
    > ```

如果在定义属性的同时定义约束条件，则为列约束；
如果单独定义约束条件，则为元组约束

```sql
ALTER TABLE <tableName> DROP CONSTRAINT <constraintName>
ALTER TABLE <tableName>
   ADD CONSTRAINT <constraintName> 
   < CHECK | UNIQUE | PRIMARY KEY | FOREIGN KEY >  (<constraintExpr>)
```

> ```sql
> ALTER TABLE Stud
>     ADD CONSTRAINT sNoCK 
>     CHECK ( studNo BETWEEN '15001' AND '25999' )
> ALTER TABLE Stud
>     DROP CONSTRAINT ageCK
> ```

也可以规则和触发器实现完整性



## 事务管理与恢复

### 事务

**全节重点**

为解决上述问题，数据库管理系统引入了事务概念，它将这些有内在联系的操作当作一个逻辑单元看待，并采取相应策略保证一个逻辑单元内的全部操作要么都执行成功，要么都不执行。
对数据库用户而言，只需将具有完整逻辑意义的一组操作正确地定义在一个事务之内即可

**概念**

对于用户而言，事务是具有完整逻辑意义的数据库操作序列的集合。
对于数据库管理系统而言，事务则是一个读写操作序列。这些操作是一个不可分割的逻辑工作单元，要么都做，要么都不做。
事务是数据库管理系统中竞争资源、并发控制和恢复的基本单元。 它是由数据库操作语言(如SQL)或高级编程语言（如Java、C、C++）提供的事务开始语句、事务结束语句以及由它们包含的全部数据库操作语句组成

事务结束的类型：

- 事务提交(commit)：将成功完成事务的执行结果(即更新)永久化，并释放事务占有的全部资源。
- 事务回滚(rollback)：中止当前事务、撤销其对数据库所做的更新，并释放事务占有的全部资源

三种事务模式：

- 显式事务是指用户使用Transact-SQL事务语句所定义的事务，其事务语句包括：
  - 事务开始：BEGIN TRANSACTION
  - 事务提交：COMMIT TRANSACTION，COMMIT WORK
  - 事务回滚：ROLLBACK TRANSACTION，ROLLBACK WORK
- 隐式事务是指事务提交或回滚后，系统自动开始新的事务。该类事务不需要采用BEGIN TRANSACTION语句标识事务的开始。
- 自动定义事务：当一个语句成功执行后，它被自动提交，而当执行过程中出错时，则被自动回滚

**特性** 为了保证事务并发执行或发生故障时数据库的一致性（完整性），事务应具有以下ACID特性

- 原子性(atomicity)。事务的所有操作要么全部都被执行，要么都不被执行。

- 一致性(consistency)。一个单独执行的事务应保证其执行结果的一致性，即总是将数据库从一个一致性状态转化到另一个一致性状态。

  > [this](https://zhuanlan.zhihu.com/p/382569958) 事务执行前后，数据的改变都是合法，且各方数据都是一致的(如主键、外键用户自定义)
  >
  > (百科)是指事务的运行并不改变数据库中数据的一致性。例如，完整性约束了a+b=10，一个事务改变了a，那么b也应该随之改变。

- 隔离性(isolation)。当多个事务并发执行时，一个事务的执行不能影响另一个事务，即并发执行的各个事务不能互相干扰。

- 持久性(durability)。一个事务成功提交后，它对数据库的改变必须是永久的，即使随后系统出现故障也不会受到影响

原子性也称为故障原子性或(故障)可靠性
由DBMS通过撤销未完成事务对数据库的影响来实现。

一致性是指单个事务的一致性，也称为并发原子性或正确性
由编写该事务代码的应用程序员负责，但有时也可利用DBMS提供的数据库完整性约束(如触发器)的自动检查功能来保证。

隔离性也称为执行原子性或可串行化，可以看作是多个事务并发执行时的一致性或正确性要求
由DBMS的并发控制模块保证。

持久性也称为恢复原子性或恢复可靠性
它是利用已记录在稳固存储介质(如磁盘阵列)中的恢复信息(如日志、备份等)来实现丢失数据(如因中断而丢失的存放在主存中但还未保存到磁盘数据库中去的数据等)的恢复。
它是由DBMS的恢复管理模块保证。



数据库管理系统允许多个事务并发执行：
优点
增加系统吞吐量(throughput)。吞吐量是指单位时间系统完成事务的数量。当一事务需等待磁盘I/O时，CPU可去处理其它正在等待CPU的事务。这样，可减少CPU和磁盘空闲时间，增加给定时间内完成事务的数量。
减少平均响应时间(average response time)。事务响应时间是指事务从提交给系统到最后完成所需要的时间。事务的执行时间有长有短，如果按事务到达的顺序依次执行，则短事务就可能会由于等待长事务导致完成时间的延长。如果允许并发执行，短事务可以较早地完成。因此，并发执行可减少事务的平均响应时间。
缺点
若不对事务的并发执行加以控制，则可能破坏数据库的一致性

问题：

读脏数据。如果事务T2读取事务T1修改但未提交的数据后，事务T1由于某种原因中止而撤销，这时事务T2就读取了不一致的数据。数据库中将这种读未提交且被撤销的数据为读“脏数据”。
丢失更新。两个或多个事务都读取了同一数据值并修改，最后提交事务的执行结果覆盖了前面提交事务的执行结果，从而导致前面事务的更新被丢失
不可重复读。是指事务Ti两次从数据库中读取的结果不同，可分为三种情况：
事务Ti读取一数据后，事务Tj对该数据进行了更改。当事务Ti再次读该数据时，则会读到与前一次不同的值。	
事务Ti按某条件读取数据库中某些记录后，事务Tj删除了其中部分记录。当事务Ti再次按相同条件读取时，发现记录数变少了。（幻影现象1）
事务Ti按某条件读取数据库中某些记录后，事务Tj插入了新的记录。当事务Ti再次按相同条件读取时，发现记录数变多了。（幻影现象2）

死锁不属于并发操作带来的问题



启示：如果一组并发执行事务的执行结果与它们串行执行得到的结果是相同的，那么就可以认为该并发执行的结果是正确的

事务并发执行顺序是随机的，将由多个事务操作组成的随机执行序列称为一个调度。
对由一组事务操作组成的调度序列而言，应满足下列条件：
 该调度应包括该组事务的全部操作；
 属于同一个事务的操作应保持在原事务中的执行顺序

在调度S中，如果属于同一事务的操作都是相邻的，则称S是串行调度。
对由n个事务组成的一组事务而言，共有n!种有效串行调度

事务串行执行可保证数据库的一致性，如果能判断一个并发调度的执行结果等价于一个串行调度的结果，就称该并发调度可保证数据库的一致性

假设调度S包含两个事务Ti与Tj，若两个相邻操作OiTi，OjTj访问不同的数据对象，则交换Oi与Oj不会影响调度中任何操作的结果。若Oi与Oj访问相同的数据对象，并且有一个为写操作时，则不能改变它们被调度执行的顺序

在一调度S中，如果Oi与Oj是不同事务在相同数据对象上的操作，并且其中至少有一个是写操作，则称Oi与Oj是冲突操作；否则称为非冲突操作

如果一调度S可以经过交换一系列非冲突操作执行的顺序而得到一个新的调度S’，则称S与S’是冲突等价的(conflict equivalent)

如果一调度S与一串行调度是冲突等价的，则称S是冲突可串行化的(conflict serializable).

冲突可串行化仅仅是正确调度的充分条件，并不是必要条件，即冲突可串行化调度执行结果一定是正确的，而正确的调度不一定都是冲突可串行化的

设S是一个调度。由S构造一个有向图，称为优先图，记为G=(V, E)，其中V是顶点集，E是边集。顶点集由所有参与调度的事务组成，边集由满足下列3个条件之一的边Ti→Tj组成：
Ti执行了Wi(Q)后Tj执行Rj(Q)；
Ti执行了Ri(Q)后Tj执行Wj(Q)；
Ti执行了Wi(Q)后Tj执行Wj(Q)

![image-20220626231738804](img/image-20220626231738804.png)

基于优先图的冲突可串行化判别准则：如果优先图中无环，则S是冲突可串行化的；如果优先图中有环，则S是非冲突可串行化的

测试冲突可串行化的算法：

构建S的优先图；
采用环路测试算法（如基于深度优先搜索的环检测算法）检测S中是否有环；
若S包含环，则S是非冲突可串行化的，否则调度S是冲突可串行化的

![image-20220626231904182](img/image-20220626231904182.png)

![image-20220626231926179](img/image-20220626231926179.png)



### 并发控制

**封锁概念**

DBMS 普遍采用封锁保证调度正确性(解决并发操作带来的数据不一致问题)

并发控制机制大体上可分为悲观的和乐观的两种。
悲观的并发控制方法认为数据库的一致性经常会受到破坏，因此在事务访问数据对象前须采取一定措施加以控制，只有得到访问许可时，才能访问数据对象，如基于封锁的并发控制方法。——事前控制
乐观的并发控制方法则认为数据库的一致性通常不会遭到破坏，故事务执行时可直接访问数据对象，只在事务结束时才验证数据库的一致性是否会遭到破坏，如基于有效性验证方法。——事后验证
本章介绍基于封锁的并发控制方法

基本思想：当事务T需访问数据对象Q时，先申请对Q的锁。如批准获得，则事务T继续执行，且此后不允许其他任何事务修改Q，直到事务T释放Q上的锁为止。
基本锁类型：
共享锁（shared lock, 记为S）：如果事务T获得了数据对象Q的共享锁，则事务T可读Q但不能写Q。
排它锁（exclusive lock, 记为X）：如果事务T获得了数据对象Q上的排它锁，则事务T既可读Q又可写Q。

“锁相容”是指如果Ti已持有数据对象Q的某类型锁后，事务Tj也申请对Q的封锁。如果允许事务Tj获得对Q的锁，则称事务Tj申请锁类型与事务Ti的持有锁类型相容；否则称为不相容。
基本锁类型的封锁相容性原则：
共享锁与共享锁相容
排它锁与共享锁、排它锁与排它锁是不相容的。

申请和释放锁操作：
SL(Q)——申请数据对象Q上的共享锁；
XL(Q)——申请数据对象Q上的排它锁；
UL(Q)——释放数据对象Q上的锁

死锁是事务故障



### 恢复与备份

对于一个事务而言，它是通过三个地址空间同数据库进行交互：
保存数据库记录的磁盘块空间——物理数据库；
缓冲区管理器所管理内存地址空间——数据缓冲区；
事务的局部地址空间——事务工作区。
数据元素A在不同地址空间之间移动的操作原语包括：
INPUT(X)：将包含数据库记录A的磁盘块X拷贝到数据缓冲区。
READ(A)：将数据记录A从缓冲区拷贝到事务工作区。如果包含A的磁盘块X不在缓冲区中，则首先执行INPUT(X)操作。
WRITE(A)：将A的修改值从事务工作区拷贝到缓冲区X块中以替换A的值。如果此时X块不在内存缓冲区中，则应先执行INPUT(X)后再替换。
OUTPUT(X)：将更新后的X块写回磁盘

注意的是发出这些命令的对象是不同的：READ和WRITE由事务发出，而INPUT和OUTPUT由缓冲区管理器发出

对于不同类型的故障在恢复时应做不同的恢复处理。
恢复的本质是利用存储的冗余数据（如日志、影子、备份副本(数据库后备副本)等）来重建数据库中已经被破坏或已经不正确的那部分数据。
DBMS中的恢复管理模块由两部分组成：
正常事务处理过程中：系统需记录冗余的恢复信息，以保证故障发生后有足够的信息进行数据库恢复；
故障发生后：利用冗余信息进行UNDO或REDO等操作，将数据库恢复到一致性状态。 

**日志概念** DBMS记录数据库全部更新操作的序列文件

主要特点有：
日志文件记录了数据库的全部更新顺序。
日志文件是一个追加（append-only）文件。
DBMS允许事务的并发执行导致日志文件是“交错的”。
属于单个事务的日志顺序与该事务更新操作的执行顺序是一致的。
日志记录通常是先写到日志缓冲区中，然后写到稳固存储器（如磁盘阵列）中

类型：
记录数据更新操作的日志记录，包括UPDATE、INSERT和DELETE操作；
记录事务操作的日志记录, 包括START、COMMIT和ABORT操作

格式：

<Ti, A, V1, V2>表示事务Ti对数据元素A执行了更新操作，V1表示A更新前的值(前映像)， V2表示A更新后的值(后映像)。对于插入操作，V1为空；对于删除操作，V2为空。
<Ti, START>表示事务Ti已经开始。此时DBMS完成对事务的初始化工作，如分配事务工作区等。
<Ti, COMMIT>表示事务Ti已经提交，即事务Ti已经执行成功（该事务对数据库的修改必须永久化）。事务提交时其更新的数据都写到了数据缓冲区中，但是由于不能控制缓冲区管理器何时将缓冲块从内存写到磁盘。因此当看到该日志记录时，通常不能确定更新是否已经写到磁盘上。
<Ti, ABORT>表示事务已经中止，即事务执行失败。此时，如果Ti所做的更新已反映到磁盘上，DBMS必须通过UNDO操作来消除Ti对磁盘数据库的影响

**基于日志的故障恢复策略**

为了保证数据库能运用日志进行恢复，要求日志文件必须放到稳固存储器(如磁盘阵列)上，并且要求每条日志记录必须在其所包含数据记录的更新值写到外存储器之前先写到稳固存储器上，即先写(write-ahead)日志规则

对于要UNDO的事务T，日志中记录有<T, START>以及T对数据库的所有更新操作的日志记录。
UNDO过程为：从T的最后一条更新日志记录开始，从日志尾向日志头(反向)依次将T更新的数据元素值恢复为旧值(V1)。
如果增加一条规则：T对数据库的所有更新值在日志<T, COMMIT>写到磁盘前不能写到磁盘。这样，如果没有看到事务T 的<T, COMMIT>日志记录，就可以确定T的更新值一定没有写到磁盘，从而可避免UNDO操作

与UNDO相反，REDO操作是对已提交事务进行重做，将数据库状态恢复到事务结束后的状态。
对于要REDO的事务T，日志中已经记录了<T, START>、T的所有更新操作日志以及<T, COMMIT>。
REDO过程为：从T的第一条更新日志记录开始，从日志头向日志尾(顺向)依次将T更新的数据元素值恢复为新值(V2)。
如果增加一条规则：事务对数据库的所有更新值必须在提交日志<Ti, COMMIT>写到磁盘之前写到磁盘，就可以避免REDO操作

总结：

- 崩溃区间有 start 没 commit，执行 undo
- 崩溃区间有 start 有 commit ，执行 redo

三个阶段：

分析阶段：从日志头开始顺向扫描日志，确定重做事务集（REDO-set）和撤销事务集（UNDO-set）。将既有<T, START>又有<T, COMMIT>日志记录的事务T加入REDO-set；将只有<T, START>没有<T, COMMIT>日志记录的事务T加入UNDO-set。
撤销阶段：从日志尾反向扫描日志，对每一条属于UNDO-set中事务的更新操作日志依次执行UNDO操作。
重做阶段：从日志头顺向扫描日志，对每一条属于REDO-set中事务的更新操作日志依次执行REDO操作

UNDO与REDO必须是幂等的，即重复执行任意次的结果与执行一次的结果是一样的

为了减少扫描开销和提高恢复效率，引入了检查点技术

检查点是周期性地向日志中写一条检查点记录并记录所有当前活跃的事务，为恢复管理器提供信息，以决定从日志的何处开始恢复。
检查点工作主要包括：
将当前位于日志缓冲区的所有日志记录输出到磁盘上；
将当前位于数据缓冲区的所有更新数据块输出到磁盘上；
记录日志记录<Checkpoint L>并输出到磁盘上，其中L是做检查点时活跃事务的列表

在检查点执行过程中，不允许事务执行任何更新动作，如写缓冲块或写日志记录，称其为静态检查点技术。
如果事务T在做检查点之前就已提交，那么它的<T, COMMIT>记录一定出现在<Checkpoint L>记录前，并且其更新在做Checkpoint时都已写到磁盘中，因此不需要对T做任何恢复操作，这样可大大减少恢复工作量。
如果数据缓冲区及日志缓冲区中缓存的更新数据很多时，就会导致系统长时间不能接受事务处理，这对响应时间要求较严格的系统来说是不可忍受的。为避免这种中断，可使用模糊检查点(fuzzy checkpoint)技术，允许在做检查点的同时接受数据库更新操作

![image-20220626233719454](img/image-20220626233719454.png)

撤销阶段：首先，从日志尾反向扫描日志文件至遇到最后一次检查点止，对每一条属于UNDO-set中事务的更新操作日志依次执行UNDO操作

从最后一次检查点开始顺向扫描日志，对每一条属于REDO-set中事务的更新操作日志依次执行REDO操作

![image-20220626234023882](img/image-20220626234023882.png)



数据备份(Backup)是数据库管理系统用来进行介质故障恢复的常用方法。它是由DBA周期性地将整个数据库的内容复制到其他外存储器上(通常为大容量的磁带或磁鼓)保存起来

数据备份操作可分为静态备份和动态备份。
静态备份是在系统中无运行事务时进行的备份操作。优点是简单。但由于备份必须等待用户事务结束后才能进行，而新的事务必须等待备份结束后才能执行，因此会降低数据库的可用性。
动态备份是指备份操作与用户事务的执行并发进行，备份期间允许对数据库进行存取或修改。动态备份克服了静态备份的缺点，它不用等待正在运行的用户事务结束，也不会影响新事务的运行。但它不能保证副本中的数据正确有效

具体进行数据备份时可以分全备份和增量备份。
全备份是指每次备份全部数据库，而增量备份是只备份上次备份后更新过的数据。
从恢复角度看，一般说来使用全备份得到的后备副本进行恢复会更方便些。但如果数据库很大，事务处理又十分频繁，则增量备份方式更实用更有效。
不管是那种形式的备份及介质故障的恢复处理，都需要DBA介入。由于备份又是十分耗费时间和资源的，不能频繁进行。所以DBA应该根据数据库使用情况确定适当的备份周期和备份方法



## 查询优化

### 基本概念

#### 查询类型

SPJ queries 表示 "Select-Project-Join"

- inner join 如果在任一表中的行找不到匹配项，这些行就不会出现在结果集中
- left join 右表中没有匹配项，则结果集中相应的列会包含 NULL 值。基本上，它包括了所有左表的行，无论是否有匹配的右表行。
- right join；vice versa
- full join；如果某一侧没有匹配项，那么结果集中该侧的列将包含 NULL 值。

Theta join($\theta$ join)使用等于，不等于，大于小于等链接。inner join 是特殊的 theta join

disjunction 指 or 连接多个逻辑条件

semi join 指从一个表中返回那些在另一个表中有对应值的记录

> ```sql
> SELECT *
> FROM Employees
> WHERE DepartmentID IN (SELECT DepartmentID FROM Departments WHERE DepartmentName = 'IT');
> ```
>

self join 一个表作为两个不同的实体连接到自身(如先修课程)

> ```sql
> SELECT e1.Name AS EmployeeName, e2.Name AS ManagerName
> FROM Employees e1
> JOIN Employees e2 ON e1.ManagerID = e2.EmployeeID;
> ```
>

cyclic join 可以从一个表通过一系列的连接操作回到这个表的起点

> ```sql
> SELECT
>  a.*, b.*, c.*
> FROM
>  TableA a
> JOIN
>  TableB b ON a.SomeColumn = b.OtherColumn
> JOIN
>  TableC c ON b.AnotherColumn = c.DifferentColumn
> JOIN
>  TableA a2 ON c.YetAnotherColumn = a2.SomeColumn;
> ```

#### 查询实现方式

> 对 join 语句如：
>
> ```sql
> SELECT E.Name, D.DepartmentName
> FROM Employees E
> JOIN Departments D ON E.DepartmentID = D.DepartmentID;
> ```

通常可能性能 nested-loop < hash < sort-merge

nested-loop join：

```pseudocode
for each row R1 in table1 do
    for each row R2 in table2 do
        if R1.joinKey == R2.joinKey then
            output combined row (R1, R2)
        end if
    end for
end for
```

hash join:

```pseudocode
// 构建阶段
create HashTable
for each row R1 in smallerTable do
    hash = hashFunction(R1.joinKey)
    insert R1 into HashTable at hash
end for

// 探测阶段
for each row R2 in largerTable do
    hash = hashFunction(R2.joinKey)
    for each row R1 in HashTable at hash do
        if R1.joinKey == R2.joinKey then
            output combined row (R1, R2)
        end if
    end for
end for
```

sort-merge join:

```pseudocode
sort table1 by joinKey
sort table2 by joinKey

i = 0
j = 0
while i < length(table1) and j < length(table2) do
    if table1[i].joinKey == table2[j].joinKey then
        output combined row (table1[i], table2[j])
        i++
        j++
    else if table1[i].joinKey < table2[j].joinKey then
        i++
    else
        j++
    end if
end while
```



#### 选择器

cardinality estimation CE 基数估计。求一个集合的不同元素的数目。

> - 数据库 $\mathcal D$ 是三元组 $(\mathcal D_T,\mathcal D_J,\mathcal D_A)$，分别表示数据表集，PK-FK joins 和属性。表 $T\in \mathcal D_T$ 有属性集 $\mathcal A_T\subseteq \mathcal D_A$。
>
> - 对 $D$ 的查询 $Q$ 是三元组 $(T(Q),J(Q),R(Q))$，分别表示表格($\subseteq\mathcal D_T$)，内连接谓词 $(\subseteq\mathcal D_J)$ 和谓词连接(conjunction)即 $\wedge\{(A,op,val)|A\in\mathcal D_A\}$。
>
> - 给定 $D,Q$，CE 为下面语句的结果 $card(Q)$：
>
>   ```sql
>   select count(*) from T(Q) where J(Q) and R(Q)
>   ```
>



常见的对数据的假设：

- 均匀(uniformity) 数据集中的每个值出现的概率是相同的

- 独立(independency) 一个变量的值不会影响或预测另一个变量的值

- 包含性原则principle of inclusion) 

- > [参考](https://zhuanlan.zhihu.com/p/377694688) 认为 $R.a\bowtie_p S.b$ 的选择率(过滤的行的比例)为 $\dfrac1{max(|dom(R.a)|,|dom(S.b)|)}$ 其中 dom 是值域大小 

  

#### 其他

UDF(User-Defined Function)

"workload"(工作负载) 通常指的是对数据库系统的一系列请求或操作，这些操作可以是查询、更新、事务处理等

### 基本过程

#### 查询优化的流程

查询优化的整体过程：

![image-20231223145104726](img/image-20231223145104726.png)

查询优化器的结构：

![image-20231223145141731](img/image-20231223145141731.png)

逻辑优化器：查询重写

<img src="img/image-20231223145316364.png" alt="image-20231223145316364" style="zoom: 33%;" />

物理优化器：

<img src="img/image-20231223145410682.png" alt="image-20231223145410682" style="zoom:33%;" />

#### 基数估计

##### 选择

选择性因子(SF, selectivity factor) $\dfrac{cardinality}{\#tuples}$ 

> 满足特定条件的元组数除以总元组数

假设：

- 均匀 uniformity

  $\sigma_{A=c}(R)\to SF=\dfrac1{V(R,A)}$ 

  (关系R中属性A的不同值的数量)

  $\sigma_{A <c}(R)\to SF=\dfrac{c-Low(R,A)}{High(R,A)-Low(R,A)}$ 

  (离最小值的距离除以极值差的大小) 

- 独立 independence

  Cond1 and Cond2 $\to SF=SF(Cond1)\cdot SF(Cond2)$

  Cond1 or Cond2 $\to SF=SF(Cond1)+SF(Cond2)-SF(Cond1)SF(Cond2)$

  not Cond1 $\to SF=1-SF(Cond1)$

- 值包含 containment of values

  $R_{\bowtie A=B} S\to SF=\dfrac1{\max(V(R,A),V(S,B))}$

  > 这是保守估计：元素较多的表每个元组都满足 $A=B$。这是因为一共有 $V(R,A)V(S,B)$ 列的话，实际上认为每个列都与另一张表的具体一列匹配了。

- 值保留 preservation of values

  $V(R_{\bowtie A=B}S,C)=V(R,C)$

  连接操作后，与连接无关的其他列的不同值的数量保持不变。

评估过程：

```sql
Q = SELECT list FROM R1, ... Rn
WHERE cond1 AND cond2 AND ... AND condk
```

- 评估结果数是 T(Q)

- 每张表的元素数是 T(R1), T(R2), ..., T(Rn)

  则 $SF(R_1.A=v)=\dfrac{T(R_1)}{V(R_1,A)}$, 

  $SF(R_1.A=R_2.B)=\dfrac{T(R_1)T(R_2)}{\max(V(R_1,A),V(R_2,B)}$

- $T(Q)=\prod_{i=1}^nT(R_i)\prod_{i=1}^kSF(cond_i)\le \prod_{i=1}^nT(R_i)$

##### 谓词

对谓词(predicate) P 的选择性(sel, selectivity)是元组满足条件的概率：

- equality $sel(P(c=x))=\dfrac{count(c=x)}{count(all)}$

- range $sel(P(c\ge x))=\dfrac{max-x+1}{max-min+1}$

- negation $sel(P(c\neq x))=1-sel(P(c=x))$

- conjunction (and) 独立假设

  $sel(P1\& P2)=sel(P1)sel(P2)$

- disjunction (or)

  $sel(P1\ or\ P2)=sel(P1)+sel(P2)-sel(P1)sel(P2)$

> ##### join

##### 直方图

histogram

设常数 $NumB$，将每列值域分为 $NumB$ 块，频数分布直方图 (参考 mit6.830)：等宽划分。

或者其他划分(euidepth)，使得每个桶的计数差不多一样多。如十分位数。

技巧：End-biased histogram(末端偏置)。在数据分布的两端分配更多的桶/出现频率很多的值更细粒度的桶。



草图

sketch

直方图的一种替代方法，代价模型(cost-model)，计算一列的值如 age=20。

sketch，是一种概率数据结构，生成数据集的近似统计特征。

常见例子：

- count-min sketch (1988)：近似频率计数元素
- hyperloglog (2007)：近似不同的元素数



##### 计数最小化草图

朴素：

定义：给定一列(一系列值)，建立哈希函数 H 和草图 M，有 w 个条目(entries)

- `M[i]` 初始化为 0，其中 $0\le i\le w-1$
- 每个 v 值，操作 `M[H[v]%w]++`
- `M[H[v]%w]` 可以计算 v 的数量

会高估(overestimate)，因为哈希冲突。

count-min sketch

使用 d 个哈希函数降低冲突。矩阵 M 有 d 行 w 列。则：

- 每个 v 值和每个哈希函数 $h_i$，$M[i][h_i(v)]+=1$

给定 v，则频次为 $f(x)=\min_{0\le i\le d-1}M[i][h_i(v)]$。取最小冲突作为答案。

> 不同的哈希函数如 $v\bmod w,v^2\bmod w,(2v+1)\bmod w,(3v^2+1)\bmod w$

##### 超对数草图

hyperloglog sketch

要解决的问题：统计一组数据中不重复元素的个数。[参考](https://zhuanlan.zhihu.com/p/77289303)

关键见解：设有完美哈希函数 h 将 `[1,r]` 整数映射到 `[0,n)`

注意到每个整数，有 $i$ 个前导零的概率是 $\dfrac1{2^{i+1}}$.即 $w < \log n$ 的前导零数目是 $\dfrac1{2^w}$



朴素 loglog 算法：

设有 `dv` 个不同的元素。对每个元素 v，先求哈希值将其映射为 01 字符串，然后统计低位前导 0 个数 `C0`，将 `dv` 与 $2^{C0}$ 取最值。

关键见解：最大前导 0 的数目的指数可以粗略估计元素的数目。

<img src="img/image-20231223161926217.png" alt="image-20231223161926217" style="zoom:50%;" />

考虑升级为几位几位的考虑，看看有多少个前导 `00,01,10,11`，将计数求平均然后乘幂再乘要看的前导种类。即：前导种类为 $m$，前导长度为 $w$。

![image-20231223162500756](img/image-20231223162500756.png)

最后再乘一个常数 $\alpha_m$，具体为：
$$
\alpha_m=\begin{cases}
0.673,&m=16\\
0.698,&m=32\\
0.709,&m=64\\
\dfrac{0.7213}{1+\dfrac{1.079}m},&m\ge128
\end{cases}
$$

> $a$ 的几何平均数是 $\sqrt[m]{\prod a_i}$，调和平均数为 $\dfrac{m}{\sum\dfrac1{a_i}}$

将 $2^{a_i}$ 的几何平均数 $2^{\sum a_i\div m}$ 用调和平均数代替，计算公式变为 $\dfrac{\alpha_m m^2}{\sum 2^{-w}}$

则简明(succinctly)描述 loglog 算法，首先构造一个映射 `[1,x]` 到 `[0,n)` 的哈希函数，然后：

1. 初始化数组 w，大小为 $m=2^b$，初始值为 0
2. 对每个元素 x，求哈希值 h(x)
   1. 将 h(x) 分成两部分前 b 位(从左往右)和后 m-b 位
   2. 设 c 是前 b 位组成的值，$w_0$ 是后 m-b 的前导 0 数目
   3. 设 $w_c=\max(w_c,w_0)$
3. 求 $\alpha_m m 2^{\sum w_c\div m}$

则 hyperloglog 为： 将第三步更换成调和平均值换过的公式。

采样

现代 DBMS 会从表格采样来评估选择性。表格变化明显时更新采样。

可以添加概念列，或与 join 独立采样。

![image-20231223171157235](img/image-20231223171157235.png)

##### 花费模型总结

cost model

基于采样，直方图，草图。

![image-20231223171329515](img/image-20231223171329515.png)

**基数估计**

- 基数(cardinality) 询问的大小
- 输入：SQL 查询语句；输出：估计的基数大小

![image-20231223171524209](img/image-20231223171524209.png)

采样：

- 核心观点：用采样评估目标询问的选择性。
- 限制：数据大时推断(inference)慢、不准确。

直方图：

- 存储每个属性的分布，使用独立假设计算选择性。
- 限制：强独立假设让复杂数据分布不准确。

草图：评估不同值元素的数目。

#### 询问重写

##### 基本规则

定义：转换一个逻辑计划为另一个等价的计划(通常代价更低)

理论依据：关系代数等量关系(equivalence)

基于规则的方法：应用重写规则

- 谓词下推 push-down predicates
- 尽早投影 do project early
- 避免叉乘 avoid cross-products if possible
- 用左偏树 use left-deep trees
- 使用限制 use of constrains
- 换子查询 subqueries -> join

常用规则：
$$
\begin{align}
\sigma_{p_1\wedge p_2}(e)&\equiv \sigma_{p_1}(\sigma_{p_2}(e))&\quad(1)\\
\sigma_{p_1}(\sigma_{p_2}(e))&\equiv \sigma_{p_2}(\sigma_{p_1}(e))&(2)\\
\Pi_{A_1}(\Pi_{A_2}(e))&\equiv\Pi_{A_1}(e)\ \mathrm{if}\ A_1\subseteq A_2&(3)\\
\sigma_p(\Pi_A(e))&\equiv \Pi_A(\sigma_p(e))\ \mathrm{if}\ \mathcal F(p)\subseteq A&(4)\\
\sigma_p(e_1\cup e_2)&\equiv \sigma_p(e_1)\cup\sigma_p(e_2)&(5)\\
\sigma_p(e_1\cap e_2)&\equiv \sigma_p(e_1)\cup\sigma_p(e_2)&(6)\\
\sigma_p(e_1\backslash e_2)&\equiv \sigma_p(e_1)\backslash \sigma_p(e_2)&(7)\\
\Pi_p(e_1\cup e_2)&\equiv \Pi_p(e_1)\cup \Pi_p(e_2)&(8)
\end{align}
$$

$$
\begin{align}
e_1\times e_2&\equiv e_2\times e_1&(9)\\
e_1\bowtie_p e_2&\equiv e_2\bowtie_p e_1&(10)\\
(e_1\times e_2)\times e_3&\equiv e_1\times(e_2\times e_3)&(11)\\
(e_1\bowtie_{p_1} e_2)\bowtie_{p_2} e_3&\equiv e_1\bowtie_{p_1}(e_2\bowtie_{p_2} e_3)&(12)\\
\sigma_p(e_1\times e_2)&\equiv e_1\bowtie_p e_2&(13)\\
\sigma_p(e_1\times e_2)&\equiv \sigma_p(e_1)\times e_2\ \mathrm{if}\ \mathcal{F}(p)\subseteq A(e_1)&(14)\\
\sigma_{p_1}(e_1\bowtie_{p_2} e_2)&\equiv \sigma_{p_1}(e_1)\bowtie_{p_2} e_2\ \mathrm{if}\ \mathcal{F}(p_1)\subseteq A(e_1)&(15)\\
\Pi_A(e_1\times e_2)&\equiv \Pi_{A_1}(e_1)\times \Pi_{A_2}(e_2)&(16)\\
&\ \mathrm{if}\ A=A_1\cup A_2,A_1\subseteq A(e_1),A_2\subseteq A(e_2)
\end{align}
$$



- $e$ 是关系表达式；$\backslash$ 是集合作差。

- $\mathcal F(p)$ 代表谓词 $p$ 涉及的所有字段。

- 规则 16 例子：

  - *e*1：一个学生表，包含属性 StudentID, Name, Age。
  - *e*2：一个课程表，包含属性 CourseID, CourseName, Instructor。

  ```sql
  SELECT Student.StudentID, Course.CourseID
  FROM Student
  CROSS JOIN Course;
  # 等价于
  SELECT StudentID, CourseID
  FROM (SELECT StudentID FROM Student) AS StudentSubset
  CROSS JOIN (SELECT CourseID FROM Course) AS CourseSubset;
  ```



##### 逻辑询问优化阶段

应用上面规则的：phases of logical query optimization

1. 打破与谓词为两个谓词 (1)->

   单谓词可以更容易移动

2. 选择语句下放 (2)-> (14)->

   尽早减少元组数目

3. 引入连接 (13)->

   join 比叉乘代价更低

4. 决定连接顺序 (9) (10) (11) (12)

   更小的放在 join 的左边

5. 投影下放 (3)<- (4)<- (16)->

   插入选择以排除无用属性

   在流水线阻断(pipeline breaker)之前，如磁盘IO,资源争夺

例子如下：

###### 1

![image-20231224213747244](img/image-20231224213747244.png)

![image-20231224213836446](img/image-20231224213836446.png)

###### 2

![image-20231224213954456](img/image-20231224213954456.png)

![image-20231224214028975](img/image-20231224214028975.png)

###### 3

![image-20231224214118651](img/image-20231224214118651.png)

![image-20231224214530161](img/image-20231224214530161.png)

![image-20231225120419913](img/image-20231225120419913.png)

###### 4

<img src="img/image-20231225120625076.png" alt="image-20231225120625076" style="zoom: 33%;" />

![image-20231225120726898](img/image-20231225120726898.png)



###### 5

<img src="img/image-20231225121032802.png" alt="image-20231225121032802" style="zoom: 50%;" />

![image-20231225121127777](img/image-20231225121127777.png)

##### 物理优化器

physical optimization

<img src="img/image-20231225134132991.png" alt="image-20231225134132991" style="zoom:50%;" />

join 顺序选择 DP

<img src="img/image-20231225134329695.png" alt="image-20231225134329695" style="zoom: 67%;" />

<img src="img/image-20231225142249721.png" alt="image-20231225142249721" style="zoom:67%;" />

Selinger 算法：

1. 生成单一关系的所有可行路径
   - 文件扫描或索引扫描 (file scan / index scan)
   - 对每个候选顺序，保持最小代价的 (keep the cheapest for each interesting order)
2. 考虑所有连接两个关系的方法
   - 使用上一步结果做外关系(outer relation)
   - 考虑所有其他关系作为内关系
   - 评估使用 sort-merge / nested-loop 的代价
   - 对每个候选顺序，保持最小代价的
3. 对三个关系、更多关系，重复上面的做法

> ##### 连接顺序选择



### 机器学习评估

#### 概述

回归问题：

数据库估计问题可以建模为回归问题，将高维输入变量转为目标特征(如花费，效用utility)。

- 基数/花费评估。可以用回归模型如 DL。
- 索引/视图优点评估。建立索引index/视图带来的改进。
- 查询延迟latency评估。询问的执行时间，基于询问和并发concurrency特征。

基于学习的基数评估。(learned cardinality estimation)

- 动机：由于传统方法基于值独立、均匀假设，对复杂数据分布表现不行。
- 挑战：数据列间的相关correlation。多表链接增加数据量和查询类型。
- 优化目标：准确率、推断延迟inference latency、模型大小、训练花费

自动基数/花费评估的动机：

- 数据库的最具挑战性问题。现代查询优化器的致命缺陷Achilles Heel
- 传统基数评估方法是：
  - 基于基本表或join采样
  - 基于核的方法(高斯模型采样)
  - 直方图(单列或多列)
- 传统费用模型：
  - 基于数据草图/直方图的方法
  - 基于采样的方法

#### 分类

监督询问方法、监督数据方法无监督数据方法。

![image-20231223213913668](img/image-20231223213913668.png)

监督询问方法：神经网络、XGBoost、多集合卷积网络

监督数据方法：核密度模型、统一混合mixture模型、预训练摘要summarization模型

无监督数据方法：SPN(sum product network), AR(autoregressive)模型、NF(normalizing flow)模型

![image-20231224162940895](img/image-20231224162940895.png)

take-aways:(要点)

基数评估：

- 数据驱动对单表最有效
- 询问驱动比数据驱动更有效率
- 数据驱动比询问驱动更健壮
- 多数数据驱动采样很重要

代价评估：需要更好的计划内嵌。

开放问题：

- 小模型/推断延迟下的高精度
- 自适应(adaptivity)

#### 监督询问方法基数评估

supervised query methods for cardinality estimation

回归问题：学习询问 Q 和真实基数的映射函数。

<img src="img/image-20231223214733533.png" alt="image-20231223214733533" style="zoom:50%;" />

##### 神经网络

询问驱动的单表的神经网络。

动机：传统评估方法基于列独立假设。

解决方案：

- 训练：表示一个询问为 $lb_1 < A_1 < ub_1,\cdots, lb_d < A_d < ub_d$，对表 T 的 d 个属性 $A_1,\cdots, A_d$，记作 $<lb_1,ub_1,\cdots, ld_d,ub_d>$。
- 构建两个隐藏层神经网络

使用训练好的模型推断回答一个询问。

<img src="img/image-20231223215258744.png" alt="image-20231223215258744" style="zoom:50%;" />

##### XGBoost

XGBoost

解决方案：

- 询问表征与神经网络相同
- 使用 XGBoost，一种基于决策树的组合模型(ensemble model)拟合

<img src="img/image-20231223215547106.png" alt="image-20231223215547106" style="zoom:50%;" />

与神经网络对比：

- 训练数据充分时，神经网络更好；否则 XGBoost 更好

##### 深度学习

动机：传统方法很难获取join跨表关联。

解决方案：

- 对输入的表集合，join集合，独热编码
- 对每个谓词 `(col,op,val)`，独热编码 col 和 op，标准化 val 取值为 `[0,1]`
- 用一些样本解决零行问题(0-tuple problem)

<img src="img/image-20231224142754080.png" alt="image-20231224142754080" style="zoom:50%;" />

训练：

- 输入的三部分在线性、激活、维度降低层后连在一起
- 之后再走一遍线性、激活层得到预测基数
- 通过反向传播梯度来修改模型参数

#### 监督数据方法基数评估

supervised data methods for cardinality estimation

密度评估问题：学习一个联合(joint)数据分布的各个数据点(除了预训练摘要模型)

![image-20231224143429355](img/image-20231224143429355.png)

##### 核密度模型

对单表格。

动机：多维直方图复杂构造和维护。

核心观点：拟合数据表概率密度分布通过核密度模型(kernel density model)
$$
\hat p_H(\vec x)=\dfrac1{s\cdot|H|}\sum_{i=1}^sK(H^{-1}[\vec t^{(i)}-\vec x])
$$
其中 s 是样本大小，K 是高斯函数，$\vec t$ 是样本点，H 是学习的参数

> [参考](https://blog.csdn.net/qq_33187050/article/details/125746878) 高斯函数 $ae^{-\cfrac{(x-b)^2}{2c^2}}$，其中 a 是峰值，b 是横坐标，c 是标准差

推断：$\hat p_H(\Omega)=\int_\Omega\hat p(\vec x)d\vec x$，询问空间是 $\Omega$。

![image-20231224144716440](img/image-20231224144716440.png)

KDE kernel-density estimation $p_i=\dfrac{n_i}{N\Delta_i}$

$p_i$ 是概率，$n_i$ 是点数，N 是总点数，$\Delta$ 是带宽(bandwidth)

<img src="img/image-20231224150343283.png" alt="image-20231224150343283" style="zoom:33%;" />

<img src="img/image-20231224150352028.png" alt="image-20231224150352028" style="zoom:33%;" />

解决方法：训练

- 获得大量有真实基数的询问
- 从表采样点(行)，初始化参数 H
- 通过随机梯度下降(预测基数和真实之差)来调整 H

推断：通过基于累积核密度的核密度方法

<img src="img/image-20231224150820116.png" alt="image-20231224150820116" style="zoom:50%;" />

> 核函数 `K(x,y)` 衡量两个输入的相似性。
>
> 高斯核，也称为径向基函数（Radial Basis Function, RBF）核 $K(x,y)=exp(-\dfrac{||x-y||^2}{2\sigma^2})$，$\sigma$ 对表现影响大

##### 统一混合模型

uniform mixture model 对单一表

动机：传统方法需要提前表扫描来获取数据。

核心观点：拟合数据表概率密度分布。

模型：
$$
f(x)=\sum_{z=1}^mh(z)g_z(x)=\sum_{z=1}^mw_zg_z(x)
$$
$w_z$ 是亚种群(subpopulation) z 的权，$g_z(x)=\dfrac1{|G_z|}$ 是均匀分布函数，分母是 z 的面积。

推断：
$$
\int_{B_i}f(x)ds=\int_{B_i}\sum_{z=1}^mw_zg_z(x)dx
$$
训练：

- 采样一些询问内的点和真实基数
- 生成这些点的子群(subgroup)
- 通过统一混合模型学习学习权 $w_z$

推断：

- 计算累积概率密度(即选择性)根据混合密度函数
- 覆盖询问矩形和数据矩形间的区域

![image-20231224152206655](img/image-20231224152206655.png)

##### 预训练摘要模型

pre-training summarization model 单表

动机：预训练模型避免逐个数据集训练

解决方案：

- 预训练编码解码器，使用大数据表梯度下降损失函数为 $|\log\dfrac{true.cad}{est.card}|$
- 使用预训练编码器编码一张表
- 输入询问和编码后的表到预训练解码器获取基数

![image-20231224152621571](img/image-20231224152621571.png)

![image-20231224152831688](img/image-20231224152831688.png)

#### 无监督数据方法基数评估

unsupervised data methods for cardinality estimation

回归模型：学习每个点的概率方程

<img src="img/image-20231224152936139.png" alt="image-20231224152936139" style="zoom: 33%;" />

##### SPN

sum-product network 多表

动机：多数评估器需要 SQL 查询。

基本观点：学习联合概率分布。

RSPN relational sum product network，三种节点：

- product nodt: 分裂表的列
- sum node: 分裂表的行
- leaf node: 表示单一(individual)变量的概率分布

<img src="img/image-20231224153146832.png" alt="image-20231224153146832" style="zoom:33%;" />

训练：

- 生成询问和基数
- 使用递归分组构建 RSPN
  - 行：k均值聚类 k-means clustering
  - 列：随机依赖系数 randomized dependency coefficient

推断：

- 自底向上评估基数
- 设多表询问通过连接采样(join sampling)，支持对子模式的询问，使用扇出缩放(fanout scaling)

![image-20231224153632413](img/image-20231224153632413.png)

扇出缩放：支持对全外连接表的子模式(sub-schema)查询 (包含冗余行)

解决：对每个 FK-PK 关系，增加一列表示一行有多少对应的 join 配偶。

如：

![image-20231224154714926](img/image-20231224154714926.png)

```sql
SELECT COUNT(*) FROM customer c WHERE c_region='EUROPE';
```

答案数 2，但有三个行被找到。所以纠正计算为：
$$
|C\bowtie O|\cdot\mathbb E(\dfrac1{F'_{C\leftarrow O}}\cdot\mathbf 1_{c_region='EU'}\cdot N_C)\\
=5\cdot\dfrac{\cfrac12+\cfrac12+1}5=2
$$
也就是说，从一个 join 表反推单一表的计数，因为存在一对多，所以同一个一通过除以扇出计数来去重。

##### 自动回归模型

autoregressive(AR) model 单表

动机：现存评估器对丰富多变量分布的获取困难。

训练：使用 AR 模型拟合联合多列概率分布。

推断：

- 使用蒙特卡洛采样，等值询问的基数估计
- 范围询问需要通过渐进(progressive)采样(通过学习概率分布)

![image-20231224155538141](img/image-20231224155538141.png)

不支持多表询问。

解决：

- 将多表用外连接拼在一起
- 多表询问通过采样，通过扇出缩放做子模式

<img src="img/image-20231224160046313.png" alt="image-20231224160046313" style="zoom:50%;" />

##### 归一化流模型

normalizing flow(NF) model 多表

动机：之前的数据驱动方法不能很好处理值域大的表格

<img src="img/image-20231224162040064.png" alt="image-20231224162040064" style="zoom:50%;" />

方案：

- 去量化(dequantize)、归一化离散值为连续值变量
- 使用 NF 模型学习数据点联合概率分布
- 累积连续 NF 分布函数，通过自适应重要采样(adaptive importance sampling)回答询问
- 使用扇出缩放支持多表询问

> 量化是将连续值或较大的值集转换为较小的、离散的值集的过程。去量化是逆过程。

![image-20231224162902791](img/image-20231224162902791.png)

#### 代价评估

问题公式化(formulation)：

- 代价(cost)：查询计划的执行代价
- 输入：SQL 询问计划；输出：估计代价。

<img src="img/image-20231224163715665.png" alt="image-20231224163715665" style="zoom:67%;" />

任务目标：近似模拟执行时间或资源耗费。

相关(correlations)：在基数评估的基础上。

难点：涉及 CPU 利用率，顺序扫描代价等因子需要考虑。

##### Tree-LSTM

动机：传统评估器没学习计划表征(representation)所以不准确

解决：

- 生成询问计划和真实代价作为训练集
- 独热编码询问计划
- 表征层学习询问计划的嵌入(embedding)，通过 Tree-LSTM
- 评估层基于表征层进行输出

![image-20231224170221531](img/image-20231224170221531.png)

模型构建：传统方法使用估计的 card，没有谓词编码不准确。

![image-20231224170521282](img/image-20231224170521282.png)

表征层：学习子询问内嵌(全局向量代表咨询问，局部向量代表根操作符)

评估层：同时输出基数和代价

> "Plan embedding" 的目标是将一个查询的执行计划嵌套到另一个查询的执行计划中，以便更好地理解和优化整个查询工作负载。这可以帮助数据库优化器更好地估计成本和执行查询的顺序，以提高查询性能。嵌套计划可以包括子查询、联接操作等。

#### 询问延迟评估

##### 深度学习

深度学习解决方案。

动机：

- 统计方法无法基于询问结构预测，产生大误差
- 与费用模型的区别：更复杂，因为依赖系统资源和费用

核心观点：DL 获取输入表，操作符，最终表现的关系。

将每个操作符 $q_i$ 表示成神经单元，如：

<img src="img/image-20231224171704237.png" alt="image-20231224171704237" style="zoom:67%;" />

链接神经单元通过查询结构，可以用树状结构

- 匹配询问结构来预测询问延迟。输出是扫描单元($N_\sigma,N_S$)，输入是 join 操作符 $N_\bowtie$，最后预测询问延迟。

<img src="img/image-20231224171958625.png" alt="image-20231224171958625" style="zoom:67%;" />

##### 图嵌入

并发询问评估：

- 数据共享 data-sharing
- 数据冲突 data-conflict
- 资源竞争 resource-competition
- 父子关系 parent-child relatioship

![image-20231224172229166](img/image-20231224172229166.png)

方法：

- 图建模：Workload2Graph 
- 图预测：GNN
- 图更新：在线(on-the-fly)更新

![image-20231224172424451](img/image-20231224172424451.png)

图建模：

- 并发询问的表现预测
- 图模型表示并发询问，内嵌图使用图卷积网络(GNN)，预测所有操作的询问使用简单稠密网络(simple dense network)

![image-20231224172602249](img/image-20231224172602249.png)

#### 优化评估

##### 深度学习

deep learning for index benefit estimation

挑战：

- 含义预测：多评估尺度(索引优化，空间耗费)；代价评估不准确
- 数据结构的相互影响(interaction)：多列访问，数据刷新，MV冲突

> MV：物化视图（Materialized View）是一个预先计算和存储在数据库中的查询结果集合，以提高查询性能和减少计算负载。
>
> 其他可能：(多版本, [Multi-Version Concurrency Control])

核心观点：将优化评估问题视作 ML 分类任务。

解决：

- 预训练数据：询问计划+不同索引下的费用
- 训练分类模型：输入有和无索引的两个询问计划；输出：布尔值表示是否有提升
- 解决索引选择问题：使用该模型采纳有提升的索引

![image-20231224192005271](img/image-20231224192005271.png)

##### 编码解码器

encoder-decoder for view benefit estimation

特征提取(extraction)：

- 曾经的研究成果是用两个等长的视图
- 编码不同数目和长度的询问/视图，注意力机制捕获相关性

模型构造：

- 很难联合地考虑冲突的 MVs
- 分解问题为：选择一个 MV，然后使用基于注意力的模型评估 MV 优化

<img src="img/image-20231224192413630.png" alt="image-20231224192413630" style="zoom: 50%;" />

##### 要点

- 学习的效用评估器比传统经验方法更准确
- 学习的效用评估器对多 MV 优化也更准确
- 基于询问的编码模型需要在数据更新时周期训练
- 开放问题：更大测试集的预测；初始化代价和更新



### 机器学习优化

learned-based query rewrite

#### 概述

##### 问题定义

![image-20231225160213683](img/image-20231225160213683.png)

动机：识别新的规则来获得表现提升

基本观点：从公共数据集获取相对简单的询问模式对，生成新的重写规则

挑战：如何生成新规则、如何验证规则等效性

![image-20231225161557593](img/image-20231225161557593.png)

![image-20231225161757739](img/image-20231225161757739.png)

解决方法：

- 通过枚举器(enumerator)生成新规则：(源模式，目标模式，限制)

- 通过 SMT 解决器(solver)验证规则等式

  - 只询问不超过 4 个运算符

  - 转换：SQL 询问 -> U 表达式 -> FOL 公式

  - Q(X), Q(Y) 等价当且仅当 

    $query_{FOL}(X)\to query_{FOL}(Y)\ and\ query_{FOL}(Y)\to query_{FOL}(X)$

- 使用验证过的规则贪心重写询问

<img src="img/image-20231225162014584.png" alt="image-20231225162014584" style="zoom:67%;" />

<img src="img/image-20231225162230400.png" alt="image-20231225162230400" style="zoom:67%;" />

<img src="img/image-20231225162243002.png" alt="image-20231225162243002" style="zoom:67%;" />

![image-20231225162253509](img/image-20231225162253509.png)

为何把启发式(heuristics)改成基于学习的：

- 许多真实世界的询问写的很糟

  - 糟糕的操作(子询问,join, union, union all)
  - 也许人看来很好，但物理上很低效 (如子查询做成临时表)

- 现存方法基于启发式规则

  - 自顶向上重写顺序也许不是最优的

    如上推一些子查询前移除一些聚合(aggregate)

  - 一些情况无法被现存规则覆盖

- 权衡 SQL 重写

  - 最好的表现：最佳重写顺序
  - 最小延迟：更小的重写时间

挑战：

- 新规则的等值验证
- 规定时限内搜索重写空间
- 多因素评估重写好处



##### 调整谓词规则

finetune predicate rules

动机：传统谓词下推在很多情况没这么强大

核心观点：生成新的谓词，有效(语义semantic等价)和最优(表现优化)

挑战：如何生成新谓词、如何验证有效和最优

解决：

- 构造经典模型(SVM) 分类模型(布尔值)-> 新谓词
- 使用真假样本调整模型：
  - 有效：模型筛掉了原本谓词有的东西就不有效 (true samples)
  - 最优：模型搞出了多的冗余样本就不最优 (false samples)

![image-20231225163755815](img/image-20231225163755815.png)

![image-20231225164211960](img/image-20231225164211960.png)

![image-20231225164229384](img/image-20231225164229384.png)

##### 自动询问重写

automatic query rewrite

问题定义：给定一个慢查询 Q 和重写规则集 R，应用规则获取等价查询得到最小花费

![image-20231225170407098](img/image-20231225170407098.png)

##### 自适应应用规则

adatively apply rewrite rules

动机：慢查询可能有多个重写序列(不同的优点)

核心观点：用树搜索算法探索最优重写序列

挑战：如何表征重写序列；如何高效找到最优重写序列

方案：

- 对新的询问初始化一个策略树(policy tree)

  节点 $v_i$ 是重写的询问；$C^{\uparrow}(v_i)$ 是之前的代价降低；$C^{\downarrow}(v_i)$ 是之后的

- MCTS 搜索树，计算节点值(节点选择)

  $u(v_i)=C^{\uparrow}(v_i)+C^{\downarrow}(v_i)+\gamma\sqrt{\dfrac{\ln\mathcal F(v_0)}{\mathcal F(v_i)}}$

![image-20231225171138091](img/image-20231225171138091.png)

##### 总结

![image-20231225205604556](img/image-20231225205604556.png)

要点：

- 传统询问重写不考虑代价，造成冗余或负面重写
- 对复杂询问，基于搜索的重写比传统更好
- 重写优化评估器提高基于重写的简单搜索表现

开放问题：

- 更多降低提前估算代价
- 适应不同的规则/数据集
- 设计新的重写规则

#### 连接顺序选择

##### 概述

join order selection

动机：

- 计划代价很难评估 (计划空间大)

- 传统优化器的限制

  - DP 优化性能良好，但延迟太高
  - 随机选择的优化能力低

- 让(steer)现存的优化器获得更高的性能

  暗示(hint)连接顺序、操作类型

问题定义：给定 SQL 询问，选择代价最小的连接顺序(根据花费模型)

<img src="img/image-20231225210734700.png" alt="image-20231225210734700" style="zoom:50%;" />

方法分类：

- 离线优化方法

  特征：给定测试(workload), 基于 RL

  关键想法：使用现存的测试训练学习优化器，预测未来的询问

- 在线优化方法

  无workload，依赖自定义的数据库

  关键想法：询问计划可以在执行时改变；询问可以转换为更好的计划；数据库执行询问时进行学习

核心区别：基于学习的才能容易更新，不局限于训练数据

![image-20231225212313481](img/image-20231225212313481.png)

基于学习的代价模型：当代价评估不准确时，从延迟学习 (learned from latency when cost estimation is inaccurate)

基于学习的计划枚举(enumeration)：

- 不止评估完整计划的执行时间，还评估好计划的生成方向
- 引导计划生成的方向，降低生成数目

<img src="img/image-20231225211617360.png" alt="image-20231225211617360" style="zoom:50%;" />

![image-20231225211632778](img/image-20231225211632778.png)

挑战：

- 需要准确预测执行时间
- 计划生成延迟足够低

优化目标：

- 质量：延迟
- 适应性：对不同数据库实例、工作载荷(workload)
- 更新：join 图，模式，数据
- 训练代价

##### rejoin&DQ

动机：

- join 顺序搜索空间大
- 传统优化器不学习之前的好/坏选择

挑战：，，

- 降低搜索空间
- 选择最佳 join 顺序

差异：

- Rejoin 使用基于策略的方法(PPO policy based mathod)指导计划搜索
- DQ 使用基于值的方法(DQN value based)，使用 DP 计划预训练值神经网络

![image-20231225212924202](img/image-20231225212924202.png)

映射为 RL 模型：

- 中介(agent)：优化器

- 动作(action)：join

- 环境：代价模型，数据库

- 奖励(reward)：代价，延迟

- 状态：join 顺序

- 长期奖励：

  - 基于 policy 的：输出所有 join 的可能性

    神经网络：三层 MLP

  - 基于值的：预测每个状态的最好终止状态的可能的花费

    神经网络：两层 MLP

<img src="img/image-20231225213942807.png" alt="image-20231225213942807" style="zoom:50%;" />

![image-20231225214009784](img/image-20231225214009784.png)

![image-20231225214108365](img/image-20231225214108365.png)

![image-20231225214136224](img/image-20231225214136224.png)



##### RTOS

动机：

- 先前的基于学习的优化器给出了好的花费，但是没有好的测试询问延迟
- 真实世界数据库模式经常改变

挑战：

- 中间值是森林，难以用简单特征向量表示
- 训练时间很大，收集延迟作为反馈
- 模式改变需要重新训练

<img src="img/image-20231225214545690.png" alt="image-20231225214545690" style="zoom:50%;" />

![image-20231225214600051](img/image-20231225214600051.png)

基于 treeLSTM 的神经网络

- 用 n 进制(n-ary)表示子树
- 用子树和表示森林

两步训练：代价预训练、延迟微调(fine-tuning)

动态神经网络：

- DFS 构建神经网络
- 多别名：参数共享
- 模式改变：局部微调

![image-20231225230750600](img/image-20231225230750600.png)

特征提取(extraction)：

- 获取结构信息对 join 顺序选择执行计划至关重要
- 编码操作符关系和询问特征元数据
- 使用 tree LSTM 嵌入询问特征
- 使用 RL 模型决定 join 顺序

##### NEO

动机：

- 之前传统优化器以来代价模型
- 之前方法解决 join 顺序不支持物理操作符选择

挑战：

- 如何构建学习代价模型自动获取直觉模式(在树结构询问计划里)并预测延迟
- 如何自动表征询问谓词语义(semantics)(支持字符串 word2vector)
- 如何克服增强学习样本不高效(inefficiency) (使用优化器指导)



- 使用树CNN设计值网络表征询问计划(join 顺序，操作符)
- 使用行向量表征谓词，每行是一个句子
- 从示范专家优化器学习
- 归一化计划花费通过优化器计划代价

![image-20231226141917751](img/image-20231226141917751.png)

![image-20231226142258890](img/image-20231226142258890.png)

##### Bao

动机：

- 长训练时间
- 不能根据数据和工作负荷改变调整
- 最坏计划的尾延迟
- 物理选择器的选择影响计划的质量

挑战：

- 如何枚举计划
- 如何学习计划延时和选择高质量计划



使用操作符暗示(hint)来生成候选计划：是否允许哈希 join 等

使用树 CNN 预测延迟和指导计划选择

- 延迟预测：编码计划为向量化树(vectorized tree)

- 上下文多臂老虎机(contextual multi-armed bandits)

  > 强化学习。“多臂”指的是多个选择或行动的机会，“老虎机”则暗示了每个选择带来的不确定回报。经典的多臂老虎机问题中，目标是找到一个策略，通过多次尝试来最大化总回报，同时面对每个选择带来的潜在回报的不确定性。然而，这个模型并不考虑任何外部信息或上下文。
  >
  > 强化学习不是从标记数据中学习，而是通过与环境的互动来学习如何完成任务或达成目标。
  >
  > Thompson Sampling（汤普森采样），也被称作后验采样，是一种用于解决多臂老虎机（Multi-Armed Bandit, MAB）问题的概率性算法。在强化学习和在线决策问题中，它是一种有效的探索与利用（Exploration-Exploitation）平衡策略。

  每个暗示设置成一个臂，使用汤姆森(Thompson)采样更新模型参数

\

![image-20231226143939543](img/image-20231226143939543.png)

使用小改动(minor hanges)提高询问优化，如是否激活 loop join 对不同的询问

模型计划暗示器(hinter)视为一个多臂老虎机问题：

- 将每个暗示集合 $HSet_i:Q\to T$ 作为一个询问优化器

- 对一个询问 q，目的是选择正确暗示集合生成最优计划，视为反悔(regret)最小化问题：
  $$
  R_q=(P(B(q)(q))-\min_i P(HSet_i(q)))^2
  $$
  <img src="img/image-20231226161208216.png" alt="image-20231226161208216" style="zoom:50%;" />

<img src="img/image-20231226161236373.png" alt="image-20231226161236373" style="zoom:67%;" />

##### SkinnerDB

动机：

- 之前的成果以来从代价模型或专家优化器学习
- 之前基于学习的优化器需要给定训练询问，难以对不同的工作载荷给出好的计划
- 执行器可以在询问执行时检测评估错误

挑战：

- 设计新的工作机制允许优化器在线学习和选择不同的 join 顺序
- 在线评估和选择不同的 join 顺序



漩涡型(Eddies-style)

- 将执行过程划分为数个时间切片
- N 种方法支持计划选择
- 基于之前的时间切片为下一个时间切片选择计划

MCTS for JOS：在每个时间切片学习和生成计划

> MTCS Monte Carlo tree search 蒙特卡洛树搜索

依赖自定义数据库：选择低延迟计划

支持在线重排序使用 MCTS，不需要预训练，时间切片是 1ms(通过运行时学习)

![image-20231226161758105](img/image-20231226161758105.png)

在线(on the fly)元组更新执行顺序，更新计划，维持执行状态

- 元组流进入旋涡(Eddy)，通过输入关系如 R,S,T 
- 旋涡转发(route)元组到对应操作符(自适应选择顺序，通过操作符代价)
- 旋涡发送元组到输出当元组已经被所有操作符处理过

<img src="img/image-20231226162510962.png" alt="image-20231226162510962" style="zoom: 50%;" />

##### 总结

Bao 和 Skinner-DB 是在线，其他是离线

![image-20231226162808239](img/image-20231226162808239.png)

不容易应用到真实的 DBMS

开放问题：

- 低延迟计划生成

  神经网络的延迟无法忽略；难以应用到低延迟 OLTP 服务

- 支持复杂询问：嵌套查询

- 学习度量(metrics)

  - 计划延迟根据系统状态和网络延迟不同
  - 更快的计划需要更多的资源，如双核 CPU

> OLTP（在线事务处理）



#### 自治数据库系统

##### 概述

autonomous database systems

动机：

- 传统数据库设计耗力(laborious)
  - 基于工作负荷/数据特征设计数据库
  - 通用模组不对所有情况表现良好
- 大部分 AI4DB 集中于单一模组工作：局部最优需要事先大量训练
- 商业实践 AI4DB 成果
  - 重 ML 模型难以内置到核里
  - 需要统一训练平台

##### Peloton

通过工作负荷预测(forecasting)安排最优执行

- 内嵌监视器：检测事件流
- 工作负荷预测模型：未来工作负荷类型
- 优化动作：调整，计划

![image-20231226164707136](img/image-20231226164707136.png)

##### SageDB

通过数据分布自定义 DB 设计

- 通过基于学习的 CDF 学习数据分布

  $M_{CDF}=F_{X_1,\cdots, X_m}(x_1,\cdots,x_m)=P(X_1\le x_1,\cdots, X_m\le x_m)$

- 设计组件基于学习的 CDF

  - 查询优化和执行
  - 数据布局设计
  - 进一步的分析

<img src="img/image-20231226165032892.png" alt="image-20231226165032892" style="zoom:67%;" />



##### openGauss

实现，验证，管理基于学习的模块

基于学习的优化器：

- 询问重写器、代价/基数评估器、计划生成器

基于学习的建议器(advisor)：

- 自监督、自诊断、自配置、自优化

模型验证

数据/模型管理

![image-20231226170307812](img/image-20231226170307812.png)



##### 开放问题

对 ML4DB 的开放问题

未来的 SLA 提升工作：

- 在噪声环境下优化数据库：训练数据清晰，模型健壮
- 优化极端复杂的询问(如嵌套查询)：自适应基数评估，高效询问计划
- 优化 OLTP 询问：多询问优化

> SLA 是“服务水平协议”（Service Level Agreement）。通常在服务提供商（如数据库托管服务、云服务供应商）和其客户之间建立。SLA 的目的是定义双方对服务质量、可用性、责任和其他服务参数的预期和要求

对事先(overhead)优化的工作：

- 冷启动问题：夸数据集/实例/硬件/数据库类型
- 轻量内核(in-kernel)组件：高效 ML 模型；稀有数据/计算依赖
- 在线优化
- 提前工作负荷执行
- 提前模型训练

未来工作：自适应

- 明显数据改变：从小数据集迁移到大数据集
- 全新实例：新数据集、工作负荷、SLA 需求
- 增量 DB 模组更新：学习的knob tuner对硬件更新，动态工作负荷的学习优化器

> 用于调整和优化数据库配置设置（即“knobs”）。这些设置是控制数据库性能和行为的参数。Knob tuner 的目的是自动或半自动地找到最佳的配置设置，以提高数据库的效率和性能。

未来工作：复杂情景

- 混合(hybrid)工作负荷：HTAP，动态流任务
- 分布式数据库：分布计划优化
- 云数据库：动态环境，无服务(serverless)优化

> HTAP（Hybrid Transactional/Analytical Processing）是一种数据库架构，它允许在同一个数据库系统中同时进行事务处理（OLTP，Online Transaction Processing）和分析处理（OLAP，Online Analytical Processing）。这种架构的主要优势在于它消除了传统上将事务处理系统和分析处理系统分开维护的需要，提供了更高效、实时的数据处理和分析能力。

未来工作：小训练数据

- 更少的训练样本：few-shot learning
- 知识+数据驱动。从数据里总结(可翻译的interpretable)经验
- 预训练模型：一个模型多个场景

未来工作：验证基于学习的模型

- 是否高效
- 是否比现有的更好
- 是否适应新场景

未来工作：一个模型适应不同场景

- 高适应性
  - 工作负荷：询问操作符；计划结构；潜在数据获取(access)
  - 数据集：表，数据分布，索引/视图，数据更新
  - 数据库实例：状态度量(DB，资源利用)，硬件配置
  - DBMS：MySQL, PostgreSQL, MongoDB, Spark
- 可能的方法：公共知识获取，元学习

未来工作：自动学习模型选择

- 自动数据库组装
  - 自动选择 ML 模型/算法对不同的任务
  - 评估整体表现

![image-20231226173655263](img/image-20231226173655263.png)

未来工作：统一数据库优化

- 安排多个数据库优化任务
  - 多个要求：优化器生成好的计划不需要很准的评估器；创建索引导致最优节点改变
  - 混合安排：不同优化器任务基于数据库配置和工作负荷特征
  - 提前优化：最大化优化，不与用户进程竞争资源
- 挑战：不同任务特征，任务关联，改变趋势

# 理论

## MySQL

### 架构组成

#### 概述

[小林coding](https://www.xiaolincoding.com/mysql/base/how_select.html#mysql-%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84)

![查询语句执行流程](img/mysql查询流程.png)

架构分两层：

1. Server 层：建立连接、分析和执行 SQL。

   大多数的核心功能模块都在这实现，主要包括连接器，查询缓存、解析器、预处理器、优化器、执行器等。另外，所有的内置函数（如日期、时间、数学和加密函数等）和所有跨存储引擎的功能（如存储过程、触发器、视图等）。

2. 存储引擎层：数据的存储和提取。

   InnoDB、MyISAM、Memory 等多个存储引擎，不同的存储引擎共用一个 Server 层。

   MySQL 5.5 版本开始， InnoDB 成为了 MySQL 的默认存储引擎。（之前是 MyISAM）

   该层实现索引，如 InnoDB 默认使用 B+ 树。（主键索引和二级索引(非主键索引)）

   > [区别在于在树里按主键存数据]

按照步骤：

1. 连接器
2. 查询缓存
3. 解析 SQL (解析器)
4. 执行 SQL (预处理器、优化器、执行器)

#### Server层

##### 连接器

MySQL 基于 TCP。

服务启动且密码正确连接成功。此时，连接器获取用户权限，之后的任何操作基于连接开始时的权限。

因此，若用户建立了连接，即使中途权限被修改，也不会影响已存在的连接的权限。

查看连接数：`show processlist`

`wait_timeout` 参数控制最大空闲时长，默认 28800s (8h)，超时自动断开。(查询：`show variables like 'wait_timeout'`)

也可以用命令手动断开连接。

处于空闲状态的连接被服务端主动断开后，这个客户端并不会马上知道，等到客户端在发起下一个请求的时候，才会收到报错。

超过最大连接数就拒绝接下来的请求。

分为短连接(三次握手，执行 SQL，四次挥手)和长连接(三次握手，执行多次 SQL，四次挥手)。

一般推荐长连接，但占用内存增多，因为 MySQL 在执行查询过程中临时使用内存管理连接对象，这些连接对象资源只有在连接断开时才会释放。可能会导致 MLE 异常重启。

可以定期断开长连接，或客户端主动使用函数接口重置连接，如 `mysql_reset_connection()`，而不需要重连重验证权限。

##### 查询缓存

如果 SQL 为查询语句(即如解析出第一个词是 select)，查找缓存，缓存以 key(SQL语句), value(查询结果)保存。

表被更新就会清空对应缓存。MySQL 8.0 版本直接将查询缓存删掉了。MySQL 8 之前，通过参数 `query_cache_type` 设置为 `DEMAND` 来关闭。

这里说的查询缓存是 server 层的，也就是 MySQL 8.0 版本移除的是 server 层的查询缓存，并不是 Innodb 存储引擎中的 buffer pool。

##### 解析器

解析器：词法分析、语法分析。

词法分析：把 token 分出关键字、非关键字。

语法分析：无语法问题则构建出 SQL 语法树。

解析器只负责检查语法和构建语法树，但是不会去查表或者字段存不存在。

执行 select 的三阶段：预处理、优化执行(prepare, optimize, execute)。

##### 预处理器

负责工作：查表或者字段存不存在、将 `select *` 的 `*` 替换为所有列。

> MySQL 5.7 判断表或字段是否存在的工作，是在词法分析&语法分析之后，prepare 阶段之前做的。
>
> 因为 MySQL 5.7 代码结构不好，所以 MySQL 8.0 代码结构变化很大。

##### 优化器

为 SQL 查询语句先制定一个执行计划。

比如在表里面有多个索引的时候，优化器会基于查询成本的考虑，来决定选择使用哪个索引。

使用 `explain` 语句查看执行计划。

> 若查询语句覆盖索引(即只需要返回索引值)，就不需要到逐渐索引查找，则这种情况二级索引可能更优。

##### 执行器

三种执行方式：主键索引查询、全表扫描、索引下推。

###### 主键索引查询

例如：

```sql
select * from product where id = 1;
```

> 这条查询语句的查询条件用到了主键索引，而且是等值查询，同时主键 id 是唯一，不会有 id 相同的记录，所以优化器决定选用访问类型为 const 进行查询，也就是使用主键索引查询一条记录，那么执行器与存储引擎的执行流程是这样的：
>
> - 执行器第一次查询，会调用 read_first_record 函数指针指向的函数，因为优化器选择的访问类型为 const，这个函数指针被指向为 InnoDB 引擎索引查询的接口，把条件 `id = 1` 交给存储引擎，让存储引擎定位符合条件的第一条记录。
> - 存储引擎通过主键索引的 B+ 树结构定位到 id = 1的第一条记录，如果记录是不存在的，就会向执行器上报记录找不到的错误，然后查询结束。如果记录是存在的，就会将记录返回给执行器；
> - 执行器从存储引擎读到记录后，接着判断记录是否符合查询条件，如果符合则发送给客户端，如果不符合则跳过该记录。
> - 执行器查询的过程是一个 while 循环，所以还会再查一次，但是这次因为不是第一次查询了，所以会调用 read_record 函数指针指向的函数，因为优化器选择的访问类型为 const，这个函数指针被指向为一个永远返回 - 1 的函数，所以当调用该函数的时候，执行器就退出循环，也就是结束查询了。
>
> 至此，这个语句就执行完成了。

###### 全表扫描

```mysql
select * from product where name = 'iphone';
```

> 这条查询语句的查询条件没有用到索引，所以优化器决定选用访问类型为 ALL 进行查询，也就是全表扫描的方式查询，那么这时执行器与存储引擎的执行流程是这样的：
>
> - 执行器第一次查询，会调用 read_first_record 函数指针指向的函数，因为优化器选择的访问类型为 all，这个函数指针被指向为 InnoDB 引擎全扫描的接口，让存储引擎读取表中的第一条记录；
> - 执行器会判断读到的这条记录的 name 是不是 iphone，如果不是则跳过；如果是则将记录发给客户的（Server 层每从存储引擎读到一条记录就会发送给客户端，之所以客户端显示的时候是直接显示所有记录的，是因为客户端是等查询语句查询完成后，才会显示出所有的记录）。
> - 执行器查询的过程是一个 while 循环，所以还会再查一次，会调用 read_record 函数指针指向的函数，因为优化器选择的访问类型为 all，read_record 函数指针指向的还是 InnoDB 引擎全扫描的接口，所以接着向存储引擎层要求继续读刚才那条记录的下一条记录，存储引擎把下一条记录取出后就将其返回给执行器（Server层），执行器继续判断条件，不符合查询条件即跳过该记录，否则发送到客户端；
> - 一直重复上述过程，直到存储引擎把表中的所有记录读完，然后向执行器（Server层） 返回了读取完毕的信息；
> - 执行器收到存储引擎报告的查询完毕的信息，退出循环，停止查询。
>
> 至此，这个语句就执行完成了。

###### 索引下推

索引下推 index condition pushdown [这也有](https://www.xiaolincoding.com/mysql/index/index_interview.html#%E6%8C%89%E5%AD%97%E6%AE%B5%E4%B8%AA%E6%95%B0%E5%88%86%E7%B1%BB)

MySQL 5.6 推出的查询优化策略。索引下推能够减少二级索引在查询时的回表操作，提高查询的效率，因为它将 Server 层部分负责的事情，交给存储引擎层去处理了。

假设有 `age, reward` 字段的联合索引：

```sql
select * from t_user  where age > 20 and reward = 100000;
```

联合索引当遇到范围查询 (>、<) 就会停止匹配，也就是 age 字段能用到联合索引，但是 reward 字段则无法利用到索引。([参见](https://xiaolincoding.com/mysql/index/index_interview.html#%E6%8C%89%E5%AD%97%E6%AE%B5%E4%B8%AA%E6%95%B0%E5%88%86%E7%B1%BB))

如果不下推：(5.6 之前)

> - Server 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age > 20 的第一条记录；
> - 存储引擎根据二级索引的 B+ 树快速定位到这条记录后，获取主键值，然后进行回表操作，将完整的记录返回给 Server 层；
> - Server 层在判断该记录的 reward 是否等于 100000，如果成立则将其发送给客户端；否则跳过该记录；
> - 接着，继续向存储引擎索要下一条记录，存储引擎在二级索引定位到记录后，获取主键值，然后回表操作，将完整的记录返回给 Server 层；
> - 如此往复，直到存储引擎把表中的所有记录读完。
>
> 每查询到一条二级索引，都要回表一次。

下推：

> - Server 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age > 20 的第一条记录；
> - 存储引擎定位到二级索引后，先不执行回表操作，而是先判断一下该索引中包含的列（reward列）的条件（reward 是否等于 100000）是否成立。如果条件不成立，则直接跳过该二级索引。如果成立，则执行回表操作，将完成记录返回给 Server 层。
> - Server 层在判断其他的查询条件（本次查询没有其他条件）是否成立，如果成立则将其发送给客户端；否则跳过该记录，然后向存储引擎索要下一条记录。
> - 如此往复，直到存储引擎把表中的所有记录读完。
>
> 因为加了一个 `=` 判断才回表，所以节省了回表操作。

若使用下推，则执行计划里的 Extr 部分显示 “Using index condition”。

#### 存储层

[小林coding](https://www.xiaolincoding.com/mysql/base/row_format.html#%E8%A1%A8%E7%A9%BA%E9%97%B4%E6%96%87%E4%BB%B6%E7%9A%84%E7%BB%93%E6%9E%84%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%E7%9A%84)

##### 存储文件

以 `InnoDB` 存储引擎为例。

查看数据库文件存放的目录：`SHOW VARIABLES LIKE 'datadir';`

该目录下每一个数据库都会建立一个以数据库名字为名的目录，将表结构与数据存放其中。文件：

- `db.opt` 当前数据库的默认字符集和字符校验规则。

- `数据表名.frm` 表结构定义，元数据信息 (form)(poe)

- `数据表名.ibd` 表数据。(InnoDB Data File)(poe)

  存在共享表空间文件(`ibdata1`)或独占(`数据表名.ibd`)，由 `innodb_file_per_table` 参数控制，若为 1，将存储的数据、索引等信息单独存储在一个独占表空间。MySQL 5.6.6 开始默认为 1。

##### 表空间结构

表空间由段（segment）、区（extent）、页（page）、行（row）组成。

![img](img/表空间结构.drawio.png)

###### 行

记录都是按行（row）进行存放的，每行记录根据不同的行格式，有不同的存储结构。

###### 页

数据库的读取并不以行为单位，否则一次读取（也就是一次 I/O 操作）只能处理一行数据，效率会非常低。InnoDB 按页读写数据。

默认每个页的大小为 16KB，也就是最多能保证 16KB 的连续存储空间。一次最少从磁盘中读取 16K 的内容到内存中，一次最少把内存中的 16K 内容刷新到磁盘中。

页的类型：数据页、undo 日志页、溢出页等。[参考](https://xiaolincoding.com/mysql/index/page.html)

###### 区

> B+ 树中每一层都是通过双向链表连接起来的，如果是以页为单位来分配存储空间，那么链表中相邻的两个页之间的物理位置并不是连续的，可能离得非常远，那么磁盘查询时就会有大量的随机I/O，随机 I/O 是非常慢的。

在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是按照区（extent）为单位分配。每个区的大小为 1MB，对于 16KB 的页来说，连续的 64 个页会被划为一个区，这样就使得链表中相邻的页的物理位置也相邻，就能使用顺序 I/O 了。

###### 段

表空间是由各个段（segment）组成的，段是由多个区（extent）组成的。段一般分为数据段、索引段和回滚段等。

- 索引段：存放 B + 树的非叶子节点的区的集合；
- 数据段：存放 B + 树的叶子节点的区的集合；
- 回滚段：存放的是回滚数据的区的集合 [参考](https://xiaolincoding.com/mysql/transaction/mvcc.html)(如MVCC 利用了回滚段实现了多版本查询数据)

##### 行格式

###### 格式分类

InnoDB 提供了 4 种行格式，分别是 Redundant、Compact、Dynamic和 Compressed 行格式。

- Redundant 古老，5.0 以前，不紧凑
- Compact 5.0 后引入，5.1 后默认，紧凑让一个数据页中可以存放更多的行记录
- Dynamic / Compressed，紧凑与 Compact 差不多，改进了一点，5.7 后默认 Dynamic

###### Compact 行格式

![img](img/COMPACT.drawio.png)

记录=记录的额外信息+记录的真实数据。

下面讲的都是 Compact 行格式。

> 建立一个 compact 表的例子：
>
> ```mysql
> CREATE TABLE `t_user` (
>   `id` int(11) NOT NULL,
>   `name` VARCHAR(20) DEFAULT NULL,
>   `phone` VARCHAR(20) DEFAULT NULL,
>   `age` int(11) DEFAULT NULL,
>   PRIMARY KEY (`id`) USING BTREE
> ) ENGINE = InnoDB DEFAULT CHARACTER SET = ascii ROW_FORMAT = COMPACT;
> ```



###### 记录额外信息

1. 变长字段长度列表。如 varchar, textt, blob

   占用字节数按变长列逆序排序，即先最后一列的长度，一直往前。

   如果变长段数值为 NULL，不会存储长度。NULL 值列表也需要逆序存放。

   逆序的理由：记录头信息指向下一个记录的指针，指向记录头信息和真实数据之间的位置，往左就是记录头，往右就是真实数据。这样可以使得位置靠前的记录的真实数据和数据对应的字段长度信息可以同时在一个 CPU Cache Line 中，这样就可以提高 CPU Cache 的命中率。[CPU cache](https://xiaolincoding.com/os/1_hardware/how_to_make_cpu_run_faster.html)

   如无变长字段，就没有这个列表。

2. NULL 值列表

   每个列对应一个二进制位（bit），二进制位按照列顺序逆序排列。

   位值为 1 代表 NULL，否则不为 NULL。

   NULL 值列表必须用整数个字节的位表示（1字节8位），如果使用的二进制位个数不足整数个字节，则在字节的高位补 0。如有三个列，第一个是 NULL，存储为 `00000001`。(即 `0x01`)

   当数据表的字段都定义成 NOT NULL 的时候，这时候表里的行格式就不会有 NULL 值列表了。所以在设计数据库表的时候，通常都是建议将字段设置为 NOT NULL。

3. 记录头信息

   包含的内容很多，包括但不限于：

   1. `delete_mask` 标识此条数据是否被删除。(即删除数据只标记 1)
   2. `next_record` 下一条记录的位置(指针，指向下一条记录头信息和真实数据之间的位置)
   3. `record_type` 当前记录类型：
      - 0 普通记录
      - 1 B+树非叶子节点记录
      - 2 最小记录
      - 3 最大记录

###### 记录的真实数据

1. `row_id`

   指定了主键或者唯一约束列，那么就没有 row_id 隐藏字段了。如果既没有指定主键，又没有唯一约束，那么 InnoDB 就会为记录添加 row_id 隐藏字段。row_id不是必需的，占用 6 个字节。

2. `trx_id`

   事务id，表示这个数据是由哪个事务生成的。 trx_id是必需的，占用 6 个字节。

3. `roll_pointer`

   记录上一个版本的指针。roll_pointer 是必需的，占用 7 个字节。 [MVCC机制](https://xiaolincoding.com/mysql/transaction/mvcc.html)

4. 定义的字段

###### varchar

MySQL 规定除了 TEXT、BLOBs 这种大对象类型之外，其他所有的列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过 65535 个字节。（一行记录除了 TEXT、BLOBs 类型的列，限制最大为 65535 字节）

varchar(n) 字段类型的 n 代表的是最多存储的字符数量，并不是字节大小。

需要考虑 storage overhead：变长字段长度列表、NULL 值列表的字节数。

如果只有一个 varchar，边长字段长度，若本字段最大字节数小于等于 255，就是 1 字节，否则就是 2 字节。

如：(65535-2-1),其中 -1 是 NULL。

```mysql
CREATE TABLE test ( 
`name` VARCHAR(65532)  NULL
) ENGINE = InnoDB DEFAULT CHARACTER SET = ascii ROW_FORMAT = COMPACT;
```

若 ASCII 就是 1 个字节，UTF8 就是 3 个字节，即 65532/3=21844。

多个字段就加起来算。

###### 行溢出

页是 16KB=16384 字节，而 varchar 可以 65532，TEXT/BLOB 更多，一页存不了一个记录，就会发生行溢出，多出的数据存到溢出页。

当发生行溢出时，在记录的真实数据处只会保存该列的一部分数据，而把剩余的数据放在溢出页中，然后真实数据处用 20 字节存储指向溢出页的地址，从而可以找到剩余数据所在的页。

Compressed 和 Dynamic 这两个行格式和 Compact 非常类似，主要的区别在于处理行溢出数据时有些区别。这两种格式采用完全的行溢出方式，记录的真实数据处不会存储该列的一部分数据，只存储 20 个字节的指针来指向溢出页。而实际的数据都存储在溢出页中。

### 索引

#### 基本概念

##### 定义

帮助存储引擎快速获取数据的一种数据结构。形象：数据的目录。

分类：

- 按数据结构：B+tree 索引、Hash 索引、Full-text 索引
- 按物理存储：聚簇索引（主键索引）、二级索引（非聚簇索引，辅助索引，Secondary Index）
- 按字段特性：主键索引、唯一索引、普通索引、前缀索引
- 按字段个数：单列索引、联合索引

##### 数据结构分类

引擎对这些索引的支持情况：

![img](img/索引分类.drawio.png)

`Memory` 不支持 full-text, 其他两个引擎不支持 hash。InnoDB 在 MySQL5.6 后支持 full-text。InnoDB 内存结构有自适应 hash 索引。

InnoDB 存储引擎会根据不同的场景选择不同的列作为索引：

- 如果有主键，默认会使用主键作为聚簇索引的索引键（key）
- 如果没有主键，就选择第一个不包含 NULL 值的唯一列作为聚簇索引的索引键（key）
- 在上面两个都没有的情况下，InnoDB 将自动生成一个隐式自增 id 列作为聚簇索引的索引键（key）

创建的主键索引和二级索引默认使用的是 B+Tree 索引。

> B+ 树非叶子只存索引，叶子存数据和双向链表指针，如：(最下面叶子层间箭头改成是双向的)

> ![主键索引 B+Tree](img/btree.drawio.png)

> 如 `select * from product where id= 5;`，每次读取节点是一个磁盘 I/O，则需要三次 I/O。B+Tree 对千万级数据也只需要 3-4 层高，这是其优势。

[B+ 树的优势](https://www.xiaolincoding.com/mysql/index/index_interview.html#%E6%8C%89%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%88%86%E7%B1%BB)

B+ 树优势：

- B 树①非叶子也存数据；B+树与之相比单个节点数据量更小，相同 I/O 能查询更多节点。②双向链表支持基于范围的顺序查找。
- 二叉树：搜索复杂度为 `O(log N)`，而 B+ 树为 `O(log_d N)`。其中 d 是最大子节点数，N 是叶子数[存疑]。
- Hash：无法做范围查询，只能做等值查询。

##### 物理存储

主键索引的叶子是实际数据，二级索引的叶子是主键值。获取主键再查一次叶子节点获取一行数据叫回表。

二级索引树是按照二级索引值按顺序存放的，在相同的二级索引值情况下， 再按主键 id 的顺序存放。[参考](https://www.xiaolincoding.com/mysql/lock/how_to_lock.html#%E9%9D%9E%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95%E7%AD%89%E5%80%BC%E6%9F%A5%E8%AF%A2)

覆盖索引：在二级索引的B+树上就能查到，而无需回表。

InnoDB B+ 树索引保存数据本身，MyISAM 存储物理地址。[参考](https://www.xiaolincoding.com/mysql/index/index_lose.html#%E7%B4%A2%E5%BC%95%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84%E9%95%BF%E4%BB%80%E4%B9%88%E6%A0%B7)

##### 字段特性

###### 主键索引

一张表最多只有一个主键索引，索引列的值不允许有空值。

###### 唯一索引

UNIQUE。一张表可以有多个唯一索引，索引列的值必须唯一，但是允许有空值。

###### 普通索引

既不要求字段为主键，也不要求字段为 UNIQUE。

###### 前缀索引

前缀索引是指对字符类型字段的前几个字符建立的索引，而不是在整个字段上建立的索引，前缀索引可以建立在字段类型为 char、 varchar、binary、varbinary 的列上。

目的是为了减少索引占用的存储空间，提升查询效率。

前缀索引有一定的局限性，例如：

- order by 无法使用前缀索引
- 无法用前缀索引来覆盖索引

##### 字段个数

联合索引先按第一个字段排序，然后再按下一个排序。

**最左匹配原则**：按照最左优先的方式进行索引的匹配。不遵循就会失效。

因为有查询优化器，所以字段在 where 子句的顺序并不重要。

> 如 `(a,b,c)` 为联合索引：a 全局有序，b/c 全局无序局部相对有序。因此下列排序失效，无法索引：
>
> - where b=2；
> - where c=3；
> - where b=2 and c=3；
>
> 这个有效：where a = 2 and b = 7

联合索引的最左匹配原则会一直向右匹配直到遇到范围查询就会停止匹配。也就是范围查询的字段可以用到联合索引，但是在范围查询字段的后面的字段无法用到联合索引。

> ```mysql
> select * from t_table where a > 1 and b = 2
> ```
>
> 因为 a>1 的范围里，b 无序，所以只有 a 使用了联合索引，b 没有用

也可以在执行计划中的 key_len 知道这一点，在使用联合索引进行查询的时候，通过 key_len 我们可以知道优化器具体使用了多少个字段的搜索条件来形成扫描区间的边界条件。

> 若 a 和 b 都是 int 类型且不为 NULL 的字段，则 key\_len 为 4 字节，若允许 NULL 就是 5 字节。
>
> 但是：
>
> ```mysql
> select * from t_table where a >= 1 and b = 2
> ```
>
> 因为 a=1 时可以使用 a,b(即 b 有序)，所以 a,b 都用到了，即 8 字节。同理：`a between 2 and 8 and b = 2` 也是 8 字节。

> 若：`(name, age)` 联合下面：
>
> ```mysql
> SELECT * FROM t_user WHERE name like 'j%' and age = 22
> ```
>
> 则相当于对 name 查询 `['j','k)`，故 `'j'` 可以用 age。所以用到了两个字段。
>
> 如若 `name` 是 utf8mb4 的 varchar(30)，需要 30x4+2，其 keylen 为 122，加上 age 共 126。
>
> key_len 的显示比较特殊，行格式是由 innodb存 储引擎实现的，而执行计划是在server 层生成的，所以它不会去问 innodb 存储引擎可变字段的长度占用多少字节，而是不管三七二十一都使用 2 字节表示可变字段的长度。毕竟 key_len 的目的只是为了告诉你索引查询中用了哪些索引字段，而不是为了准确告诉这个字段占用多少字节空间。

联合索引的排序：

> ```mysql
> select * from order where status = 1 order by create_time asc
> ```
>
> 更好的方式给 status 和 create_time 列建立一个联合索引，因为这样可以避免 MySQL 数据库发生文件排序。
>
> 只用到 status 的索引，但是这条语句还要对 create_time 排序，这时就要用文件排序 filesort，也就是在 SQL 执行计划中，Extra 列会出现 Using filesort。

##### 索引区分度

区分度就是某个字段 column 不同值的个数除以表的总行数。

建立联合索引时，要把区分度大的字段排在前面，这样区分度大的字段越有可能被更多的 SQL 使用到。

> 比如，性别的区分度就很小，不适合建立索引或不适合排在联合索引列的靠前的位置。

查询优化器发现某个值出现在表的数据行中的百分比（惯用的百分比界线是"30%"）很高的时候，它一般会忽略索引，进行全表扫描。

##### 索引的优缺点

好处：提高查询速度。

坏处：占用物理空间、创建和维护索引耗时，降低表的增删改效率。

适用索引的情况：

- 字段有唯一性限制的，比如商品编码
- 经常用于 `WHERE` 查询条件的字段，这样能够提高整个表的查询速度，如果查询条件不是一个字段，可以建立联合索引
- 经常用于 `GROUP BY` 和 `ORDER BY` 的字段，这样在查询的时候就不需要再去做一次排序了

不需要索引的情况：

- `WHERE` 条件，`GROUP BY`，`ORDER BY` 里用不到的字段
- 字段中存在大量重复数据，不需要创建索引，比如性别
- 表数据太少
- 经常更新的字段

#### 优化方法

##### 前缀索引优化

使用前缀索引是为了减小索引字段大小，可以增加一个索引页中存储的索引值，有效提高索引的查询速度。在一些大字符串的字段作为索引时，使用前缀索引可以帮助我们减小索引项的大小。

前缀索引有一定的局限性，例如：

- order by 就无法使用前缀索引
- 无法把前缀索引用作覆盖索引

##### 覆盖索引优化

对频繁查询的数据列，建立联合索引，避免回表。

##### 主键索引自增

建表的时候，都会默认将主键索引设置为自增的。

使用自增主键，那么每次插入的新数据就会按顺序添加到当前索引节点的位置，不需要移动已有的数据，当页面写满，就会自动开辟一个新页面。因为每次插入一条新记录，都是追加操作，不需要重新移动数据，因此这种插入数据的方法效率非常高。

非自增：可能不得不移动其它数据来满足新数据的插入，甚至需要从一个页面复制数据到另外一个页面，我们通常将这种情况称为页分裂。页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率。

主键字段的长度不要太大，因为主键字段长度越小，意味着二级索引的叶子节点越小（二级索引的叶子节点存放的数据是主键值），这样二级索引占用的空间也就越小

##### NOT NULL

- 索引列存在 NULL 就会导致优化器在做索引选择的时候更加复杂，更加难以优化，因为可为 NULL 的列会使索引、索引统计和值比较都更复杂，比如进行索引统计时，count 会省略值为 NULL 的行。
- NULL 值是一个没意义的值，但是它会占用物理空间，所以会带来的存储空间的问题。

##### 索引失效

[参考](https://mp.weixin.qq.com/s/lEx6iRRP3MbwJ82Xwp675w)

###### 常见情况

情况：(具体参见下文笔记)

- 左或者左右模糊匹配，即 `like %xx` 或者 `like %xx%`
- 查询条件中对索引列做了计算、函数、类型转换操作
- 没有遵循最左匹配原则
- 在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列

###### 执行计划

执行计划 explain：

**参数**

对于执行计划，参数有：

- possible_keys 字段表示可能用到的索引；
- key 字段表示实际用的索引，如果这一项为 NULL，说明没有使用索引；
- key_len 表示索引的长度；
- rows 表示扫描的数据行数。
- type 表示数据扫描类型，我们需要重点看这个。

**type**

type 字段就是描述了找到所需数据时使用的扫描方式是什么，常见扫描类型的执行效率从低到高的顺序为：

- All（全表扫描）；
- index（全索引扫描）；
- range（索引范围扫描）；
- ref（非唯一索引扫描）；
- eq_ref（唯一索引扫描）；
- const（结果只有一条的主键或唯一索引扫描）。

all 和 index 效率差不多。

range 表示采用了索引范围扫描，一般在 where 子句中使用 < 、>、in、between 等关键词，只检索给定范围的行，属于范围查找

ref 非唯一索引，或者是唯一索引的非唯一性前缀，返回数据返回可能是多条。因为虽然使用了索引，但该索引列的值并不唯一，有重复。这样即使使用索引快速查找到了第一条数据，仍然不能停止，要进行目标值附近的小范围扫描。

eq_ref 类型是使用主键或唯一索引时产生的访问方式，通常使用在多表联查中。比如，对两张表进行联查，关联条件是两张表的 user_id 相等，且 user_id 是唯一索引

const 类型表示使用了主键或者唯一索引与常量值进行比较。const 是与常量进行比较，查询效率会更快，而 eq_ref 通常用于多表联查中。

**extra**

- Using filesort ：当查询语句中包含 group by 操作，而且无法利用索引完成排序操作的时候， 这时不得不选择相应的排序算法进行，甚至可能会通过文件排序，效率是很低的，所以要避免这种问题的出现。
- Using temporary：使了用临时表保存中间结果，MySQL 在对查询结果排序时使用临时表，常见于排序 order by 和分组查询 group by。效率低，要避免这种问题的出现。
- Using index：所需数据只需在索引即可全部获得，不须要再到表中取数据，也就是使用了覆盖索引，避免了回表操作，效率不错

#### B+树

##### 定义

[参考](https://www.xiaolincoding.com/mysql/index/why_index_chose_bpuls_tree.html#%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%A0%91) [oiwiki](https://oi-wiki.org/ds/bplus-tree/)

B 树每个节点最多包括 m 个子节点(m 是 B 树的阶)，对 B 树范围查询需要中序遍历，速度下降。

B+ 树：(mysql 里的)(百科定义的索引数是 m-1)

- 叶子节点（最底部的节点）才会存放实际数据（索引+记录），非叶子节点只会存放索引；
- 所有索引都会在叶子节点出现，叶子节点之间构成一个有序链表；
- 非叶子节点的索引也会同时存在在子节点中，并且是在子节点中所有索引的最大（或最小）。
- 非叶子节点中有多少个子节点，就有多少个索引

B 树单点查询最优复杂度比 B+ 好，但插入删除更麻烦。

对于大量的单个索引查询的场景，可以考虑 B 树，比如 nosql 的MongoDB

##### 数据页

[参考](https://www.xiaolincoding.com/mysql/index/page.html#innodb-%E6%98%AF%E5%A6%82%E4%BD%95%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E7%9A%84)

###### 组成

数据页组成：

![图片](img/243b1466779a9e107ae3ef0155604a17.png)

各部分作用如下：

![图片](img/fabd6dadd61a0aa342d7107213955a72.png)

###### 作用

1. 文件头 38字节 File Header

   表示页的信息。分别指向上一个数据页和下一个数据页，连接起来的页相当于一个双向的链表

2. 页头 56字节 Page Header

   表示页的状态信息

3. 最大、最小记录 26字节 Infimum + Supermum

   两个虚拟伪记录，表示页的最小和最大记录

4. 用户记录 User Records

   行记录内容。按照主键顺序组成单向链表（插删方便，但检索效率不高）

5. 空闲空间 Free Space

   未使用空间

6. 页目录 Page Directory

   用户记录相对位置，对记录索引，提高检索作用

   构建过程：

   1. 所有的记录划分成几个组，这些记录包括最小记录和最大记录，但不包括标记为“已删除”的记录
   2. 每个记录组的最后一条记录就是组内最大的那条记录，并且最后一条记录的头信息中会存储该组一共有多少条记录，作为 n_owned 字段（上图中粉红色字段）
   3. 页目录用来存储每组最后一条记录的地址偏移量，这些地址偏移量会按照先后顺序存储起来，每组的地址偏移量也被称之为槽（slot），每个槽相当于指针指向了不同组的最后一个记录

   ![图片](img/261011d237bec993821aa198b97ae8ce.png)

   记录是按照主键值从小到大排序，所以可以对槽二分再遍历槽内所有记录

   如上图查逐渐 11，因为槽 3 最大是 12，所以从槽 3 开始

   槽内最小=上一个槽的最大的下一条

   槽内记录数目：

   - 第一个分组中的记录只能有 1 条记录；
   - 最后一个分组中的记录条数范围只能在 1-8 条之间；
   - 剩下的分组中记录条数范围只能在 4-8 条之间

7. 文件尾 8字节 File Trailer

   校验页是否完整

###### 查询

![图片](img/7c635d682bd3cdc421bb9eea33a5a413.png)

> 查找主键为 6 的记录，以上图为例子：
>
> - 主键值为 6，在[1, 7)范围之间，所以到页 30 中查找更详细的目录项
> - 同理到页 16
> - 二分槽，找到主键 6 的数据

##### 磁盘

内存的访问速度是纳秒级别的，而磁盘访问的速度是毫秒级别的，也就是说读取同样大小的数据，磁盘中读取的速度比从内存中读取的速度要慢上万倍，甚至几十万倍。

磁盘读写的最小单位是扇区，扇区的大小只有 `512B` 大小，操作系统一次会读写多个扇区，所以操作系统的最小读写单位是块（Block）。Linux 中的块大小为 `4KB`，也就是一次磁盘 I/O 操作会直接读写 8 个扇区。

#### 单表数据上限

##### 实验

2kw 行：

```sql
CREATE TABLE person(
    id int NOT NULL AUTO_INCREMENT PRIMARY KEY comment '主键',
    person_id tinyint not null comment '用户id',
    person_name VARCHAR(200) comment '用户名称',
    gmt_create datetime comment '创建时间',
    gmt_modified datetime comment '修改时间'
) comment '人员信息表';
```

插入一条：

```sql
insert into person values(1, 1,'user_1', NOW(), now());
```

设置伪列起始点：(大意是 i从100开始增长，循环 person 表行数次，把 person 表遍历与 i=100 两个表笛卡尔积，nx1=n)

```mysql
select (@i:=@i+1) as rownum, person_name 
from person, (select @i:=100) as init; 
set @i=1;
```

翻倍增加：

```sql
insert into person(id, person_id, person_name, gmt_create, gmt_modified)
select @i:=@i+1,
left(rand()*10,10) as person_id,
concat('user_',@i%2048),
date_add(gmt_create,interval + @i*cast(rand()*100 as signed) SECOND),
date_add(date_add(gmt_modified,interval +@i*cast(rand()*100 as signed) SECOND), interval + cast(rand()*1000000 as signed) SECOND)
from person;
```

如果不想翻倍翻倍的增加数据，而是想少量，少量的增加，有个技巧，就是在 SQL 的后面增加 where 条件，如 id > 某一个值去控制增加的数据量即可。

临时表内存扩大：

```mysql
SET GLOBAL tmp_table_size =512*1024*1024; （512M）
SET global innodb_buffer_pool_size= 1*1024*1024*1024 (1G);
```

##### 单表建议值

理论主键int 的话是 `2^32-1` 个最多。

设数据页的非叶子节点指向其他也的数量为 x，叶子节点有 y 行数据，B+ 树层数是 z，则若两层显然 `xy`，三层的话，第二层 `x` 个每层又 `x` 个故 `xxy`，故类推 z 层 `x^{z-1}y`。

设主键 bigint 8byte, 页号 4byte, 假设一页有 15K 有用，则 `x=15*1024/12=1280`。

假设一条数据 1K，则 `y=15`。若 `z=2`，则可以 19200 条，`z=3`，可以 `24576000`(2.45kw) 层。四层大约三百多亿，所以一般三层。

若一条数据 5K，即 `y=3`，则三层 500w。

影响查询性能的还有很多其他因素，比如，数据库版本，服务器配置，sql 的编写。

MySQL 为了提高性能，会将表的索引装载到内存中，在 InnoDB buffer size 足够的情况下，其能完成全加载进内存，查询不会有问题。

但是，当单表数据库到达某个量级的上限时，导致内存无法存储其索引，使得之后的 SQL 查询会产生磁盘 IO，从而导致性能下降，所以增加硬件配置（比如把内存当磁盘使），可能会带来立竿见影的性能提升。

#### 索引失效

##### 左模糊匹配

如 `select * from t_user where name like '%林';` 或 `%林%`

理由：按索引值排序，只能根据前缀比较。

当 `*` 除了索引和主键还有其他列，如果是 `林` 或 `林%` 就是 range，`%林` 就是 all

如果没其他列了，那么是( range: `林` 或 `林%` 的 possible keys 有，index: 而 `%林` 没有 possible keys)

> 因为二级索引树的记录东西很少，就只有索引列+主键值，而聚簇索引记录的东西会更多，比如聚簇索引中的叶子节点则记录了主键值、事务 id、用于事务和 MVCC 的回滚指针以及所有的剩余列。

##### 函数

如 `select * from t_user where length(name)=6;`

因为索引保存的是索引字段的原始值，而不是经过函数计算后的值。

但 MySQL 8 开始，增加了函数索引，如果用了对应的就可以：

```mysql
alter table t_user add key idx_name_length ((length(name)));
```

##### 表达式

`select * from t_user where id + 1 = 10;`

但是若 `id=10-1` 就可以。原理与索引使用函数类似。MySQL 不会实现表达式化简。

##### 隐式类型转换

索引字段是字符串类型，但是在条件查询中，输入的参数是整型会失效：

`select * from t_user where phone = 1300000001;`

但是如果索引字段是整型类型，查询条件中的输入参数即使字符串，是不会导致索引失效，还是可以走索引扫描。

`select * from t_user where id = '1';`

理由：数据类型转换规则，以 `select "10">9` 为例，表明：

MySQL 遇到字符串与数字比较，会把字符串转为数字。所以等效于：

```mysql
select * from t_user where CAST(phone AS signed int) = 1300000001;
select * from t_user where id = CAST("1" AS signed int);
```

##### 联合索引非最左匹配

创建了一个 `(a, b, c)` 联合索引，如果查询条件是以下这几种，就可以匹配上联合索引：

1. where a=1；
2. where a=1 and b=2；
3. where a=1 and b=2 and c=3；

因为有查询优化器，所以 a 字段在 where 子句的顺序并不重要。

注意：`where a=1 and c=3`：索引截断

- MySQL 5.5 a 走索引，回表找 c 

- MySQL 5.6：索引下推

  截断的字段不会在 Server 层进行条件判断，而是会被下推到存储引擎层进行条件判断（因为 c 字段的值是在 `(a, b, c)` 联合索引里的），然后过滤出符合条件的数据后再返回给 Server 层。由于在引擎层就过滤掉大量的数据，无需再回表读取数据来进行判断，减少回表次数，从而提升了性能。

[参考](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247495686&idx=2&sn=dfa18870d8cd2f430f893d402b9f4e54&chksm=f98db4accefa3dba680c1b343700ef87d184c45d4d7739bb0263cece3c1b21d0ca5f875736f6&scene=21#wechat_redirect)

特别地，如果表只有联合索引+主键里的值，那么直接 `where c=0` 就会用到索引因为会索引覆盖。所以结果是 type=index (full index scan)；如果是 `a=0` 就是 `ref` (基于索引的等值查询)。当然如果查询的有索引里没有的，就要扫描全表了。

##### where 子句的 or

如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。(也就是 or 的任意一个不是索引列)

因为 OR 的含义就是两个只要满足一个即可，因此只有一个条件列是索引列是没有意义的，只要有条件列不是索引列，就会进行全表扫描。

若两个都是索引：`select * from t_user where id = 1 or age = 18;`

type 是 index merge：对 id 和 age 分别进行了扫描，然后将这两个结果集进行了合并，这样做的好处就是避免了全表扫描

#### count

##### 概述

按照性能排序：

```sql
count(*) = count(1) > count(主键字段) > count(字段)
```

count() 是一个聚合函数，函数的参数不仅可以是字段名，也可以是其他任意表达式，该函数作用是统计符合查询条件的记录中，函数指定的参数不为 NULL 的记录有多少个。即：

`select count(name) from t_order;`  t_order 表中，name 字段不为 NULL 的记录有几个

`select count(1) from t_order;` 是 t_order 表中有几个记录(不管是不是 NULL，因为 1 恒不是 NULL)

##### count(主键)

如果表里只有主键索引，没有二级索引时，那么，InnoDB 循环遍历聚簇索引，将读取到的记录返回给 server 层，然后读取记录中的 id 值(主键值)，就会 id 值判断是否为 NULL，如果不为 NULL，就将 count 变量加 1。

如果表里有二级索引时，InnoDB 循环遍历的对象就是二级索引。

##### count(1)

区别在于不会读取记录中的任何字段的值，所以效率比主键高一些

会对 count(*) 和 count(1) 有个优化，如果有多个二级索引的时候，优化器会使用 key_len 最小的二级索引进行扫描。

只有当没有二级索引的时候，才会采用主键索引来进行统计

##### count(*)

等价于 `count(0)`，故等价于 `count(1)`。

##### count(字段)

若字段不是索引，全表扫描的方式来计数

如果非要统计表中该字段不为 NULL 的记录个数，建议给这个字段建立一个二级索引

##### 遍历的原因

通常在没有任何查询条件下的 count(*)，MyISAM 的查询速度要明显快于 InnoDB，只需要 O1，因为 MyISAM 每个表 meta 信息有 row\_count 值，表级锁保证一致性

InnoDB 支持事务，同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的，所以无法像 MyISAM一样，只维护一个 row_count 变量

当带上 where 条件语句之后，MyISAM 跟 InnoDB 就没有区别了，它们都需要扫描表来进行记录个数的统计

##### 优化

方法一：近似值：

`show table status` 或 `explain select count(*) from table_name`

explain 命令效率是很高的，因为它并不会真正的去查询，rows 字段值就是 explain 命令对表记录的估算值

方法二：开一个计数表，单独表示计数值。

### 事务

#### 基本概念

[参考](https://www.xiaolincoding.com/mysql/transaction/mvcc.html#%E4%BA%8B%E5%8A%A1%E6%9C%89%E5%93%AA%E4%BA%9B%E7%89%B9%E6%80%A7)

##### ACID

ACID：

- 原子性 atomicity：要么全部完成，要么全部不完成。
- 一致性 consistency：事务操作前和操作后，数据满足完整性约束，数据库保持一致性状态。如用户自定义业务规则符合(金额总数相等)。
- 隔离性 isolation：多个并发事务互不干扰。
- 持久性 durability：事务结束对数据的修改就是永久的，即便系统故障也不会丢失。

保证技术：

- 持久性：redo log (重做日志)
- 原子性：undo log (回滚日志)
- 隔离性：MVCC (多版本并发控制) / 锁机制
- 一致性：上面三个的结合

##### 隔离性问题

脏读(dirty read)、不可重复读(non-repeatable read)、幻读(phantom read)

- 脏读：读到其他事务未提交的数据
- 不可重复读：前后读取的数据不一致
- 幻读：前后读取的记录数量不一致

严重程度：脏读 > 不可重复读 > 幻读

###### 脏读

一个事务读到了另一个未提交事务修改过的数据。

> ![图片](img/10b513008ea35ee880c592a88adcb12f.png)
>
> 假设有 A 和 B 这两个事务同时在处理，事务 A 先开始从数据库中读取小林的余额数据，然后再执行更新操作，如果此时事务 A 还没有提交事务，而此时正好事务 B 也从数据库中读取小林的余额数据，那么事务 B 读取到的余额数据是刚才事务 A 更新后的数据，即使没有提交事务。因为事务 A 是还没提交事务的，也就是它随时可能发生回滚操作，如果在上面这种情况事务 A 发生了回滚，那么事务 B 刚才得到的数据就是过期的数据，这种现象就被称为脏读。

###### 不可重复读

在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了不可重复读现象。

> ![图片](img/f5b4f8f0c0adcf044b34c1f300a95abf.png)

###### 幻读

在一个事务内多次查询某个符合查询条件的记录数量，如果出现前后两次查询到的记录数量不一样的情况，就意味着发生了幻读现象。

> ![图片](img/d19a1019dc35dfe8cfe7fbff8cd97e31.png)
>
> 假设有 A 和 B 这两个事务同时在处理，事务 A 先开始从数据库查询账户余额大于 100 万的记录，发现共有 5 条，然后事务 B 也按相同的搜索条件也是查询出了 5 条记录。接下来，事务 A 插入了一条余额超过 100 万的账号，并提交了事务，此时数据库超过 100 万余额的账号个数就变为 6。然后事务 B 再次查询账户余额大于 100 万的记录，此时查询到的记录数量有 6 条，发现和前一次读到的记录数量不一样了，就感觉发生了幻觉一样，这种现象就被称为幻读
>
> 个人理解：与不可重复的区别在于不可重复是修改，幻读是增删。

The so-called phantom problem occurs within a transaction when the same query produces different sets of rows at different times. For example, if a SELECT is executed twice, but returns a row the second time that was not returned the first time, the row is a “phantom” row.

当同一个查询在不同的时间产生不同的结果集时，事务中就会出现所谓的幻象问题。例如，如果 SELECT 执行了两次，但第二次返回了第一次没有返回的行，则该行是“幻像”行。如：

```mysql
SELECT * FROM t_test WHERE id > 100;
```

只要 T1 和 T2 时刻执行产生的结果集是不相同的，那就发生了幻读的问题，比如：

- T1 时间执行的结果是有 5 条行记录，而 T2 时间执行的结果是有 6 条行记录，那就发生了幻读的问题
- T1 时间执行的结果是有 5 条行记录，而 T2 时间执行的结果是有 4 条行记录，也是发生了幻读的问题

##### 隔离级别

SQL 标准提出了四种隔离级别来规避这些隔离性问题现象，隔离级别越高，性能效率就越低

1. 读未提交 (read uncommitted)：指一个事务还没提交时，它做的变更就能被其他事务看到
2. 读提交 (read commited)：指一个事务提交之后，它做的变更才能被其他事务看到
3. 可重复读 (repeatable read, RR)：指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，MySQL InnoDB 引擎的默认隔离级别
4. 串行化 (serializable)：对记录加上读写锁，在多个事务对这条记录进行读写操作时，如果发生了读写冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行

隔离性水平越往下越高。

- 在读未提交隔离级别下，可能发生脏读、不可重复读和幻读现象；
- 在读提交隔离级别下，可能发生不可重复读和幻读现象，但不可能发生脏读现象
- 在可重复读隔离级别下，可能发生幻读现象，但是不可能脏读和不可重复读现象
- 在串行化隔离级别下，脏读、不可重复读和幻读现象都不可能会发生

不同的数据库厂商对 SQL 标准中规定的 4 种隔离级别的支持不一样，有的数据库只实现了其中几种隔离级别，MySQL 虽然支持 4 种隔离级别，但是与SQL 标准中规定的各级隔离级别允许发生的现象却有些出入

MySQL 在可重复读隔离级别下，可以很大程度上避免幻读现象的发生(参见下文 [参考](https://xiaolincoding.com/mysql/transaction/phantom.html))

- 针对快照读(select)，MVCC 解决幻读。因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题

- 针对当前读(select ... for update 等(`后端.md-MySQL-指令-事务指令-当前读`) )，通过 next-key lock (记录锁+间隙锁)

  加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题

> 例子：
>
> <img src="img/d5de450e901ed926d0b5278c8b65b9fe.png" alt="图片" style="zoom:50%;" />
>
> - 读未提交：200,200,200
> - 读提交：100,200,200
> - 可重复读：100,100,200
> - 串行化：100,100,200 (B被锁住了，提交 A 后才做 B)

具体实现：

1. 读未提交：直接读

2. 串行化：读写锁

3. 读提交 / 可重复读：Read View 来实现的，它们的区别在于创建 Read View 的时机不同；Read View 类比为数据快照。

   读提交隔离级别是在每个语句执行前都会重新生成一个 Read View，而可重复读隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View

##### 事务开始指令

> 开始事务的两种方法：

> - 第一种：begin/start transaction 命令；
> - 第二种：start transaction with consistent snapshot 命令；
>
> 这两种开启事务的命令，事务的启动时机是不同的：
>
> - 执行了 begin/start transaction 命令后，并不代表事务启动了。只有在执行这个命令后，执行了第一条 select 语句，才是事务真正启动的时机；
> - 执行了 start transaction with consistent snapshot 命令，就会马上启动事务。

> 自动提交：执行一条语句是否自动提交事务，是由 `autocommit` 参数决定的，默认是开启。所以，执行一条 update 语句也是会使用事务的。

##### Read View, MVCC

![img](img/readview结构.drawio.png)

字段：

1. `m_ids` 创建 Read View 时，当前数据库中活跃事务的事务 id 列表

   “活跃事务”指的就是，启动了但还没提交的事务。

2. `min_trx_id` 当前活跃事务的 id 最小的事务

3. `max_trx_id` 当前数据库应该给的下一个事务 id (max + 1)

4. `creator_trx_id` 创建该 read view 的事务 id

聚簇索引的两个隐藏列：

1. `trx_id` 改动该记录的事务 id，按时间排序
2. `roll_pointer` 指向旧版本记录(undo 日记)，找到修改前的记录

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/ReadView.drawio.png" alt="img" style="zoom:50%;" />

一个事务去访问记录的时候，除了自己的更新记录总是可见之外，还有这几种情况：

- `trx_id < min_trx_id`，可见，创建前已提交
- `trx_id >= max_trx_id` 不可见，创建后才启动
- `min_trx_id <= trx_id < max_trx_id`：
  - 在 `m_ids`，不可见，仍未提交
  - 不在 `m_ids`，可见，已提交

这种通过版本链来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制，Multiversion Concurrency Control）

##### 可重复读

可重复读隔离级别中，普通的 select 语句就是基于 MVCC 实现的快照读，也就是不会加锁的

如：

<img src="img/事务ab的视图-new.png" alt="img" style="zoom:50%;" />

- 事务 B 读取小林的账户余额记录，读到余额是 100 万；
- 事务 A 将小林的账户余额记录修改成 200 万，并没有提交事务；
- 事务 B 读取小林的账户余额记录，读到余额还是 100 万；
- 事务 A 提交事务；
- 事务 B 读取小林的账户余额记录，读到余额依然还是 100 万；

事务 A 修改时造成版本链：

<img src="img/image-20240525174503341.png" alt="image-20240525174503341" style="zoom:50%;" />

事务 B 第二次读：沿着 undo log 链条往下找旧版本的记录，直到找到 trx_id 小于事务 B 的 Read View 中的 min_trx_id 值的第一条记录

第三次读：同理，因为 read view 不变。(即 `m_ids` 等内容不变)

##### 读提交

如：

- 事务 B 读取数据（创建 Read View），小林的账户余额为 100 万；
- 事务 A 修改数据（还没提交事务），将小林的账户余额从 100 万修改成了 200 万；
- 事务 B 读取数据（创建 Read View），小林的账户余额为 100 万；
- 事务 A 提交事务；
- 事务 B 读取数据（创建 Read View），小林的账户余额为 200 万；

前两次事务 B 读取时创建的 read review 一样。最后一次时，read review 发生了变化，所以不再找版本链往下。

#### 幻读

[参考](https://www.xiaolincoding.com/mysql/transaction/phantom.html#%E4%BB%80%E4%B9%88%E6%98%AF%E5%B9%BB%E8%AF%BB)

##### 快照读

基于 MVCC，执行第一个查询后创建 Read View。参见上文。

##### 当前读

即 update, insert, delete 或 select...for update。若不加锁，新插入的数据会被查询出来。所以引入了间隙锁，来解决当前读，可重复读级别的幻读问题。

对查询范围加锁，如： `where id > 2` 的更新，则对这个 id 范围加锁(next-key lock，是间隙锁和记录锁的组合)。则后面想要插入时，因为被加了锁，于是要插入的事务生成插入意向锁，进入等待状态，直到加锁事务提交了。

##### 幻读场景

可重复读级别的幻读例子。

###### 例子一

设表格有 id (=1,2,3,4 都有数据), name 和 score 三列。

设事务 A：

```mysql
begin;
select * from t_stu where id = 5;
```

事务 B：

```sql
begin;
insert into t_stu values(5, '白茶', 18);
commit;
```

然后，事务 A 虽然看不到 id=5 的记录，但是试图去更新它，再查：

```sql
update t_stu set name = '果冻' where id = 5;
select * from t_stu where id = 5;
```

###### 例子二

T1 时刻，事务 A 快照读 id>100

T2 时刻，事务 B 插入 id=200 提交

T3 时刻，事务 A 当前读 id>100，发生幻读。

避免：开启事务马上执行 select for update 即可。

### 锁

#### 概念

[参考](https://www.xiaolincoding.com/mysql/lock/mysql_lock.html)

1. 全局锁：rtwrl
2. 表级锁：表锁、元数据锁(MDL)、意向锁、AUTO-INC 锁
3. 行级锁：Record Lock、Gap Lock、Next-Key Lock

##### 全局锁

使用：

```sql
flush tables with read lock
```

会阻塞的：数据 CUD，表结构更改(alter, drop)

释放：手动或会话断开

```sql
unlock tables
```

应用场景：全库逻辑备份，这样在备份数据库期间，不会因为数据或表结构的更新，而出现备份文件的数据与预期的不一样。

> 反例：
>
> 1. 先备份了用户表的数据；
> 2. 然后有用户发起了购买商品的操作；
> 3. 接着再备份商品表的数据。
>
> 则在备份里用户数据没变，商品少了一件。
>
> 缺点：无法更新，业务停滞。

支持可重复读，备份前开启事务，创建 Read View，整个事务执行(备份事务)期间都在用这个 Read View，而且由于 MVCC 的支持，备份期间业务依然可以对数据进行更新操作。

备份：`mysqldump`，加 `–single-transaction` 参数的时候，就会在备份数据库之前先开启事务。这种方法只适用于支持可重复读隔离级别的事务的存储引擎。对于 MyISAM 这种不支持事务的引擎，在备份数据库时就要使用全局锁的方法。

##### 表锁

指令详见 `后端.md`。

```sql
lock tables t_student read; -- 表级别的共享锁，也就是读锁
lock tables t_student write; -- 表级别的独占锁，也就是写锁
```

表锁除了会限制别的线程的读写外，也会限制本线程接下来的读写操作。也就是说如果本线程对学生表加了共享表锁，那么本线程接下来如果要对学生表执行写操作的语句，是会被阻塞的，当然其他线程对学生表进行写操作时也会被阻塞，直到锁被释放。

会话退出或手动释放所有：

```sql
unlock tables
```

尽量避免在使用 InnoDB 引擎的表使用表锁，因为表锁的颗粒度太大，会影响并发性能。

##### 元数据锁

MDL(metadata lock)

不需要显示的使用 MDL，因为当我们对数据库表进行操作时，会自动给这个表加上 MDL：

- 对一张表进行 CRUD 操作时，加的是 MDL 读锁；
- 对一张表做结构变更操作的时候，加的是 MDL 写锁；

MDL 是为了保证当用户对表执行 CRUD 操作时，防止其他线程对这个表结构做了变更。

当有线程在执行 select 语句（ 加 MDL 读锁）的期间，如果有其他线程要更改该表的结构（ 申请 MDL 写锁），那么将会被阻塞，直到执行完 select 语句（ 释放 MDL 读锁）。

反之，当有线程对表结构进行变更（ 加 MDL 写锁）的期间，如果有其他线程执行了 CRUD 操作（ 申请 MDL 读锁），那么就会被阻塞，直到表结构变更完成（ 释放 MDL 写锁）。

MDL 是在事务提交后才会释放，这意味着事务执行期间，MDL 是一直持有的。

> 考虑：
>
> 1. 首先，线程 A 先启用了事务（但是一直不提交），然后执行一条 select 语句，此时就先对该表加上 MDL 读锁；
> 2. 然后，线程 B 也执行了同样的 select 语句，此时并不会阻塞，因为读读并不冲突；
> 3. 接着，线程 C 修改了表字段，此时由于线程 A 的事务并没有提交，也就是 MDL 读锁还在占用着，这时线程 C 就无法申请到 MDL 写锁，就会被阻塞，
>
> 那么在线程 C 阻塞后，后续有对该表的 select 语句，就都会被阻塞，如果此时有大量该表的 select 语句的请求到来，就会有大量的线程被阻塞住，这时数据库的线程很快就会爆满了。

申请 MDL 锁的操作会形成一个队列，队列中写锁获取优先级高于读锁，一旦出现 MDL 写锁等待，会阻塞后续该表的所有 CRUD 操作。

所以为了能安全的对表结构进行变更，在对表结构变更前，先要看看数据库中的长事务，是否有事务已经对表加上了 MDL 读锁，如果可以考虑 kill 掉这个长事务，然后再做表结构的变更。

##### 意向锁

[参考+](https://blog.csdn.net/weixin_48052161/article/details/121985667)

intension locks (共享 IS，排他 IX)

- 在使用 InnoDB 引擎的表里对某些记录加上共享锁之前，需要先在表级别加上一个意向共享锁；
- 在使用 InnoDB 引擎的表里对某些纪录加上独占锁之前，需要先在表级别加上一个意向独占锁；

也就是，当执行插入、更新、删除操作，需要先对表加上意向独占锁，然后对该记录加独占锁。

而普通的 select 是不会加行级锁的，普通的 select 语句是利用 MVCC 实现一致性读，是无锁的。

select 也是可以对记录加共享锁和独占锁的，具体方式如下：

```sql
select ... lock in share mode;-- 先在表上加上意向共享锁，然后对读取的记录加共享锁
select ... for update;-- 先表上加上意向独占锁，然后对读取的记录加独占锁
```

意向共享锁和意向独占锁是表级锁，不会和行级的共享锁和独占锁发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁（`lock tables ... read`）和独占表锁（`lock tables ... write`）发生冲突。也就是说存在表级的排他(X)和共享(S)的话：

|      | S      | X      | IS     | IX     |
| ---- | ------ | ------ | ------ | ------ |
| S    | 兼容   | 不兼容 | 兼容   | 不兼容 |
| X    | 不兼容 | 不兼容 | 不兼容 | 不兼容 |
| IS   | 兼容   | 不兼容 | 兼容   | 兼容   |
| IX   | 不兼容 | 不兼容 | 兼容   | 兼容   |

表锁和行锁是满足读读共享、读写互斥、写写互斥的。

如果没有意向锁，那么加独占表锁时，就需要遍历表里所有记录，查看是否有记录存在独占锁，这样效率会很慢。

那么有了意向锁，由于在对记录加独占锁前，先会加上表级别的意向独占锁，那么在加独占表锁时，直接查该表是否有意向独占锁，如果有就意味着表里已经有记录被加了独占锁，这样就不用去遍历表里的记录。

所以，意向锁的目的是为了快速判断表里是否有记录被加锁。

##### 自增锁

AUTO-INC 锁是特殊的表锁机制，锁不是再一个事务提交后才释放，而是再执行完插入语句后就会立即释放。

在插入数据时，会加一个表级别的 AUTO-INC 锁，然后为被 `AUTO_INCREMENT` 修饰的字段赋值递增的值，等插入语句执行完成后，才会把 AUTO-INC 锁释放掉。

那么，一个事务在持有 AUTO-INC 锁的过程中，其他事务的如果要向该表插入语句都会被阻塞，从而保证插入数据时，被 `AUTO_INCREMENT` 修饰的字段的值是连续递增的。

但是， AUTO-INC 锁再对大量数据进行插入的时候，会影响插入性能，因为另一个事务中的插入会被阻塞。

因此， 在 MySQL 5.1.22 版本开始，InnoDB 存储引擎提供了一种轻量级的锁来实现自增。

一样也是在插入数据的时候，会为被 `AUTO_INCREMENT` 修饰的字段加上轻量级锁，然后给该字段赋值一个自增的值，就把这个轻量级锁释放了，而不需要等待整个插入语句执行完后才释放锁。

InnoDB 存储引擎提供了个 innodb_autoinc_lock_mode 的系统变量，是用来控制选择用 AUTO-INC 锁，还是轻量级的锁。

- 当 innodb_autoinc_lock_mode = 0，就采用 AUTO-INC 锁，语句执行结束后才释放锁；
- 当 innodb_autoinc_lock_mode = 2，就采用轻量级锁，申请自增主键后就释放锁，并不需要等语句执行后才释放。
- 当 innodb_autoinc_lock_mode = 1：
  - 普通 insert 语句，自增锁在申请之后就马上释放；
  - 类似 insert … select 这样的批量插入数据的语句，自增锁还是要等语句结束后才被释放；

> 当 innodb_autoinc_lock_mode = 2 是性能最高的方式，但是当搭配 binlog 的日志格式是 statement 一起使用的时候，在主从复制的场景中会发生数据不一致的问题。如：

> ![img](img/innodb_autoinc_lock_mode=2.png)

> 可能出现这样的情况：
>
> - session B 先插入了两个记录，(1,1,1)、(2,2,2)；
> - 然后，session A 来申请自增 id 得到 id=3，插入了（3,5,5)；
> - 之后，session B 继续执行，插入两条记录 (4,3,3)、 (5,4,4)。
>
> binlog 面对 t2 表的更新只会记录这两个 session 的 insert 语句，记录的顺序要么先记 session A 的 insert 语句，要么先记 session B 的 insert 语句。
>
> 这个 binlog 拿去从库执行，这时从库是按顺序执行语句的，只有当执行完一条 SQL 语句后，才会执行下一条 SQL。因此，在从库上不会发生像主库那样两个 session 同时执行向表 t2 中插入数据的场景。所以，在备库上执行了 session B 的 insert 语句，生成的结果里面，id 都是连续的。这时，主从库就发生了数据不一致。
>
> 要解决这问题，binlog 日志格式要设置为 row，这样在 binlog 里面记录的是主库分配的自增值，到备库执行的时候，主库的自增值是什么，从库的自增值就是什么。
>
> 所以，当 innodb_autoinc_lock_mode = 2 时，并且 binlog_format = row，既能提升并发性，又不会出现数据一致性问题。

##### 行级锁

InnoDB 引擎是支持行级锁的，而 MyISAM 引擎并不支持行级锁

普通的 select 语句是不会对记录加锁的，因为它属于快照读。如果要在查询时对记录加行锁，可以使用下面这两个方式，这种查询会加锁的语句称为锁定读。

```mysql
select ... lock in share mode -- 对读取的记录加共享锁
select ... for update; -- 对读取的记录加独占锁
```

必须在一个事务中，因为当事务提交了，锁就会被释放，所以在使用这两条语句的时候，要加上 begin、start transaction 或者 set autocommit = 0。当事务执行 commit 后，事务过程中生成的锁都会被释放。

##### 记录锁

Record Lock，分共享锁 S 锁和排他锁 X 锁。记录级别。显然，SS 兼容，XS 和 XX 不兼容。

对 +∞ 的锁可以同时持有，不会冲突。

##### 间隙锁

Gap Lock，只存在于可重复读隔离级别，目的是为了解决可重复读隔离级别下幻读的现象。

假设，表中有一个范围 id 为（3，5）间隙锁，那么其他事务就无法插入 id = 4 这条记录了，这样就有效的防止幻读现象的发生。

间隙锁虽然存在 X 型间隙锁和 S 型间隙锁，但是并没有什么区别，间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系。没有互斥是因为加锁是查，插入有专门的意向锁，不是用间隙的。

间隙锁的意义只在于阻止区间被插入，因此是可以共存的。一个事务获取的间隙锁不会阻止另一个事务获取同一个间隙范围的间隙锁，共享和排他的间隙锁是没有区别的，他们相互不冲突，且功能相同，即两个事务可以同时持有包含共同间隙（区间一样或包含关系）的间隙锁。

##### 临键锁

Next-Key Lock。记录锁+间隙锁。锁定一个范围，并且锁定记录本身。

假设，表中有一个范围 id 为（3，5] 的 next-key lock，那么其他事务即不能插入 id = 4 记录，也不能修改 id = 5 这条记录。所以兼容性与记录锁一样。

作用：防止幻读。

##### 插入意向锁

insert intention

一个事务在插入一条记录的时候，需要判断插入位置是否已被其他事务加了间隙锁（next-key lock 也包含间隙锁）。

> 即：每插入一条新记录，都需要看一下待插入记录的下一条记录上是否已经被加了间隙锁，如果已加间隙锁，此时会生成一个插入意向锁，然后锁的状态设置为等待状态，现象就是 Insert 语句会被阻塞

如果有的话，插入操作就会发生阻塞，直到拥有间隙锁的那个事务提交为止（释放间隙锁的时刻），在此期间会生成一个插入意向锁，表明有事务想在某个区间插入新记录，但是现在处于等待状态。

MySQL 加锁时，是先生成锁结构，然后设置锁的状态，如果锁状态是等待状态，并不是意味着事务成功获取到了锁，只有当锁状态为正常状态时，才代表事务成功获取到了锁。

插入意向锁名字虽然有意向锁，但是它并不是意向锁，它是一种特殊的间隙锁，属于行级别锁。

如果说间隙锁锁住的是一个区间，那么插入意向锁锁住的就是一个点。因而从这个角度来说，插入意向锁确实是一种特殊的间隙锁。

插入意向锁与间隙锁的另一个非常重要的差别是：尽管插入意向锁也属于间隙锁，但两个事务却不能在同一时间内，一个拥有间隙锁，另一个拥有该间隙区间内的插入意向锁（当然，插入意向锁如果不在间隙锁区间内则是可以的）。(不可以拥有的意思是处于 waiting 等待状态)

插入到同一个 gap 区间的多个意向锁(假设行不一样)不会相互冲突。

##### 隐式锁

Insert 语句在正常执行时是不会生成锁结构的，它是靠聚簇索引记录自带的 trx_id 隐藏列来作为隐式锁来保护记录的。(就是把这一列当锁用)

当事务需要加锁的时，如果这个锁不可能发生冲突，InnoDB会跳过加锁环节，这种机制称为隐式锁。隐式锁是 InnoDB 实现的一种延迟加锁机制，其特点是只有在可能发生冲突时才加锁，从而减少了锁的数量，提高了系统整体性能。

特殊情况转化为显式锁：

1. 记录之间有间隙锁 -> 设置插入意向锁
2. 与已有记录存在唯一键冲突
   - 主键索引重复，给该主键值聚簇索引记录添加 S 型记录锁
   - 唯一的二级索引重复，给该二级索引记录添加 S 型 next-key 锁

考虑：事务 A,B 插入同一条记录：

- A 先插入成功，该二级索引被隐式锁保护，未提交。
- B 插入，遇到重复，于是 A 的隐式锁变成了 X 记录锁，B 遇到锁冲突，申请 S next-key 阻塞。

如果不是唯一索引，那么不会阻塞。

#### 加锁机制

[参考](https://www.xiaolincoding.com/mysql/lock/how_to_lock.html)

##### 概述

普通的 select 语句是不会对记录加锁的（除了串行化隔离级别），因为它属于快照读，是通过 MVCC（多版本并发控制）实现的。锁定读可以加 S 或 X。update/delete 是 X。

在读已提交隔离级别下，行级锁的种类只有记录锁，也就是仅仅把一条记录锁上。

在可重复读隔离级别下，行级锁的种类除了有记录锁，还有间隙锁（目的是为了避免幻读），所以行级锁的种类主要有三类（和其组合 Next-Key Lock）。

行级锁加锁规则比较复杂，不同的场景，加锁的形式是不同的。

加锁的对象是索引，加锁的基本单位是 next-key lock，它是由记录锁和间隙锁组合而成的，next-key lock 是前开后闭区间，而间隙锁是前开后开区间。在能使用记录锁或者间隙锁就能避免幻读现象的场景下， next-key lock 就会退化成记录锁或间隙锁。基本原则是如果没有恰好的行完美覆盖区间，宁可加锁区间比实际区间更大。

下文 MySQL 版本是 8.0.26，隔离级别是可重复读。

##### 唯一索引等值查询

以主键索引为例。

- 当查询的记录是存在的，在索引树上定位到这一条记录后，将该记录的索引中的 next-key lock 会退化成记录锁。
- 当查询的记录是不存在的，在索引树找到第一条大于该查询记录的记录后，将该记录的索引中的 next-key lock 会退化成间隙锁。(找不到不退化，加一个 X)

如果是用二级索引（不管是不是唯一索引）进行锁定读查询的时候，除了会对二级索引项加行级锁（如果是唯一索引的二级索引，加锁规则和主键索引的案例相同），而且还会对查询到的记录的主键索引项上加记录锁。

查询锁：(参见 `后端.md`)(以 `select for update` 为例)

- 如果 LOCK_MODE 为 `X`，说明是 next-key 锁；
- 如果 LOCK_MODE 为 `X, REC_NOT_GAP`，说明是记录锁；
- 如果 LOCK_MODE 为 `X, GAP`，说明是间隙锁；

所以，记录存在时，因为唯一性，这个记录锁就可以防止幻读，且唯一索引的唯一性保证了它不能被插入重复的。

如果对一个 `id=x` 的地方(`LOCK_DATA`) 有间隙锁那么 `x` 和小于 `x` 的上一个值的开区间形成间隙锁。则这个区间内的插入会阻塞，而这两个值端点因为索引唯一性冲突会自然报错所以不必加额外的锁。不必往 `x` 加锁是因为加了之后 `x` 就无法删除了(删除不影响)。

##### 唯一索引范围查询

唯一索引进行范围查询时，会对每一个扫描到的索引加 next-key 锁，然后如果遇到下面这些情况，会退化成记录锁或者间隙锁：

- 针对大于等于的范围查询，因为存在等值查询的条件，那么如果等值查询的记录是存在于表中，那么该记录的索引中的 next-key 锁会退化成记录锁。
- 针对小于或者小于等于的范围查询。
  - 条件值不在表，扫描到终止范围查询的记录时，该记录的索引的 next-key 锁会退化成间隙锁，其他扫描到的记录，都是在这些记录的索引上加 next-key 锁。
  - 条件值在表：
    - 小于范围查询：扫描到终止范围查询的记录时，该记录的索引的 next-key 锁会退化成间隙锁。其他扫描到的记录，都是在这些记录的索引上加 next-key 锁。
    - 小于等于查询：不会退化的，都是 next-key。

例如：表里有 `id=1,5,15,15,20`

- 查 `>15`，加入两个 next-key: `(15,20]` 和 `(20, +∞]`。
- 查 `>=15`，则 `[15],(15,20],(20,+∞]`
- 查 `<6`，加入 `(-∞,1],(1,5],(6,10)` (第三个是间隙)
- 查 `<=5`，加入 `(-∞,1],(1,5]`。
- 查 `<=5`，加入 `(-∞,1],(1,5)`。

##### 非唯一索引等值查询

非唯一索引进行等值查询的时候，因为存在两个索引，一个是主键索引，一个是非唯一索引（二级索引），所以在加锁时，同时会对这两个索引都加锁，但是对主键索引加锁的时候，只有满足查询条件的记录才会对它们的主键索引加锁

- 当查询的记录存在时，由于不是唯一索引，所以肯定存在索引值相同的记录，于是非唯一索引等值查询的过程是一个扫描的过程，直到扫描到第一个不符合条件的二级索引记录就停止扫描，然后在扫描的过程中，对扫描到的二级索引记录加的是 next-key 锁，而对于第一个不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。同时，在符合查询条件的记录的主键索引上加记录锁。
- 当查询的记录不存在时，扫描到第一条不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。因为不存在满足查询条件的记录，所以不会对主键索引加锁。

如：(非唯一索引值为 `19,20,21,22,39`)

- 查询 `=25`，把 `(22,39)` 在二级索引加锁。

  因为二级索引相同时按主键排序，所以其 `22` 若有多个，指向最大的主键 `id`(不妨设为 `10`)

  对 `22` 的插入，有时会成功有时会阻塞。阻塞情况：

  - 如果小于主键 id(`10`)，插入成功。
  - 否则：(等于主键重复失败，大于阻塞)

  对 `39` 同理(不妨设 `id=20`)：

  - 大于 id 成功
  - 小于 id 阻塞，等于 id 失败

  此时 `LOCK_DATA` 先后是二级索引、聚簇索引的值。

- 查询`=22`，先加 `(21,22]`，且设对 `id=10` 加记录锁。然后再加一个 `(22,39)` 间隙锁。对临界值判断同上理。

  加上 `(22,39)` 的理由：没办法完全锁住 `22`，因为主键不是无穷，即比 `22` 对应主键更大的都可以插入。

##### 非唯一索引范围查询

非唯一索引范围查询，索引的 next-key lock 不会有退化为间隙锁和记录锁的情况，也就是非唯一索引进行范围查询时，对二级索引记录加锁都是加 next-key 锁。

如：`>=22`，求得：`(21,22],(22,39],(39,+∞]`。因为对主键无法完全包住所以一定要间隙。那么对应的所有行的主键索引都加记录锁。

##### 不加索引的查询

查询条件不加索引，锁定读, update, delete 都会给每一条记录索引加上 next-key 锁来锁住全表。

#### update无索引

[参考](https://www.xiaolincoding.com/mysql/lock/update_index.html) InnoDB 可重复读

可以设置 `sql_safe_updates=1` 参数。此时，update 必须满足：

- 使用含索引列的 where
- 使用 limit
- 使用不含索引的 where 和 limit

delete 必须满足：

- 使用 where(不管有无索引列) 和 limit

如果 where 条件带上了索引列，但是优化器最终扫描选择的是全表，而不是索引的话，我们可以使用 `force index([index_name])` 可以告诉优化器使用哪个索引，以此避免有几率锁全表带来的隐患。

```sql
SELECT * FROM users FORCE INDEX (idx_name) WHERE age > 30;
```

#### 死锁

##### 概述

[参考](https://www.xiaolincoding.com/mysql/lock/deadlock.html)

显然死锁条件为：互斥、占有等待、不可抢占、循环等待。

设 InnoDB，可重复读，没有打开思索检测。两事务要插入同一张表(如where查普通索引)，插之前先 select for update，然后事务 A select 完事务 B 也 select，就会相互等待造成死锁而如果不等待就会幻读(两个重复行)。

数据库层，打破循环等待：

- 事务等待锁的超时时间。如 InnoDB, `innodb_lock_wait_timeout` 默认 50 秒。
- 死锁检测。参数 `innodb_deadlock_detect` 设置为 on。

##### 例子

[加锁情况](https://www.xiaolincoding.com/mysql/lock/show_lock.html)

<img src="img/字节mysql面试题.jpeg" alt="img" style="zoom:50%;" />

id 是主键。

事务A执行第一条语句，加上了 X 间隙锁 `(20,30)`，和表 IX 类型意向锁。

事务B也执行第一条语句，加上同样的间隙锁。

事务A执行第二条，被事务B的间隙锁阻塞了，于是加入了插入意向锁。

同理，事务B执行第二条也等待事务A。

满足了：互斥、占有且等待、不可强占用、循环等待。

### 日志

#### 概述

[参考](https://www.xiaolincoding.com/mysql/log/how_update.html)

##### 三种日志

- undo log（回滚日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的原子性，主要用于事务回滚和 MVCC。
- redo log（重做日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的持久性，主要用于掉电等故障恢复；
- binlog （归档日志）：是 Server 层生成的日志，主要用于数据备份和主从复制；

##### 执行update的过程

1. 执行器调用存储引擎接口，若在缓存池直接返回不在就读盘到缓冲池再返回
1. 得到聚簇记录后，查看更新前后值是否一样，一样就忽略，不一样把更新前后的值传给 InnoDB 层。(实践一样也会加锁)
1. 开启事务，更新前记录 undo log，写入 undo 页，然后记录对应 redo log
1. 更新内存的记录，标记脏页，记录到 redo log(WAL 写脏页)
1. 记录语句1对应的 binlog 到 binlog cache
1. 两阶段提交，先 redo log 刷盘(状态 prepare)，binlog 刷盘，然后刷 redo log 状态为 commit。

#### undo log

执行一条语句是否自动提交事务，是由 `autocommit` 参数决定的，默认是开启。所以，执行一条 update 语句也是会使用事务的。

undo log 是一种用于撤销回退的日志。在事务没提交之前，MySQL 会先记录更新前的数据到 undo log 日志文件里面，当事务回滚时，可以利用 undo log 来进行回滚，如：

- 在插入一条记录时，要把这条记录的主键值记下来，这样之后回滚时只需要把这个主键值对应的记录删掉就好了；
- 在删除一条记录时，要把这条记录中的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了；
- 在更新一条记录时，要把被更新的列的旧值记下来，这样之后回滚时再把这些列更新为旧值就好了。

一条记录的每一次更新操作产生的 undo log 格式都有一个 roll_pointer 指针和一个 trx_id 事务id：

- 通过 trx_id 可以知道该记录是被哪个事务修改的
- 通过 roll_pointer 指针可以将这些 undo log 串成一个链表，这个链表就被称为版本链

刷盘（持久化到硬盘）：数据页的刷盘策略是一样的，都需要通过 redo log 保证持久化。

buffer pool 中有 undo 页，对 undo 页(undo log 会写入 Buffer Pool 中的 Undo 页面)的修改也都会记录到 redo log。redo log 会每秒刷盘，提交事务时也会刷盘，数据页和 undo 页都是靠这个机制保证持久化的。



#### buffer pool

缓存池 buffer pool，在存储层，Innod 存储引擎里。

修改完的记录先不刷盘先缓存起来，下次有读取查询直接命中。修改数据时，若这个数据在缓存池里，直接修改，设置这页为脏页（内存和磁盘不一致数据），减少磁盘 I/O 不会马上写脏页。

页默认大小 16KB，缓冲页同理。启动 MySQL 时给缓存池申请连续内存空间，划分为若干页。此时这些缓存页都是空闲的，之后随着程序的运行，才会有磁盘上的页被缓存到 Buffer Pool 中。

MySQL 刚启动的时候，会观察到使用的虚拟内存空间很大，而使用到的物理内存空间却很小，这是因为只有这些虚拟内存被访问后，操作系统才会触发缺页中断，申请物理内存，接着将虚拟地址和物理地址建立映射关系。

Buffer Pool 除了缓存索引页和数据页，还包括了 Undo 页，插入缓存、自适应哈希索引、锁信息等等。

查询一条记录时，InnoDB 是会把整个页的数据加载到 Buffer Pool 中，将页加载到 Buffer Pool 后，再通过页里的页目录去定位到某条具体的记录。

InnoDB 引擎会在适当的时候，由后台线程将缓存在 Buffer Pool 的脏页刷新到磁盘里，这就是 WAL （Write-Ahead Logging）技术。WAL 技术指的是， MySQL 的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写到磁盘上。 

![img](img/wal.png)

Buffer Pool 是基于内存的，而内存总是不可靠，万一断电重启，还没来得及落盘的脏页数据就会丢失。

#### redo log

##### 概述

为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来，这个时候更新就算完成了。

redo log 是物理日志，记录了某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新，每当执行一个事务就会产生这样的一条或者多条物理日志。

被修改 Undo 页面，需要记录对应 redo log。

开启事务后，InnoDB 层更新记录前，首先要记录相应的 undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。在内存修改该 Undo 页面后，需要记录对应的 redo log。

- redo log 记录了此次事务完成后的数据状态，记录的是更新之后的值；
- undo log 记录了此次事务开始前的数据状态，记录的是更新之前的值；

所以有了 redo log，再通过 WAL 技术，InnoDB 就可以保证即使数据库发生异常重启，之前已提交的记录都不会丢失，这个能力称为 crash-safe（崩溃恢复）。可以看出来， redo log 保证了事务四大特性中的持久性。

<img src="img/事务恢复.png" alt="事务恢复" style="zoom: 67%;" />

##### 写入方式

写入 redo log 的方式使用了追加操作， 所以磁盘操作是顺序写，而写入数据需要先找到写入位置，然后才写到磁盘，所以磁盘操作是随机写。

磁盘的顺序写比随机写高效的多，因此 redo log 写入磁盘的开销更小。

WAL 技术的另外一个优点：MySQL 的写操作从磁盘的随机写变成了顺序写，提升语句的执行性能。

产生的 redo log 也不是直接写入磁盘的，因为这样会产生大量的 I/O 操作，而且磁盘的运行速度远慢于内存。所以，redo log 也有自己的缓存—— redo log buffer，每当产生一条 redo log 时，会先写入到 redo log buffer。

redo log buffer 默认大小 16 MB，可以通过 `innodb_log_Buffer_size` 参数动态的调整大小，增大它的大小可以让 MySQL 处理大事务是不必写入磁盘，进而提升写 IO 性能。

<img src="img/redologbuf.webp" alt="事务恢复" style="zoom: 33%;" />

##### 刷盘策略

刷盘时机：

1. MySQL 正常关闭时
2. 当 redo log buffer 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘
3. InnoDB 的后台线程每隔 1 秒，将 redo log buffer 持久化到磁盘
4. 每次事务提交时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘（这个策略可由 innodb_flush_log_at_trx_commit 参数控制

单独执行一个更新语句的时候，InnoDB 引擎会自己启动一个事务，在执行更新语句的过程中，生成的 redo log 先写入到 redo log buffer 中，然后等事务提交的时候，再将缓存在 redo log buffer 中的 redo log 按组的方式顺序写到磁盘。上面这种 redo log 刷盘时机是在事务提交的时候，这个默认的行为。

除此之外，InnoDB 还提供了另外两种策略，由参数 `innodb_flush_log_at_trx_commit` 参数控制，可取的值有：0、1、2，默认值为 1，这三个值分别代表的策略如下：

- 0。表示每次事务提交时 ，还是将 redo log 留在 redo log buffer 中 ，该模式下在事务提交时不会主动触发写入磁盘的操作。

- 1。每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘，这样可以保证 MySQL 异常重启之后数据不会丢失。

- 2。提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到 redo log 文件并不意味着写入到了磁盘。[参考](https://xiaolincoding.com/os/6_file_system/pagecache.html)

  因为操作系统的文件系统中有个 Page Cache，专门用来缓存文件数据的，所以写入redo log文件意味着写入到了操作系统的文件缓存。

每隔一秒：

- 参数 0，将在 redo log buffer 中的 redo log ，通过调用 `write()` 写到操作系统的 Page Cache，然后调用 `fsync()` 持久化到磁盘。所以参数为 0 的策略，MySQL 进程的崩溃会导致上一秒钟所有事务数据的丢失
- 参数 2，调用 fsync，将缓存在操作系统中 Page Cache 里的 redo log 持久化到磁盘。所以参数为 2 的策略，较取值为 0 情况下更安全，因为 MySQL 进程的崩溃并不会丢失数据，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失

<img src="img/innodb_flush_log_at_trx_commit2.drawio.png" alt="img" style="zoom:67%;" />

这三个参数的数据安全性和写入性能的比较如下：(不可兼得)

- 数据安全性：参数 1 > 参数 2 > 参数 0
- 写入性能：参数 0 > 参数 2> 参数 1

具体而言：

- 在一些对数据安全性要求比较高的场景中，显然 `innodb_flush_log_at_trx_commit` 参数需要设置为 1。
- 在一些可以容忍数据库崩溃时丢失 1s 数据的场景中，我们可以将该值设置为 0，这样可以明显地减少日志同步到磁盘的 I/O 操作。
- 安全性和性能折中的方案就是参数 2，虽然参数 2 没有参数 0 的性能高，但是数据安全性方面比参数 0 强，因为参数 2 只要操作系统不宕机，即使数据库崩溃了，也不会丢失数据，同时性能方便比参数 1 高

##### 写满

nnoDB 存储引擎有 1 个重做日志文件组( redo log Group），重做日志文件组由有 2 个 redo log 文件组成，这两个 redo 日志的文件名叫 ：`ib_logfile0` 和 `ib_logfile1` 。

在重做日志组中，每个 redo log File 的大小是固定且一致的，假设每个 redo log File 设置的上限是 1 GB，那么总共就可以记录 2GB 的操作。重做日志文件组是以循环写的方式工作的，从头开始写，写到末尾就又回到开头，相当于一个环形。

所以 InnoDB 存储引擎会先写 ib_logfile0 文件，当 ib_logfile0 文件被写满的时候，会切换至 ib_logfile1 文件，当 ib_logfile1 文件也被写满时，会切换回 ib_logfile0 文件。

redo log 是为了防止 Buffer Pool 中的脏页丢失而设计的，那么如果随着系统运行，Buffer Pool 的脏页刷新到了磁盘中，那么 redo log 对应的记录也就没用了，这时候我们擦除这些旧记录，以腾出空间记录新的更新操作。

redo log 是循环写的方式，相当于一个环形，InnoDB 用 write pos 表示 redo log 当前记录写到的位置，用 checkpoint 表示当前要擦除的位置，如下图：

<img src="img/checkpoint.png" alt="img" style="zoom: 25%;" />

- write pos 和 checkpoint 的移动都是顺时针方向；
- write pos ～ checkpoint 之间的部分（图中的红色部分），用来记录新的更新操作；
- check point ～ write pos 之间的部分（图中蓝色部分）：待落盘(缓存更新了还没刷盘)的脏数据页记录；

如果 write pos 追上了 checkpoint，就意味着 redo log 文件满了，这时 MySQL 不能再执行新的更新操作，也就是说 MySQL 会被阻塞（因此所以针对并发量大的系统，适当设置 redo log 的文件大小非常重要）。此时会停下来将 Buffer Pool 中的脏页刷新到磁盘中，然后标记 redo log 哪些记录可以被擦除，接着对旧的 redo log 记录进行擦除，等擦除完旧记录腾出了空间，checkpoint 就会往后移动（图中顺时针），然后 MySQL 恢复正常运行，继续执行新的更新操作。

所以，一次 checkpoint 的过程就是脏页刷新到磁盘中变成干净页，然后标记 redo log 哪些记录可以被覆盖的过程

#### bin log

##### 概念

在存储引擎层的两个日志之外，MySQL 在完成一条更新操作后，Server 层还会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的所有 binlog 统一写 入 binlog 文件。binlog 文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作，比如 SELECT 和 SHOW 操作。

##### 对比

bin log 与 redo log 并存的原因：最开始 MySQL 里并没有 InnoDB 引擎，MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用 redo log 来实现 crash-safe 能力。

二者的区别：

###### 适用对象

binlog 所有存储引擎都可以，redo log 只有 InnoDB。

###### 文件格式

binlog 格式：

1. STATEMENT：每一条修改数据的 SQL 都会被记录到 binlog 中（相当于记录了逻辑操作，所以针对这种格式， binlog 可以称为逻辑日志），主从复制中 slave 端再根据 SQL 语句重现。但 STATEMENT 有动态函数的问题，比如用了 uuid 或者 now 这些函数，在主库上执行的结果并不是你在从库执行的结果，这种随时在变的函数会导致复制的数据不一致
2. ROW：记录行数据最终被修改成什么样了（这种格式的日志，就不能称为逻辑日志了），不会出现 STATEMENT 下动态函数的问题。但 ROW 的缺点是每行数据的变化结果都会被记录，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，而在 STATEMENT 格式下只会记录一个 update 语句而已
3. MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式；

redo log 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新

###### 写入方式

- binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。
- redo log 是循环写，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志

###### 用途

- binlog 用于备份恢复、主从复制；
- redo log 用于掉电等故障恢复

因此若突发故障删库全部数据，redo log 有边写边擦除，不能完全恢复数据。而只要记录在 binlog 上的数据，都可以恢复。

##### 主从复制

记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。

这个过程一般是异步的，也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。

三阶段：

1. 写入 Binlog：库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应
2. 同步 Binlog：从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应
3. 回放 Binlog：从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性

完成主从复制之后，你就可以在写数据时只写主库，在读数据时只读从库，这样即使写请求会锁表或者锁记录，也不会影响读请求的执行

因为从库数量增加，从库连接上来的 I/O 线程也比较多，主库也要创建同样多的 log dump 线程来处理复制的请求，对主库资源消耗比较高，同时还受限于主库的网络带宽。

所以在实际使用中，一个主库一般跟 2～3 个从库（1 套数据库，1 主 2 从 1 备主），这就是一主多从的 MySQL 集群结构。

主从复制的模型：

- 同步复制：MySQL 主库提交事务的线程要等待所有从库的复制成功响应，才返回客户端结果。这种方式在实际项目中，基本上没法用，原因有两个：一是性能很差，因为要复制到所有节点才返回响应；二是可用性也很差，主库和所有从库任何一个数据库出问题，都会影响业务。
- 异步复制（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。
- 半同步复制：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险

##### 刷盘

事务执行过程中，先把日志写到 binlog cache（Server 层的 cache），事务提交的时候，再把 binlog cache 写到 binlog 文件中

> 一个事务的 binlog 是不能被拆开的，因此无论这个事务有多大（比如有很多条语句），也要保证一次性写入。（这是因为有一个线程只能同时有一个事务在执行的设定）（这句没看懂），所以每当执行一个 begin/start transaction 的时候，就会默认提交上一个事务，这样如果一个事务的 binlog 被拆开的时候，在备库执行就会被当做多个事务分段自行，这样破坏了原子性，是有问题的。

因为 binlog 里连续，不能两个事务混着，所以事务的 binlog 先缓存在内存。MySQL 给每个线程分配了一片内存用于缓冲 binlog ，该内存叫 binlog cache，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。在事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 文件中，并清空 binlog cache

<img src="img/binlogcache.drawio.png" alt="binlog cach" style="zoom: 67%;" />

- 图中的 write，指的就是指把日志写入到 binlog 文件，但是并没有把数据持久化到磁盘，因为数据还缓存在文件系统的 page cache 里，write 的写入速度还是比较快的，因为不涉及磁盘 I/O。
- 图中的 fsync，才是将数据持久化到磁盘的操作，这里就会涉及磁盘 I/O，所以频繁的 fsync 会导致磁盘的 I/O 升高。

可以控制 fsync：（默认是参数 0）

- sync_binlog = 0 的时候，表示每次提交事务都只 write，不 fsync，后续交由操作系统决定何时将数据持久化到磁盘；
- sync_binlog = 1 的时候，表示每次提交事务都会 write，然后马上执行 fsync；
- sync_binlog =N(N>1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。

因为当设置为 1 的时候，即使主机发生异常重启，最多丢失一个事务的 binlog，而已经持久化到磁盘的数据就不会有影响，不过就是对写入性能影响太大。如果能容少量事务的 binlog 日志丢失的风险，为了提高写入的性能，一般会 sync_binlog 设置为 100~1000 中的某个数值。

#### 两阶段提交

##### 概念

事务提交后，redo log 和 binlog 都要持久化到磁盘，但是这两个是独立的逻辑，可能出现半成功的状态，这样就造成两份日志之间的逻辑不一致：

- 将 redo log 刷入到磁盘之后， MySQL 突然宕机了，而 binlog 还没有来得及写入。由于 binlog 丢失了这条更新语句，从库的这一行是旧值，与主库的值不一致性
- 如果在将 binlog 刷入到磁盘之后， MySQL 突然宕机了，而 redo log 还没有来得及写入。崩溃导致事务无效，主库是旧值，从库是新值。

 为了避免出现两份日志之间的逻辑不一致的问题，使用了两阶段提交来解决，两阶段提交其实是分布式事务一致性协议，它可以保证多个逻辑操作要不全部成功，要不全部失败，不会出现半成功的状态。

两阶段提交把单个事务的提交拆分成了 2 个阶段，分别是准备（Prepare）阶段和提交（Commit）阶段，每个阶段都由协调者（Coordinator）和参与者（Participant）共同完成。commit 语句执行的时候，会包含提交（Commit）阶段。(语句包含阶段)

##### 过程

为了保证这两个日志的一致性，MySQL 使用了内部 XA 事务 eXtended Architecture

<img src="img/两阶段提交.drawio.png" alt="两阶段提交" style="zoom: 50%;" />

redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog。

- prepare 阶段：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘（innodb_flush_log_at_trx_commit = 1 的作用
- commit 阶段：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘（sync_binlog = 1 的作用），接着调用引擎的提交事务接口，将 redo log 状态设置为 commit，此时该状态并不需要持久化到磁盘，只需要 write 到文件系统的 page cache 中就够了，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功

##### 异常分析

对 redo log 的 prepare 的 XID ，查在不在 binlog：

- 没有：是 prepare 后 commit 前崩溃。可以回滚。
- 有：写入了 binlog 但还没设置 commit。可以恢复。

两阶段提交是以 binlog 写成功为事务提交成功的标识，因为 binlog 写成功了，就意味着能在 binlog 中查找到与 redo log 相同的 XID。设计理由：

binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。

若事务没提交就崩溃了，redo log 持久化到硬盘(每秒一次)，因为 binlog 没持久化，所以会回滚。即：但是 binlog 必须在事务提交之后，才可以持久化到磁盘。

##### 缺陷

性能差：

- 磁盘 I/O 次数高：对于“双1”配置，每个事务提交都会进行两次 fsync（刷盘），一次是 redo log 刷盘，另一次是 binlog 刷盘。
- 锁竞争激烈：两阶段提交虽然能够保证单事务两个日志的内容一致，但在多事务的情况下，却不能保证两者的提交顺序一致，因此，在两阶段提交的流程基础上，还需要加一个锁来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致。

在早期的 MySQL 版本中，通过使用 prepare_commit_mutex 锁来保证事务提交的顺序，在一个事务获取到锁时才能进入 prepare 阶段，一直到 commit 阶段结束才能释放锁，下个事务才可以继续进行 prepare 操作。

通过加锁虽然完美地解决了顺序一致性的问题，但在并发量较大的时候，就会导致对锁的争用，性能不佳

##### 组提交

###### 概念

MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数，如果说 10 个事务依次排队刷盘的时间成本是 10，那么将这 10 个事务一次性一起刷盘的时间成本则近似于 1。

引入了组提交机制后，prepare 阶段不变，只针对 commit 阶段，将 commit 阶段拆分为三个过程：

- flush 阶段：多个事务按进入的顺序将 binlog 从 cache 写入文件（不刷盘）；
- sync 阶段：对 binlog 文件做 fsync 操作（多个事务的 binlog 合并一次刷盘）；
- commit 阶段：各个事务按顺序做 InnoDB commit 操作；

每个阶段都有一个队列，每个阶段有锁进行保护，因此保证了事务写入的顺序，第一个进入队列的事务会成为 leader，leader领导所在队列的所有事务，全权负责整队的操作，完成后通知队内其他事务操作结束。

对每个阶段引入了队列后，锁就只针对每个队列进行保护，不再锁住提交事务的整个过程，可以看的出来，锁粒度减小了，这样就使得多个阶段可以并发执行，从而提升效率。MySQL 5.6 没有 redo log 组提交，MySQL 5.7 有 redo log 组提交。

在 MySQL 5.6 的组提交逻辑中，每个事务各自执行 prepare 阶段，也就是各自将 redo log 刷盘，这样就没办法对 redo log 进行组提交。

在 MySQL 5.7 版本中，做了个改进，在 prepare 阶段不再让事务各自执行 redo log 刷盘操作，而是推迟到组提交的 flush 阶段，也就是说 prepare 阶段融合在了 flush 阶段。

双 1 例子：

###### flush

（sync_binlog 和 innodb_flush_log_at_trx_commit 都配置为 1）

<img src="img/组提交1.png" alt="img" style="zoom: 33%;" />

绿色事务组的 Leader 对 redo log 做一次 write + fsync，即一次将同组事务的 redolog 刷盘

<img src="img/组提交2.png" alt="img" style="zoom: 33%;" />

完成了 prepare 阶段后，将绿色这一组事务执行过程中产生的 binlog 写入 binlog 文件（调用 write，不会调用 fsync，所以不会刷盘，binlog 缓存在操作系统的文件系统中）

<img src="img/write_binlog.png" alt="img" style="zoom:33%;" />

flush 阶段队列的作用是用于支撑 redo log 的组提交

如果在这一步完成后数据库崩溃，由于 binlog 中没有该组事务的记录，所以 MySQL 会在重启后回滚该组事务。

###### sync

绿色这一组事务的 binlog 写入到 binlog 文件后，并不会马上执行刷盘的操作，而是会等待一段时间，这个等待的时长由 `Binlog_group_commit_sync_delay` 参数控制，目的是为了组合更多事务的 binlog，然后再一起刷盘

<img src="img/组提交4.png" alt="img" style="zoom:33%;" />

不过，在等待的过程中，如果事务的数量提前达到了 `Binlog_group_commit_sync_no_delay_count` 参数设置的值，就不用继续等待了，就马上将 binlog 刷盘

sync 阶段队列的作用是用于支持 binlog 的组提交。

如果想提升 binlog 组提交的效果，可以通过设置下面这两个参数来实现：

- `binlog_group_commit_sync_delay= N`，表示在等待 N 微妙后，直接调用 fsync，将处于文件系统中 page cache 中的 binlog 刷盘，也就是将 binlog 文件持久化到磁盘。
- `binlog_group_commit_sync_no_delay_count = N`，表示如果队列中的事务数达到 N 个，就忽视binlog_group_commit_sync_delay 的设置，直接调用 fsync，将处于文件系统中 page cache 中的 binlog 刷盘。

如果在这一步完成后数据库崩溃，由于 binlog 中已经有了事务记录，MySQL会在重启后通过 redo log 刷盘的数据继续进行事务的提交

###### commit

调用引擎的提交事务接口，将 redo log 状态设置为 commit

<img src="img/组提交6.png" alt="img" style="zoom:33%;" />

commit 阶段队列的作用是承接 sync 阶段的事务，完成最后的引擎提交，使得 sync 可以尽早的处理下一组事务，最大化组提交的效率

#### 磁盘IO优化

需要将 binlog 和 redo log 持久化到磁盘，那么如果出现 MySQL 磁盘 I/O 很高的现象，我们可以通过控制以下参数，来 “延迟” binlog 和 redo log 刷盘的时机，从而降低磁盘 I/O 的频率：

- 设置组提交的两个参数： binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，延迟 binlog 刷盘的时机，从而减少 binlog 的刷盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但即使 MySQL 进程中途挂了，也没有丢失数据的风险，因为 binlog 早被写入到 page cache 了，只要系统没有宕机，缓存在 page cache 里的 binlog 就会被持久化到磁盘。
- 将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000），表示每次提交事务都 write，但累积 N 个事务后才 fsync，相当于延迟了 binlog 刷盘的时机。但是这样做的风险是，主机掉电时会丢 N 个事务的 binlog 日志。
- 将 innodb_flush_log_at_trx_commit 设置为 2。表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到 redo log 文件并不意味着写入到了磁盘，因为操作系统的文件系统中有个 Page Cache，专门用来缓存文件数据的，所以写入 redo log 文件意味着写入到了操作系统的文件缓存，然后交由操作系统控制持久化到磁盘的时机。但是这样做的风险是，主机掉电的时候会丢数据

### 内存

即 buffer pool 缓存池。[参考](https://www.xiaolincoding.com/mysql/buffer_pool/buffer_pool.html)，并参见日志一节。

#### 概述

##### 概念

Buffer Pool 是在 MySQL 启动的时候，向操作系统申请的一片连续的内存空间，默认配置下 Buffer Pool 只有 `128MB` 。页默认大小 16KB。

可以通过调整 `innodb_buffer_pool_size` 参数来设置 Buffer Pool 的大小，一般建议设置成可用物理内存的 60%~80%

Buffer Pool 除了缓存索引页和数据页，还包括了 undo 页，插入缓存、自适应哈希索引、锁信息等等。

##### 控制块

为了更好的管理这些在 Buffer Pool 中的缓存页，InnoDB 为每一个缓存页都创建了一个控制块，控制块信息包括缓存页的表空间、页号、缓存页地址、链表节点等

控制块也是占有内存空间的，它是放在 Buffer Pool 的最前面，接着才是缓存页。控制块和缓存页之间灰色部分称为碎片空间(不足以形成一个控制块+一个缓冲页的)，把 Buffer Pool 的大小设置的刚刚好的话，也可能不会产生碎片。

<img src="img/缓存页.drawio.png" alt="img" style="zoom: 50%;" />

#### 管理策略

##### 空闲页

遍历这一片连续的内存空间来找到空闲的缓存页效率太低，所以使用链表结构，将空闲缓存页的控制块作为链表的节点，这个链表称为 Free 链表（空闲链表）。

Free 链表上除了有控制块，还有一个头节点，该头节点包含链表的头节点地址，尾节点地址，以及当前链表中节点的数量等信息。

有了 Free 链表后，每当需要从磁盘中加载一个页到 Buffer Pool 中时，就从 Free链表中取一个空闲的缓存页，并且把该缓存页对应的控制块的信息填上，然后把该缓存页对应的控制块从 Free 链表中移除

##### 脏页

更新数据的时候，不需要每次都要写入磁盘，而是将 Buffer Pool 对应的缓存页标记为脏页，然后再由后台线程将脏页写入到磁盘。

那为了能快速知道哪些缓存页是脏的，于是就设计出 Flush 链表，它跟 Free 链表类似的，链表的节点也是控制块，区别在于 Flush 链表的元素都是脏页

##### 干净页

三种页和链表来管理数据：

- Free Page（空闲页），表示此页未被使用，位于 Free 链表；
- Clean Page（干净页），表示此页已被使用，但是页面未发生修改，位于LRU 链表(或其他，下同)。
- Dirty Page（脏页），表示此页已被使用且已经被修改，其数据和磁盘上的数据已经不一致。当脏页上的数据写入磁盘后，内存数据和磁盘数据一致，那么该页就变成了干净页。脏页同时存在于 LRU 链表和 Flush 链表

##### 淘汰策略

Buffer Pool 的大小是有限的，对于一些频繁访问的数据我们希望可以一直留在 Buffer Pool 中，而一些很少访问的数据希望可以在某些时机可以淘汰掉，从而保证 Buffer Pool 不会因为满了而导致无法再缓存新的数据，同时还能保证常用数据留在 Buffer Pool 中

##### LRU

LRU：Least recently used

链表头部的节点是最近使用的，而链表末尾的节点是最久没被使用的。那么，当空间不够了，就淘汰最久没被使用的节点，从而腾出空间。

- 当访问的页在 Buffer Pool 里，就直接把该页对应的 LRU 链表节点移动到链表的头部。
- 当访问的页不在 Buffer Pool 里，除了要把页放入到 LRU 链表的头部，还要淘汰 LRU 链表末尾的节点(满时)。

缺陷：

- 预读失效
- Buffer Pool 污染

###### 预读失效

MySQL 的预读机制。程序是有空间局部性的，靠近当前被访问数据的数据，在未来很大概率会被访问到。

所以，MySQL 在加载数据页时，会提前把它相邻的数据页一并加载进来，目的是为了减少磁盘 IO。

如果这些预读页如果一直不会被访问到，就会出现一个很奇怪的问题，不会被访问的预读页却占用了 LRU 链表前排的位置，而末尾淘汰的页，可能是频繁访问的页，这样就大大降低了缓存命中率。

我们不能因为害怕预读失效，而将预读机制去掉，大部分情况下，局部性原理还是成立的。

最好就是让预读的页停留在 Buffer Pool 里的时间要尽可能的短，让真正被访问的页才移动到 LRU 链表的头部，从而保证真正被读取的热数据留在 Buffer Pool 里的时间尽可能长。

将 LRU 划分了 2 个区域：old 区域 和 young 区域。young 区域在 LRU 链表的前半部分，old 区域则是在后半部分。old 区域占整个 LRU 链表长度的比例可以通过 `innodb_old_blocks_pct` 参数来设置，默认是 37，代表整个 LRU 链表中 young 区域与 old 区域比例是 63:37。

划分这两个区域后，预读的页就只需要加入到 old 区域的头部，当页被真正访问的时候，才将页插入 young 区域的头部。如果预读的页一直没有被访问，就会从 old 区域移除，这样就不会影响 young 区域中的热点数据。

old 尾淘汰直接删除出链表，young 尾淘汰变成 old 头。

<img src="img/lrutwo2.png" alt="img" style="zoom: 33%;" />

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/lrutwo3.png" alt="img" style="zoom:33%;" />

###### buffer pool 污染

当某一个 SQL 语句扫描了大量的数据时，在 Buffer Pool 空间比较有限的情况下，可能会将 Buffer Pool 里的所有页都替换出去，导致大量热数据被淘汰了，等这些热数据又被再次访问的时候，由于缓存未命中，就会产生大量的磁盘 IO，MySQL 性能就会急剧下降，这个过程被称为 Buffer Pool 污染。

注意， Buffer Pool 污染并不只是查询语句查询出了大量的数据才出现的问题，即使查询出来的结果集很小，也会造成 Buffer Pool 污染。如全表扫描。

解决：进入到 young 区域条件增加了一个停留在 old 区域的时间判断

在对某个处在 old 区域的缓存页进行第一次访问时，就在它对应的控制块中记录下来这个访问时间：

- 如果后续的访问时间与第一次访问的时间在某个时间间隔内，那么该缓存页就不会被从 old 区域移动到 young 区域的头部；(偶然密集访问)
- 如果后续的访问时间与第一次访问的时间不在某个时间间隔内，那么该缓存页移动到 young 区域的头部；(频繁访问)

只有同时满足被访问与在 old 区域停留时间超过 1 秒两个条件，才会被插入到 young 区域头部，这样就解决了 Buffer Pool 污染的问题

MySQL 针对 young 区域其实做了一个优化，为了防止 young 区域节点频繁移动到头部。young 区域前面 1/4 被访问不会移动到链表头部，只有后面的 3/4被访问了才会

##### 脏页刷盘时机

Write Ahead Log 策略，即先写日志，再写入磁盘，通过 redo log 日志让 MySQL 拥有了崩溃恢复能力 (日记顺序，磁盘随机，所以写日记很快)

下面几种情况会触发脏页的刷新：

- 当 redo log 日志满了的情况下，会主动触发脏页刷新到磁盘；
- Buffer Pool 空间不足时，需要将一部分数据页淘汰掉，如果淘汰的是脏页，需要先将脏页同步到磁盘；
- MySQL 认为空闲时，后台线程会定期将适量的脏页刷入到磁盘；
- MySQL 正常关闭之前，会把所有的脏页刷入到磁盘

在我们开启了慢 SQL 监控后，如果你发现偶尔会出现一些用时稍长的 SQL，这可能是因为脏页在刷新到磁盘时可能会给数据库带来性能开销，导致数据库操作抖动。

如果间断出现这种现象，就需要调大 Buffer Pool 空间或 redo log 日志的大小

## Redis

其实践部分见 `NoSQL-Redis`

### 基础

[参考](https://xiaolincoding.com/redis/base/redis_interview.html#%E8%AE%A4%E8%AF%86-redis)

#### 概念

##### 概述

Redis 是用 C 语言实现的一种基于内存的数据库，对数据的读写操作都是在内存中完成，因此读写速度非常快，常用于缓存，消息队列、分布式锁等场景。

Redis 提供了多种数据类型来支持不同的业务场景，比如 String(字符串)、Hash(哈希)、 List (列表)、Set(集合)、Zset(有序集合)、Bitmaps（位图）、HyperLogLog（基数统计）、GEO（地理信息）、Stream（流），并且对数据类型的操作都是原子性的，因为执行命令由单线程负责的，不存在并发竞争的问题。

Redis 还支持事务 、持久化、Lua 脚本、多种集群方案（主从复制模式、哨兵模式、切片机群模式）、发布/订阅模式，内存淘汰机制、过期删除机制等。

##### Memcached

Memcached 也是基于内存的数据库

 Redis 与 Memcached 共同点：

1. 都是基于内存的数据库，一般都用来当做缓存使用。
2. 都有过期策略。
3. 两者的性能都非常高。

Redis 与 Memcached 区别：

- Redis 支持的数据类型更丰富（String、Hash、List、Set、ZSet），而 Memcached 只支持最简单的 key-value 数据类型；
- Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用，而 Memcached 没有持久化功能，数据全部存在内存之中，Memcached 重启或者挂掉后，数据就没了；
- Redis 原生支持集群模式，Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；
- Redis 支持发布订阅模型、Lua 脚本、事务等功能，而 Memcached 不支持

##### 用作缓存的原因

兼具：高性能、高并发。

1. 假如用户第一次访问 MySQL 中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据缓存在 Redis 中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了，操作 Redis 缓存就是直接操作内存，所以速度相当快。

   如果 MySQL 中的对应数据改变的之后，同步改变 Redis 缓存中相应的数据即可，不过这里会有 Redis 和 MySQL 双写一致性的问题

2. 单台设备的 Redis 的 QPS（Query Per Second，每秒钟处理完请求的次数） 是 MySQL 的 10 倍，Redis 单机的 QPS 能轻松破 10w，而 MySQL 单机的 QPS 很难破 1w。

   所以，直接访问 Redis 能够承受的请求是远远大于直接访问 MySQL 的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库

### 数据结构

#### 数据类型

##### 概述

<img src="img/key.png" alt="img" style="zoom:50%;" />

<img src="img/五种数据类型.png" alt="img" style="zoom: 50%;" />

随着 Redis 版本的更新，后面又支持了四种数据类型： BitMap（2.2 版新增）、HyperLogLog（2.8 版新增）、GEO（3.2 版新增）、Stream（5.0 版新增）。 

- String 类型的应用场景：缓存对象、常规计数、分布式锁、共享 session 信息等。
- List 类型的应用场景：消息队列（但是有两个问题：1. 生产者需要自行实现全局唯一 ID；2. 不能以消费组形式消费数据）等。
- Hash 类型：缓存对象、购物车等。
- Set 类型：聚合计算（并集、交集、差集）场景，比如点赞、共同关注、抽奖活动等。
- Zset 类型：排序场景，比如排行榜、电话和姓名排序等。

Redis 后续版本又支持四种数据类型，它们的应用场景如下：

- BitMap（2.2 版新增）：二值状态统计的场景，比如签到、判断用户登陆状态、连续签到用户总数等；
- HyperLogLog（2.8 版新增）：海量数据基数统计的场景，比如百万级网页 UV 计数等；
- GEO（3.2 版新增）：存储地理位置信息的场景，比如滴滴叫车；
- Stream（5.0 版新增）：消息队列，相比于基于 List 类型实现的消息队列，有这两个特有的特性：自动生成全局唯一消息ID，支持以消费组形式消费数据。

#### String

##### 基本

基本的 key-value 结构，key 是唯一标识，value 是具体的值，value其实不仅是字符串， 也可以是数字（整数或浮点数），value 最多可以容纳的数据长度是 `512M`

##### 指令

```
# 设置 key-value 类型的值
> SET name lin
OK
# 根据 key 获得对应的 value
> GET name
"lin"
# 判断某个 key 是否存在
> EXISTS name
(integer) 1
# 返回 key 所储存的字符串值的长度
> STRLEN name
(integer) 3
# 删除某个 key 对应的值
> DEL name
(integer) 1
```

```
# 批量设置 key-value 类型的值
> MSET key1 value1 key2 value2 
OK
# 批量获取多个 key 对应的 value
> MGET key1 key2 
1) "value1"
2) "value2"
```

```
# 设置 key-value 类型的值
> SET number 0
OK
# 将 key 中储存的数字值增一
> INCR number
(integer) 1
# 将key中存储的数字值加 10
> INCRBY number 10
(integer) 11
# 将 key 中储存的数字值减一
> DECR number
(integer) 10
# 将key中存储的数字值键 10
> DECRBY number 10
(integer) 0
```

```
# 设置 key 在 60 秒后过期（该方法是针对已经存在的key设置过期时间）
> EXPIRE name  60 
(integer) 1
# 查看数据还有多久过期
> TTL name 
(integer) 51

#设置 key-value 类型的值，并设置该key的过期时间为 60 秒
> SET key  value EX 60
OK
> SETEX key  60 value
OK
```

```
# 不存在就插入（not exists）
>SETNX key value
(integer) 1
```

##### 应用场景

###### 缓存对象

使用 String 来缓存对象有两种方式：

- 直接缓存整个对象的 JSON，命令例子： `SET user:1 '{"name":"xiaolin", "age":18}'`。
- 采用将 key 进行分离为 user:ID:属性，采用 MSET 存储，用 MGET 获取各属性值，命令例子： `MSET user:1:name xiaolin user:1:age 18 user:2:name xiaomei user:2:age 20`。

###### 计数

因为 Redis 处理命令是单线程，所以执行命令的过程是原子的。因此 String 数据类型适合计数场景，比如计算访问次数、点赞、转发、库存数量等等。

比如计算文章的阅读量：

```
# 初始化文章的阅读量
> SET aritcle:readcount:1001 0
OK
#阅读量+1
> INCR aritcle:readcount:1001
(integer) 1
#阅读量+1
> INCR aritcle:readcount:1001
(integer) 2
#阅读量+1
> INCR aritcle:readcount:1001
(integer) 3
# 获取对应文章的阅读量
> GET aritcle:readcount:1001
"3"
```

###### 分布式锁

参见杂项

###### 共享 Session

开发后台管理系统时，会使用 Session 来保存用户的会话(登录)状态，这些 Session 信息会被保存在服务器端，但这只适用于单系统应用，如果是分布式系统此模式将不再适用

例如用户一的 Session 信息被存储在服务器一，但第二次访问时用户一被分配到服务器二，这个时候服务器并没有用户一的 Session 信息，就会出现需要重复登录的问题，问题在于分布式系统每次会把请求随机分配到不同的服务器

借助 Redis 对这些 Session 信息进行统一的存储和管理，这样无论请求发送到那台服务器，服务器都会去同一个 Redis 获取相关的 Session 信息

#### List

##### 基本

List 列表是简单的字符串列表，按照插入顺序排序，可以从头部或尾部向 List 列表添加元素。

列表的最大长度为 `2^32 - 1`，也即每个列表支持超过 `40 亿`个元素

##### 常用命令

```
# 将一个或多个值value插入到key列表的表头(最左边)，最后的值在最前面
LPUSH key value [value ...] 
# 将一个或多个值value插入到key列表的表尾(最右边)
RPUSH key value [value ...]
# 移除并返回key列表的头元素
LPOP key     
# 移除并返回key列表的尾元素
RPOP key 

# 返回列表key中指定区间内的元素，区间以偏移量start和stop指定，从0开始
LRANGE key start stop

# 从key列表表头弹出一个元素，没有就阻塞timeout秒，如果timeout=0则一直阻塞
BLPOP key [key ...] timeout
# 从key列表表尾弹出一个元素，没有就阻塞timeout秒，如果timeout=0则一直阻塞
BRPOP key [key ...] timeout
```

##### 应用场景

###### 消息队列

消息队列在存取消息时，必须要满足三个需求，分别是消息保序、处理重复的消息和保证消息可靠性

Redis 的 List 和 Stream 两种数据类型，就可以满足消息队列的这三个需求

List 可以使用 LPUSH + RPOP （或者反过来，RPUSH+LPOP）命令实现消息队列

- 生产者使用 `LPUSH key value[value...]` 将消息插入到队列的头部，如果 key 不存在则会创建一个空的队列再插入消息。
- 消费者使用 `RPOP key` 依次读取队列的消息，先进先出

潜在的性能风险点：在生产者往 List 中写入数据时，List 并不会主动地通知消费者有新消息写入，如果消费者想要及时处理消息，就需要在程序中不停地调用 `RPOP` 命令（比如使用一个while(1)循环）。如果有新消息写入，RPOP命令就会返回结果，否则，RPOP命令返回空值，再继续循环，所以，即使没有新消息写入List，消费者也要不停地调用 RPOP 命令，这就会导致消费者程序的 CPU 一直消耗在执行 RPOP 命令上，带来不必要的性能损失

解决：提供了 BRPOP 命令。BRPOP命令也称为阻塞式读取，客户端在没有读到队列数据时，自动阻塞，直到有新的数据写入队列，再开始读取新数据

重复消息：

消费者要实现重复消息的判断，需要 2 个方面的要求：

- 每个消息都有一个全局的 ID。
- 消费者要记录已经处理过的消息的 ID。当收到一条消息后，消费者程序就可以对比收到的消息 ID 和记录的已处理过的消息 ID，来判断当前收到的消息有没有经过处理。如果已经处理过，那么，消费者程序就不再进行处理了。

但是 List 并不会为每个消息生成 ID 号，所以需要自行为每个消息生成一个全局唯一ID，生成之后，在用 LPUSH 命令把消息插入 List 时，需要在消息中包含这个全局唯一 ID

可靠性：

当消费者程序从 List 中读取一条消息后，List 就不会再留存这条消息了。所以，如果消费者程序在处理消息的过程出现了故障或宕机，就会导致消息没有处理完成，那么，消费者程序再次启动后，就没法再次从 List 中读取消息了。

为了留存消息，List 类型提供了 `BRPOPLPUSH` 命令，这个命令的作用是让消费者程序从一个 List 中读取消息，同时，Redis 会把这个消息再插入到另一个 List（可以叫作备份 List）留存

如果消费者程序读了消息但没能正常处理，等它重启后，就可以从备份 List 中重新读取消息并进行处理了

- 消息保序：使用 LPUSH + RPOP；
- 阻塞读取：使用 BRPOP；
- 重复消息处理：生产者自行实现全局唯一 ID；
- 消息的可靠性：使用 BRPOPLPUSH

缺陷：List 不支持多个消费者消费同一条消息，因为一旦消费者拉取一条消息后，这条消息就从 List 中删除了，无法被其它消费者再次消费。

要实现一条消息可以被多个消费者消费，那么就要将多个消费者组成一个消费组，使得多个消费者可以消费同一条消息，但是 List 类型并不支持消费组的实现。

Redis 从 5.0 版本开始提供的 Stream 数据类型，Stream 同样能够满足消息队列的三大需求，而且它还支持消费组形式的消息读取

#### Hash

##### 简介

键值对

Hash 特别适合用于存储对象

##### 常用命令

```
# 存储一个哈希表key的键值
HSET key field value   
# 获取哈希表key对应的field键值
HGET key field

# 在一个哈希表key中存储多个键值对
HMSET key field value [field value...] 
# 批量获取哈希表key中多个field键值
HMGET key field [field ...]       
# 删除哈希表key中的field键值
HDEL key field [field ...]    

# 返回哈希表key中field的数量
HLEN key       
# 返回哈希表key中所有的键值
HGETALL key 

# 为哈希表key中field键的值加上增量n
HINCRBY key field n       
```

##### 应用场景

###### 缓存对象

```
# 存储一个哈希表uid:1的键值
> HMSET uid:1 name Tom age 15
2
# 存储一个哈希表uid:2的键值
> HMSET uid:2 name Jerry age 13
2
# 获取哈希表用户id为1中所有的键值(无序)
> HGETALL uid:1
1) "name"
2) "Tom"
3) "age"
4) "15"
```

存储对象一般用 String + Json 存储，对象中某些频繁变化的属性可以考虑抽出来用 Hash 类型存储

###### 购物车

用户 id 为 key，商品 id 为 field，商品数量为 value，恰好构成了购物车的3个要素

涉及的命令如下：

- 添加商品：`HSET cart:{用户id} {商品id} 1`
- 添加数量：`HINCRBY cart:{用户id} {商品id} 1`
- 商品总数：`HLEN cart:{用户id}`
- 删除商品：`HDEL cart:{用户id} {商品id}`
- 获取购物车所有商品：`HGETALL cart:{用户id}`

当前仅仅是将商品ID存储到了Redis 中，在回显商品具体信息的时候，还需要拿着商品 id 查询一次数据库，获取完整的商品的信息。

#### Set

##### 基本

无序并唯一的键集合，不会按照插入的先后顺序进行存储

一个集合最多可以存储 `2^32-1` 个元素。概念和数学中个的集合基本类似，可以交集，并集，差集等等，所以 Set 类型除了支持集合内的增删改查，同时还支持多个集合取交集、并集、差集

Set 类型和 List 类型的区别如下：

- List 可以存储重复元素，Set 只能存储非重复元素；
- List 是按照元素的先后顺序存储元素的，而 Set 则是无序方式存储元素的

##### 基本操作

```
# 往集合key中存入元素，元素存在则忽略，若key不存在则新建
SADD key member [member ...]
# 从集合key中删除元素
SREM key member [member ...] 
# 获取集合key中所有元素
SMEMBERS key
# 获取集合key中的元素个数
SCARD key

# 判断member元素是否存在于集合key中
SISMEMBER key member

# 从集合key中随机选出count个元素，元素不从key中删除
SRANDMEMBER key [count]
# 从集合key中随机选出count个元素，元素从key中删除
SPOP key [count]
```

```
# 交集运算
SINTER key [key ...]
# 将交集结果存入新集合destination中
SINTERSTORE destination key [key ...]

# 并集运算
SUNION key [key ...]
# 将并集结果存入新集合destination中
SUNIONSTORE destination key [key ...]

# 差集运算
SDIFF key [key ...]
# 将差集结果存入新集合destination中
SDIFFSTORE destination key [key ...]
```

##### 应用场景

数据去重和保障数据的唯一性，还可以用来统计多个集合的交集、错集和并集等，当我们存储的数据是无序并且需要去重的情况下，比较适合使用集合类型进行存储

Set 的差集、并集和交集的计算复杂度较高，在数据量较大的情况下，如果直接执行这些计算，会导致 Redis 实例阻塞。

在主从集群中，为了避免主库因为 Set 做聚合计算（交集、差集、并集）时导致主库被阻塞，我们可以选择一个从库完成聚合统计，或者把数据返回给客户端，由客户端来完成聚合统计

###### 点赞

Set 类型可以保证一个用户只能点一个赞，这里举例子一个场景，key 是文章id，value 是用户id。

`uid:1` 、`uid:2`、`uid:3` 三个用户分别对 article:1 文章点赞了。

```shell
# uid:1 用户对文章 article:1 点赞
> SADD article:1 uid:1
(integer) 1
# uid:2 用户对文章 article:1 点赞
> SADD article:1 uid:2
(integer) 1
# uid:3 用户对文章 article:1 点赞
> SADD article:1 uid:3
(integer) 1
```

`uid:1` 取消了对 article:1 文章点赞。

```text
> SREM article:1 uid:1
(integer) 1
```

获取 article:1 文章所有点赞用户 :

```shell
> SMEMBERS article:1
1) "uid:3"
2) "uid:2"
```

获取 article:1 文章的点赞用户数量：

```shell
> SCARD article:1
(integer) 2
```

判断用户 `uid:1` 是否对文章 article:1 点赞了：

```shell
> SISMEMBER article:1 uid:1
(integer) 0  # 返回0说明没点赞，返回1则说明点赞了
```

###### 共同好友

Set 类型支持交集运算，所以可以用来计算共同关注的好友、公众号等。

key 可以是用户id，value 则是已关注的公众号的id。

`uid:1` 用户关注公众号 id 为 5、6、7、8、9，`uid:2` 用户关注公众号 id 为 7、8、9、10、11。

```shell
# uid:1 用户关注公众号 id 为 5、6、7、8、9
> SADD uid:1 5 6 7 8 9
(integer) 5
# uid:2  用户关注公众号 id 为 7、8、9、10、11
> SADD uid:2 7 8 9 10 11
(integer) 5
```

`uid:1` 和 `uid:2` 共同关注的公众号：

```shell
# 获取共同关注
> SINTER uid:1 uid:2
1) "7"
2) "8"
3) "9"
```

给 `uid:2` 推荐 `uid:1` 关注的公众号：

```shell
> SDIFF uid:1 uid:2
1) "5"
2) "6"
```

验证某个公众号是否同时被 `uid:1` 或 `uid:2` 关注:

```shell
> SISMEMBER uid:1 5
(integer) 1 # 返回0，说明关注了
> SISMEMBER uid:2 5
(integer) 0 # 返回0，说明没关注
```

###### 抽奖

存储某活动中中奖的用户名 ，Set 类型因为有去重功能，可以保证同一个用户不会中奖两次。

key为抽奖活动名，value为员工名称，把所有员工名称放入抽奖箱 ：

```shell
>SADD lucky Tom Jerry John Sean Marry Lindy Sary Mark
(integer) 5
```

如果允许重复中奖，可以使用 SRANDMEMBER 命令。

```shell
# 抽取 1 个一等奖：
> SRANDMEMBER lucky 1
1) "Tom"
# 抽取 2 个二等奖：
> SRANDMEMBER lucky 2
1) "Mark"
2) "Jerry"
# 抽取 3 个三等奖：
> SRANDMEMBER lucky 3
1) "Sary"
2) "Tom"
3) "Jerry"
```

如果不允许重复中奖，可以使用 SPOP 命令。

```shell
# 抽取一等奖1个
> SPOP lucky 1
1) "Sary"
# 抽取二等奖2个
> SPOP lucky 2
1) "Jerry"
2) "Mark"
# 抽取三等奖3个
> SPOP lucky 3
1) "John"
2) "Sean"
3) "Lindy"
```

#### Zset

##### 概念

Zset 类型（Sorted Set，有序集合）

每个存储元素相当于有两个值组成的，一个是有序集合的元素值，一个是排序值。

有序集合保留了集合不能有重复成员的特性（分值可以重复）

##### 常用命令

```shell
# 往有序集合key中加入带分值元素
ZADD key score member [[score member]...]   
# 往有序集合key中删除元素
ZREM key member [member...]                 
# 返回有序集合key中元素member的分值
ZSCORE key member
# 返回有序集合key中元素个数
ZCARD key 

# 为有序集合key中元素member的分值加上increment
ZINCRBY key increment member 

# 正序获取有序集合key从start下标到stop下标的元素
ZRANGE key start stop [WITHSCORES]
# 倒序获取有序集合key从start下标到stop下标的元素
ZREVRANGE key start stop [WITHSCORES]

# 返回有序集合中指定分数区间内的成员，分数由低到高排序。
ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]

# 返回指定成员区间内的成员，按字典正序排列, 分数必须相同。
ZRANGEBYLEX key min max [LIMIT offset count]
# 返回指定成员区间内的成员，按字典倒序排列, 分数必须相同
ZREVRANGEBYLEX key max min [LIMIT offset count]
```

Zset 运算操作（相比于 Set 类型，ZSet 类型没有支持差集运算）：

```shell
# 并集计算(相同元素分值相加)，numberkeys一共多少个key，WEIGHTS每个key对应的分值乘积
ZUNIONSTORE destkey numberkeys key [key...] 
# 交集计算(相同元素分值相加)，numberkeys一共多少个key，WEIGHTS每个key对应的分值乘积
ZINTERSTORE destkey numberkeys key [key...]
```

##### 应用场景

###### 排行榜

例如学生成绩的排名榜、游戏积分排行榜、视频播放排名、电商系统中商品的销量排名等。

我们以博文点赞排名为例，小林发表了五篇博文，分别获得赞为 200、40、100、50、150。

```shell
# arcticle:1 文章获得了200个赞
> ZADD user:xiaolin:ranking 200 arcticle:1
(integer) 1
# arcticle:2 文章获得了40个赞
> ZADD user:xiaolin:ranking 40 arcticle:2
(integer) 1
# arcticle:3 文章获得了100个赞
> ZADD user:xiaolin:ranking 100 arcticle:3
(integer) 1
# arcticle:4 文章获得了50个赞
> ZADD user:xiaolin:ranking 50 arcticle:4
(integer) 1
# arcticle:5 文章获得了150个赞
> ZADD user:xiaolin:ranking 150 arcticle:5
(integer) 1
```

文章 arcticle:4 新增一个赞，可以使用 ZINCRBY 命令（为有序集合key中元素member的分值加上increment）：

```shell
> ZINCRBY user:xiaolin:ranking 1 arcticle:4
"51"
```

查看某篇文章的赞数，可以使用 ZSCORE 命令（返回有序集合key中元素个数）：

```shell
> ZSCORE user:xiaolin:ranking arcticle:4
"50"
```

获取小林文章赞数最多的 3 篇文章，可以使用 ZREVRANGE 命令（倒序获取有序集合 key 从start下标到stop下标的元素）：

```shell
# WITHSCORES 表示把 score 也显示出来
> ZREVRANGE user:xiaolin:ranking 0 2 WITHSCORES
1) "arcticle:1"
2) "200"
3) "arcticle:5"
4) "150"
5) "arcticle:3"
6) "100"
```

获取小林 100 赞到 200 赞的文章，可以使用 ZRANGEBYSCORE 命令（返回有序集合中指定分数区间内的成员，分数由低到高排序）：

```shell
> ZRANGEBYSCORE user:xiaolin:ranking 100 200 WITHSCORES
1) "arcticle:3"
2) "100"
3) "arcticle:5"
4) "150"
5) "arcticle:1"
6) "200"
```

###### 名册排序

数值（电话/数字字符串）或字符串的查询：

将电话号码存储到 SortSet 中，然后根据需要来获取号段：

```shell
> ZADD phone 0 13100111100 0 13110114300 0 13132110901 
(integer) 3
> ZADD phone 0 13200111100 0 13210414300 0 13252110901 
(integer) 3
> ZADD phone 0 13300111100 0 13310414300 0 13352110901 
(integer) 3
```

获取所有号码:

```shell
> ZRANGEBYLEX phone - +
1) "13100111100"
2) "13110114300"
3) "13132110901"
4) "13200111100"
5) "13210414300"
6) "13252110901"
7) "13300111100"
8) "13310414300"
9) "13352110901"
```

获取 132 号段的号码：

```shell
> ZRANGEBYLEX phone [132 (133
1) "13200111100"
2) "13210414300"
3) "13252110901"
```

获取132、133号段的号码：

```shell
> ZRANGEBYLEX phone [132 (134
1) "13200111100"
2) "13210414300"
3) "13252110901"
4) "13300111100"
5) "13310414300"
6) "13352110901"
```

*2、姓名排序*

```shell
> zadd names 0 Toumas 0 Jake 0 Bluetuo 0 Gaodeng 0 Aimini 0 Aidehua 
(integer) 6
```

获取所有人的名字:

```shell
> ZRANGEBYLEX names - +
1) "Aidehua"
2) "Aimini"
3) "Bluetuo"
4) "Gaodeng"
5) "Jake"
6) "Toumas"
```

获取名字中大写字母A开头的所有人：

```shell
> ZRANGEBYLEX names [A (B
1) "Aidehua"
2) "Aimini"
```

获取名字中大写字母 C 到 Z 的所有人：

```shell
> ZRANGEBYLEX names [C [Z
1) "Gaodeng"
2) "Jake"
3) "Toumas"
```

#### BitMap

##### 介绍

位图，是一串连续的二进制数组（0和1），可以通过偏移量（offset）定位元素。BitMap通过最小的单位bit来进行`0|1`的设置，表示某个元素的值或者状态

非常节省空间，特别适合一些数据量大且使用二值统计的场景

##### 常用命令

bitmap 基本操作：

```shell
# 设置值，其中value只能是 0 和 1
SETBIT key offset value

# 获取值
GETBIT key offset

# 获取指定范围内值为 1 的个数
# start 和 end 以字节为单位
BITCOUNT key start end
```

bitmap 运算操作：

```shell
# BitMap间的运算
# operations 位移操作符，枚举值
  AND 与运算 &
  OR 或运算 |
  XOR 异或 ^
  NOT 取反 ~
# result 计算的结果，会存储在该key中
# key1 … keyn 参与运算的key，可以有多个，空格分割，not运算只能一个key
# 当 BITOP 处理不同长度的字符串时，较短的那个字符串所缺少的部分会被看作 0。返回值是保存到 destkey 的字符串的长度（以字节byte为单位），和输入 key 中最长的字符串长度相等。
BITOP [operations] [result] [key1] [keyn…]

# 返回指定key中第一次出现指定value(0/1)的位置
BITPOS [key] [value]
```

##### 应用场景

###### 签到统计

第一步，执行下面的命令，记录该用户 6 月 3 号已签到。

```shell
SETBIT uid:sign:100:202206 2 1
```

第二步，检查该用户 6 月 3 日是否签到。

```shell
GETBIT uid:sign:100:202206 2 
```

第三步，统计该用户在 6 月份的签到次数。

```shell
BITCOUNT uid:sign:100:202206
```

Redis 提供了 `BITPOS key bitValue [start] [end]`指令，返回数据表示 Bitmap 中第一个值为 `bitValue` 的 offset 位置。

在默认情况下， 命令将检测整个位图， 用户可以通过可选的 `start` 参数和 `end` 参数指定要检测的范围。所以我们可以通过执行这条命令来获取 userID = 100 在 2022 年 6 月份首次打卡日期：

```text
BITPOS uid:sign:100:202206 1
```

需要注意的是，因为 offset 从 0 开始的，所以我们需要将返回的 value + 1

###### 登录

只需要一个 key = login_status 表示存储用户登陆状态集合数据， 将用户 ID 作为 offset，在线就设置为 1，下线设置 0。通过 `GETBIT`判断对应的用户是否在线。 5000 万用户只需要 6 MB 的空间。

假如我们要判断 ID = 10086 的用户的登陆情况：

第一步，执行以下指令，表示用户已登录。

```shell
SETBIT login_status 10086 1
```

第二步，检查该用户是否登陆，返回值 1 表示已登录。

```text
GETBIT login_status 10086
```

第三步，登出，将 offset 对应的 value 设置成 0。

```shell
SETBIT login_status 10086 0
```

###### 连续签到人数

我们把每天的日期作为 Bitmap 的 key，userId 作为 offset，若是打卡则将 offset 位置的 bit 设置成 1。

key 对应的集合的每个 bit 位的数据则是一个用户在该日期的打卡记录。

一共有 7 个这样的 Bitmap，如果我们能对这 7 个 Bitmap 的对应的 bit 位做 『与』运算。同样的 UserID offset 都是一样的，当一个 userID 在 7 个 Bitmap 对应对应的 offset 位置的 bit = 1 就说明该用户 7 天连续打卡。

结果保存到一个新 Bitmap 中，我们再通过 `BITCOUNT` 统计 bit = 1 的个数便得到了连续打卡 7 天的用户总数了。

Redis 提供了 `BITOP operation destkey key [key ...]`这个指令用于对一个或者多个 key 的 Bitmap 进行位元操作。

- `operation` 可以是 `and`、`OR`、`NOT`、`XOR`。当 BITOP 处理不同长度的字符串时，较短的那个字符串所缺少的部分会被看作 `0` 。空的 `key` 也被看作是包含 `0` 的字符串序列。

假设要统计 3 天连续打卡的用户数，则是将三个 bitmap 进行 AND 操作，并将结果保存到 destmap 中，接着对 destmap 执行 BITCOUNT 统计，如下命令：

```shell
# 与操作
BITOP AND destmap bitmap:01 bitmap:02 bitmap:03
# 统计 bit 位 =  1 的个数
BITCOUNT destmap
```

即使一天产生一个亿的数据，Bitmap 占用的内存也不大，大约占 12 MB 的内存（10^8/8/1024/1024），7 天的 Bitmap 的内存开销约为 84 MB。同时我们最好给 Bitmap 设置过期时间，让 Redis 删除过期的打卡数据，节省内存

#### HyperLogLog

##### 介绍

Redis 2.8.9 版本新增的数据类型，是一种用于统计基数的数据集合类型，基数统计就是指统计一个集合中不重复的元素个数。HyperLogLog 是统计规则是基于概率完成的，不是非常准确，标准误算率是 0.81% [原理参见](https://lr580.github.io/2024/06/13/HyperLogLog/)

优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的内存空间总是固定的、并且是很小的。每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 `2^64` 个不同元素的基数

##### 常用命令

```shell
# 添加指定元素到 HyperLogLog 中
PFADD key element [element ...]

# 返回给定 HyperLogLog 的基数估算值。
PFCOUNT key [key ...]

# 将多个 HyperLogLog 合并为一个 HyperLogLog
PFMERGE destkey sourcekey [sourcekey ...]
```

##### 应用场景

###### UV计数

unique visitor 计数 指访问某个站点或点击某个网页的不同IP地址的人数

#### GEO

##### 介绍

Redis 3.2 版本新增的数据类型，主要用于存储地理位置信息，并对存储的信息进行操作

基于位置信息服务（Location-Based Service，LBS）的应用。LBS 应用访问的数据是和人或物关联的一组经纬度信息，而且要能查询相邻的经纬度范围

##### 常用命令

```shell
# 存储指定的地理空间位置，可以将一个或多个经度(longitude)、纬度(latitude)、位置名称(member)添加到指定的 key 中。
GEOADD key longitude latitude member [longitude latitude member ...]

# 从给定的 key 里返回所有指定名称(member)的位置（经度和纬度），不存在的返回 nil。
GEOPOS key member [member ...]

# 返回两个给定位置之间的距离。
GEODIST key member1 member2 [m|km|ft|mi]

# 根据用户给定的经纬度坐标来获取指定范围内的地理位置集合。
GEORADIUS key longitude latitude radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key]
```

##### 应用场景

###### 打车

假设车辆 ID 是 33，经纬度位置是（116.034579，39.030452），我们可以用一个 GEO 集合保存所有车辆的经纬度，集合 key 是 cars:locations。

执行下面的这个命令，就可以把 ID 号为 33 的车辆的当前经纬度位置存入 GEO 集合中：

```shell
GEOADD cars:locations 116.034579 39.030452 33
```

当用户想要寻找自己附近的网约车时，LBS 应用就可以使用 GEORADIUS 命令。

例如，LBS 应用执行下面的命令时，Redis 会根据输入的用户的经纬度信息（116.054579，39.030452 ），查找以这个经纬度为中心的 5 公里内的车辆信息，并返回给 LBS 应用。

```shell
GEORADIUS cars:locations 116.054579 39.030452 5 km ASC COUNT 10
```

#### Stream

##### 介绍

Redis 5.0 版本新增加的数据类型，Redis 专门为消息队列设计的数据类型

之前模式的缺陷：

- 发布订阅模式，不能持久化也就无法可靠的保存消息，并且对于离线重连的客户端不能读取历史消息的缺陷；
- List 实现消息队列的方式不能重复消费，一个消息消费完就会被删除，而且生产者需要自行实现全局唯一 ID。

支持消息的持久化、支持自动生成全局唯一 ID、支持 ack 确认消息的模式、支持消费组模式等

##### 常用命令

Stream 消息队列操作命令：

- XADD：插入消息，保证有序，可以自动生成全局唯一 ID；
- XLEN ：查询消息长度；
- XREAD：用于读取消息，可以按 ID 读取数据；
- XDEL ： 根据消息 ID 删除消息；
- DEL ：删除整个 Stream；
- XRANGE ：读取区间消息
- XREADGROUP：按消费组形式读取消息；
- XPENDING 和 XACK：
  - XPENDING 命令可以用来查询每个消费组内所有消费者已读取、但尚未确认的消息；
  - XACK 命令用于向消息队列确认消息处理已完成

##### 应用场景

###### 基操

```
# * 表示让 Redis 为插入的数据自动生成一个全局唯一的 ID
# 往名称为 mymq 的消息队列中插入一条消息，消息的键是 name，值是 xiaolin
> XADD mymq * name xiaolin
"1654254953808-0"
```

插入成功后会返回全局唯一的 ID："1654254953808-0"。消息的全局唯一 ID 由两部分组成：

- 第一部分“1654254953808”是数据插入时，以毫秒为单位计算的当前服务器时间；
- 第二部分表示插入消息在当前毫秒内的消息序号，这是从 0 开始编号的。例如，“1654254953808-0”就表示在“1654254953808”毫秒内的第 1 条消息

消费者通过 XREAD 命令从消息队列中读取消息时，可以指定一个消息 ID，并从这个消息 ID 的下一条消息开始进行读取（注意是输入消息 ID 的下一条信息开始读取，不是查询输入ID的消息）。

```shell
# 从 ID 号为 1654254953807-0 的消息开始，读取后续的所有消息（示例中一共 1 条）。
> XREAD STREAMS mymq 1654254953807-0
1) 1) "mymq"
   2) 1) 1) "1654254953808-0"
         2) 1) "name"
            2) "xiaolin"
```

如果想要实现阻塞读（当没有数据时，阻塞住），可以调用 XRAED 时设定 BLOCK 配置项，实现类似于 BRPOP 的阻塞读取操作。

下例，XREAD 在读取最新消息时，如果没有消息到来，XREAD 将阻塞 10000 毫秒（即 10 秒），然后再返回。

```shell
# 命令最后的“$”符号表示读取最新的消息
> XREAD BLOCK 10000 STREAMS mymq $
(nil)
(10.00s)
```

###### 消费组

创建两个消费组，这两个消费组消费的消息队列是 mymq，都指定从第一条消息开始读取：

```shell
# 创建一个名为 group1 的消费组，0-0 表示从第一条消息开始读取。
> XGROUP CREATE mymq group1 0-0
OK
# 创建一个名为 group2 的消费组，0-0 表示从第一条消息开始读取。
> XGROUP CREATE mymq group2 0-0
OK
```

消费组 group1 内的消费者 consumer1 从 mymq 消息队列中读取所有消息的命令如下：

```shell
# 命令最后的参数“>”，表示从第一条尚未被消费的消息开始读取。
> XREADGROUP GROUP group1 consumer1 STREAMS mymq >
1) 1) "mymq"
   2) 1) 1) "1654254953808-0"
         2) 1) "name"
            2) "xiaolin"
```

消息队列中的消息一旦被消费组里的一个消费者读取了，就不能再被该消费组内的其他消费者读取了，即同一个消费组里的消费者不能消费同一条消息。

比如说，我们执行完刚才的 XREADGROUP 命令后，再执行一次同样的命令，此时读到的就是空值了：

```shell
> XREADGROUP GROUP group1 consumer1 STREAMS mymq >
(nil)
```

但是，不同消费组的消费者可以消费同一条消息（但是有前提条件，创建消息组的时候，不同消费组指定了相同位置开始读取消息）。

比如说，刚才 group1 消费组里的 consumer1 消费者消费了一条 id 为 1654254953808-0 的消息，现在用 group2 消费组里的 consumer1 消费者消费消息：

```shell
> XREADGROUP GROUP group2 consumer1 STREAMS mymq >
1) 1) "mymq"
   2) 1) 1) "1654254953808-0"
         2) 1) "name"
            2) "xiaolin"
```

因为我创建两组的消费组都是从第一条消息开始读取，所以可以看到第二组的消费者依然可以消费 id 为 1654254953808-0 的这一条消息。因此，不同的消费组的消费者可以消费同一条消息。

使用消费组的目的是让组内的多个消费者共同分担读取消息，所以，我们通常会让每个消费者读取部分消息，从而实现消息读取负载在多个消费者间是均衡分布的。如：让 group2 中的 consumer1、2、3 各自读取一条消息

```
# 让 group2 中的 consumer1 从 mymq 消息队列中消费一条消息
> XREADGROUP GROUP group2 consumer1 COUNT 1 STREAMS mymq >
1) 1) "mymq"
   2) 1) 1) "1654254953808-0"
         2) 1) "name"
            2) "xiaolin"
# 让 group2 中的 consumer2 从 mymq 消息队列中消费一条消息
> XREADGROUP GROUP group2 consumer2 COUNT 1 STREAMS mymq >
1) 1) "mymq"
   2) 1) 1) "1654256265584-0"
         2) 1) "name"
            2) "xiaolincoding"
# 让 group2 中的 consumer3 从 mymq 消息队列中消费一条消息
> XREADGROUP GROUP group2 consumer3 COUNT 1 STREAMS mymq >
1) 1) "mymq"
   2) 1) 1) "1654256271337-0"
         2) 1) "name"
            2) "Tom"
```

###### 故障分析

Streams 会自动使用内部队列（也称为 PENDING List）留存消费组里每个消费者读取的消息，直到消费者使用 XACK 命令通知 Streams“消息已经处理完成”。

消费确认增加了消息的可靠性，一般在业务处理完成之后，需要执行 XACK 命令确认消息已经被消费完成

如果消费者没有成功处理消息，它就不会给 Streams 发送 XACK 命令，消息仍然会留存。此时，消费者可以在重启后，用 XPENDING 命令查看已读取、但尚未确认处理完成的消息

```
127.0.0.1:6379> XPENDING mymq group2
1) (integer) 3
2) "1654254953808-0"  # 表示 group2 中所有消费者读取的消息最小 ID
3) "1654256271337-0"  # 表示 group2 中所有消费者读取的消息最大 ID
4) 1) 1) "consumer1"
      2) "1"
   2) 1) "consumer2"
      2) "1"
   3) 1) "consumer3"
      2) "1"
```

如果想查看某个消费者具体读取了哪些数据，可以执行下面的命令：

```shell
# 查看 group2 里 consumer2 已从 mymq 消息队列中读取了哪些消息
> XPENDING mymq group2 - + 10 consumer2
1) 1) "1654256265584-0"
   2) "consumer2"
   3) (integer) 410700
   4) (integer) 1
```

可以看到，consumer2 已读取的消息的 ID 是 1654256265584-0。

一旦消息 1654256265584-0 被 consumer2 处理了，consumer2 就可以使用 XACK 命令通知 Streams，然后这条消息就会被删除

```
> XACK mymq group2 1654256265584-0
(integer) 1
```

##### 优缺点

###### 概述

一个专业的消息队列，必须要做到两大块：

- 消息不丢
- 消息可堆积

总结：

- 如果你的业务场景足够简单，对于数据丢失不敏感，而且消息积压概率比较小的情况下，把 Redis 当作队列是完全可以的。
- 如果你的业务有海量消息，消息积压的概率比较大，并且不能接受数据丢失，那么不能用

###### 消息丢失

使用一个消息队列，其实就分为三大块：生产者、队列中间件、消费者，所以要保证消息就是保证三个环节都不能丢失数据

- Redis 生产者会不会丢消息，取决于生产者对于异常情况的处理是否合理。 从消息被生产出来，然后提交给 MQ 的过程中，只要能正常收到 （ MQ 中间件） 的 ack 确认响应，就表示发送成功，所以只要处理好返回值和异常，如果返回异常则进行消息重发，那么这个阶段是不会出现消息丢失的。

- Redis 消费者不会丢消息。因为 Stream （ MQ 中间件）会自动使用内部队列（也称为 PENDING List）留存消费组里每个消费者读取的消息，但是未被确认的消息。消费者可以在重启后，用 XPENDING 命令查看已读取、但尚未确认处理完成的消息。等到消费者执行完业务逻辑后，再发送消费确认 XACK 命令，也能保证消息的不丢失。

- Redis 消息中间件会会消消息

  Redis 在以下 2 个场景下，都会导致数据丢失：

  - AOF 持久化配置为每秒写盘，但这个写盘过程是异步的，Redis 宕机时会存在数据丢失的可能
  - 主从复制也是异步的，主从切换时，也存在丢失数据的可能

可以看到，Redis 在队列中间件环节无法保证消息不丢。像 RabbitMQ 或 Kafka 这类专业的队列中间件，在使用时是部署一个集群，生产者在发布消息时，队列中间件通常会写多个节点，也就是有多个副本，这样一来，即便其中一个节点挂了，也能保证集群的数据不丢失

###### 消息堆积

Redis 的数据都存储在内存中，这就意味着一旦发生消息积压，则会导致 Redis 的内存持续增长，如果超过机器内存上限，就会面临被 OOM 的风险。

所以 Redis 的 Stream 提供了可以指定队列最大长度的功能，就是为了避免这种情况发生。

当指定队列最大长度时，队列长度超过上限后，旧消息会被删除，只保留固定长度的新消息。这么来看，Stream 在消息积压时，如果指定了最大长度，还是有可能丢失消息的。

但 Kafka、RabbitMQ 专业的消息队列它们的数据都是存储在磁盘上，当消息积压时，无非就是多占用一些磁盘空间

###### 发布订阅机制缺点

> 发布者：发布者向一个频道（channel）发送消息。消息可以被任何订阅了该频道的订阅者接收。
>
> 订阅者：订阅者订阅一个或多个频道，并监听来自这些频道的消息。当任何消息在这些频道上发布时，所有订阅了这些频道的订阅者都会接收到这些消息。
>
> 频道：频道是消息传递的媒介，发布者将消息发布到指定的频道，订阅者从频道接收消息
>
> - SUBSCRIBE channel [channel ...]：订阅一个或多个频道。
> - PUBLISH channel message：向指定频道发布消息。
> - UNSUBSCRIBE [channel [channel ...]]：取消订阅一个或多个频道。
> - PSUBSCRIBE pattern [pattern ...]：按模式订阅频道，模式可以使用通配符，如 `news.*` 表示订阅所有以 `news.` 开头的频道。
> - PUNSUBSCRIBE [pattern [pattern ...]]：取消按模式订阅的频道。

发布订阅机制存在以下缺点，都是跟丢失数据有关：

1. 发布/订阅机制没有基于任何数据类型实现，所以不具备数据持久化的能力，也就是发布/订阅机制的相关操作，不会写入到 RDB 和 AOF 中，当 Redis 宕机重启，发布/订阅机制的数据也会全部丢失。
2. 发布订阅模式是“发后既忘”的工作模式，如果有订阅者离线重连之后不能消费之前的历史消息。
3. 当消费端有一定的消息积压时，也就是生产者发送的消息，消费者消费不过来时，如果超过 32M 或者是 60s 内持续保持在 8M 以上，消费端会被强行断开，这个参数是在配置文件中设置的，默认值是 `client-output-buffer-limit pubsub 32mb 8mb 60`。

所以，发布/订阅机制只适合即时通讯的场景，比如构建哨兵集群的场景采用了发布/订阅机制

#### 实现

[参考](https://www.xiaolincoding.com/redis/base/redis_interview.html#%E4%BA%94%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84-redis-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0) [参考2](https://www.xiaolincoding.com/redis/data_struct/command.html#string)

##### 概述

<img src="img/9fa26a74965efbf0f56b707a03bb9b7f.png" alt="img" style="zoom: 44%;" />

Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了

##### string

SDS（简单动态字符串）

- 可以保存文本/二进制

  使用 len 属性的值而不是空字符来判断字符串是否结束，以处理二进制的方式来处理 SDS 存放在 buf[] 数组里的数据。二进制所以可以处理图片等多媒体二进制数据。

- O(1) 获取字符串长度

  直接用 len 属性

- 拼接字符串不会缓冲区溢出

  拼接字符串之前会检查 SDS 空间是否满足要求，如果空间不够会自动扩容

String 类型的底层的数据结构实现主要是 int 和 SDS（简单动态字符串）

内部编码（encoding）有 3 种 ：int、raw和 embstr

如果一个字符串对象保存的是整数值，并且这个整数值可以用`long`类型来表示，那么字符串对象会将整数值保存在字符串对象结构的`ptr`属性里面（将`void*`转换成 long），并将字符串对象的编码设置为`int`

<img src="img/int.png" alt="img" style="zoom:33%;" />

如果字符串对象保存的是一个字符串，并且这个字符申的长度小于等于 32 字节（redis 2.+版本），那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串，并将对象的编码设置为`embstr`， `embstr`编码是专门用于保存短字符串的一种优化编码方式

如果字符串对象保存的是一个字符串，并且这个字符串的长度大于 32 字节（redis 2.+版本），那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串，并将对象的编码设置为`raw`

embstr 编码和 raw 编码的边界在 redis 不同版本中是不一样的：

- redis 2.+ 是 32 字节
- redis 3.0-4.0 是 39 字节
- redis 5.0 是 44 字节

```
embstr`和`raw`编码都会使用`SDS`来保存值，但不同之处在于`embstr`会通过一次内存分配函数来分配一块连续的内存空间来保存`redisObject`和`SDS`，而`raw`编码会通过调用两次内存分配函数来分别分配两块空间来保存`redisObject`和`SDS
```

##### list

双向链表或压缩列表

- 如果列表的元素个数小于 512 个（默认值，可由 list-max-ziplist-entries 配置），列表每个元素的值都小于 64 字节（默认值，可由 list-max-ziplist-value 配置），Redis 会使用压缩列表作为 List 类型的底层数据结构
- 如果列表的元素不满足上面的条件，Redis 会使用双向链表作为 List 类型的底层数据结构

在 Redis 3.2 版本之后，List 数据类型底层数据结构就只由 quicklist 实现了，替代了双向链表和压缩列表

embstr：

- `embstr`编码将创建字符串对象所需的内存分配次数从 `raw` 编码的两次降低为一次；
- 释放 `embstr`编码的字符串对象同样只需要调用一次内存释放函数；
- 因为`embstr`编码的字符串对象的所有数据都保存在一块连续的内存里面可以更好的利用 CPU 缓存提升性能
- 如果字符串的长度增加需要重新分配内存时，整个redisObject和sds都需要重新分配空间，所以embstr编码的字符串对象实际上是只读的，redis没有为embstr编码的字符串对象编写任何相应的修改程序。当我们对embstr编码的字符串对象执行任何修改命令（例如append）时，程序会先将对象的编码从embstr转换成raw，然后再执行修改命令

##### hash

压缩列表或哈希表

- 如果哈希类型元素个数小于 512 个（默认值，可由 hash-max-ziplist-entries 配置），所有值小于 64 字节（默认值，可由 hash-max-ziplist-value 配置）的话，Redis 会使用压缩列表作为 Hash 类型的底层数据结构；
- 如果哈希类型元素不满足上面条件，Redis 会使用哈希表作为 Hash 类型的底层数据结构

##### set

哈希表或整数集合

- 如果集合中的元素都是整数且元素个数小于 512 （默认值，set-maxintset-entries配置）个，Redis 会使用整数集合作为 Set 类型的底层数据结构；
- 如果集合中的元素不满足上面条件，则 Redis 使用哈希表作为 Set 类型的底层数据结构

##### zset

压缩列表或跳表实现的：

- 如果有序集合的元素个数小于 128 个，并且每个元素的值小于 64 字节时，Redis 会使用压缩列表作为 Zset 类型的底层数据结构；
- 如果有序集合的元素不满足上面的条件，Redis 会使用跳表作为 Zset 类型的底层数据结构

##### bitmap

String 类型作为底层数据结构实现的一种统计二值状态的数据类型。

String 类型是会保存为二进制的字节数组

##### hyperloglog

[参考](https://lr580.github.io/2024/06/13/HyperLogLog/)

###### 概率原理

近似快速计算一个集合有多少个不重元素。

设元素的哈希值有 $L$ 位，哈希值第一个 $1$ 出现的位置记作 $\rho(a)$，假设一定有 $1$ 出现，则所有哈希值的最高出现位置设为 $\rho_\max$，则可以近似估计不重元素数为 $n=2^{\rho_\max}$。

第一次出现的位置是第 $k$ 位，其概率等价于抛硬币伯努利过程，即 $P=\dfrac1{2^k}$，也就是说出现位置大于等于 $k$ 概率为 $\dfrac1{2^k}$，所以小于 $k$ 的概率是 $1-\dfrac1{2^k}$，所以 $n$ 个哈希值第一次出现 $1$ 的位都小于 $k$ 的概率为 $P_n(X <  k)=(1-\dfrac1{2^k})^n$。都大于等于 $k$ 的概率为 $P_n(X\ge k)=\dfrac1{2^{kn}}$

则 $n$ 个哈希值至少有一个的位等于 $k$ 的概率是：上面的概率减去每次都不超过 $k-1$ 的概率，即：$P_n(X = k)=(1-\dfrac1{2^k})^n-(1-\dfrac1{2^{k-1}})^n$。

当 $n\ll 2^k$ 时，$P_n(X\ge k)\to 0$，而 $n\gg 2^k$ 时 $P_n(X < k)\to 0$。也就是说，哈希值个数 $n$ 远小于 $2^k$ 时，至少一个哈希值在 $k$ 位和之后出现 $1$ 几乎不可能；$n$ 远大于 $2^k$ 时，也几乎不可能在 $k$ 位之前都没出现过 $1$。与直观经验吻合。

设集合基数为 $n$，则 $2^{\rho_\max}$ 应当约等于 $n$，$\rho_\max$ 偏离很大的概率都约等于零。

也就是说，未知基数 $n$，可以通过观测最大的 $1$ 的位置 $\rho_\max$ 来预估 $n\approx 2^{\rho_\max}$。

> #### 平均数
>
> 后面 hyper log log 会用到。序列 $a$ 长度为 $n$：
>
> - Arithmetic Mean 算术平均 $\dfrac 1n\sum_{i=1}^n a_i$
>
>   对极端值敏感，极大或极小的值会对平均结果产生较大影响
>
> - Geometric Mean 几何平均 $\sqrt[n]{\prod_{i=1}^n a_i}$
>
>   广泛用于处理比率或增长率数据，如金融投资的年化回报率
>
>   当数据集包含的数值跨越多个量级时，几何平均数能有效平衡各数值的影响，避免极端值对结果的过大影响
>
> - Harmonic Mean 调和平均 $\dfrac n{\sum\dfrac1{a_i}}$
>
>   在处理速度、效率或其他倒数形式的数据时，调和平均数提供更为合适的平均值
>
>   合并不同大小的样本时计算平均值；对数据集中的小值较为敏感



###### LogLogCounting

LLC [论文 Loglog Counting of Large Cardinalities](http://redisgate.kr/download/DuFl03-LNCS.pdf)

定性地设计一个尽可能满足下面条件的哈希函数：

1. 均匀（哈希结果理想上要均匀分布，但已经证明不可能通过一个哈希函数将一组不服从均匀分布的数据映射为绝对均匀分布，但是很多哈希函数可以生成几乎服从均匀分布的结果）(D. Knuth)
2. 哈希碰撞概率忽略不计
3. 哈希结果长度固定

将哈希值分成 $m=2^k$ 个桶，哈希值高 $k$ 位作为桶编号 $\in [0,m)$，剩下的位里找到第一个 $1$ 的位置。设桶 $i$ 的最大的位置是 $M_i$。

根据论文描述，原本是估计 $2^k$，但是不同 $k$ 差别太大了，所以用一个 $k$ 的平均值来代替，即取 $m$ 个桶的平均 $\rho_\max$ 即 $\dfrac 1m\sum_{i=1}^mM_i$ 来近似 $k$。则这几个桶的平均值来计数来估算 $n$：$\hat n=2^{\frac1m\sum_{i=o}^{m-1}M_i}$。

该表达式可以认为是幂 $\rho_\max$ 用了算术平均数，也可以认为是所有 $2^{M_i}$ 的几何平均数。

设哈希值长度为 $16$ 位，设 $m=2^5=32$，若有哈希值为：`0001001010001010`，则 `00010` 是桶编号剩下部分的第一个 $1$ 的位数为 $2$，所以该元素存桶就是把 $2$ 存进去。如果该桶只有一个元素，则 $M_2=2$。

该估计是有偏估计，转化为无偏估计：(详见论文)
$$
E=\alpha_mm2^{\frac1m\sum_{i=0}^{m-1}M_i}
$$
其中 $\alpha_m=\left(\Gamma(-\dfrac1m)\dfrac{1-2^{\frac 1m}}{\log 2}\right)^n,\Gamma(s)=\dfrac1s\int_0^{\infty}e^{-t}t^sdt$

论文给出结论，该观测值与实际值的标准误差为：$StdError(\dfrac{\hat n}n)\approx\dfrac{1.30}{\sqrt m}$

根据误差分析，如果想要误差在 $\epsilon$ 内，则 $m > (\dfrac{1.30}\epsilon)^2$

例如，最多可能有一亿类 $2^{27}$，设 $m=2^{10}$，则剩下 $27-10=17$ 位，也就是说 $M_i$ 的取值在 $[0,17]$，需要 $\lceil\log_2 17\rceil=5$ 位来存每个桶，即共 $5m$ 位，$5\times 1024\div 8=640 Byte$，只需要 $1KB$  不到就可以近似存储一亿个类别。

因此，其空间复杂度为 $O(m\lceil\log_2((\log_2 k)-m)\rceil)$，其中 $m$ 是常数，$k$ 是值域，故复杂度为 $O(\log\log k)$。

###### HyperLogLog

HLLC [论文 HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm](https://dmtcs.episciences.org/3545/pdf) [在线演示 ](http://content.research.neustar.biz/blog/hll.html) [redis源码](https://github.com/redis/redis/blob/unstable/src/hyperloglog.c) [源码解析](https://zhuanlan.zhihu.com/p/681313778) (如哈希函数的设计可以参考源码)

改进一：调和平均数代替几何平均数。几何平均数对离群值（例如这里的 0）特别敏感，因此当存在离群值时，LLC 的偏差就会很大。n 较小时，可能存在较多空桶，而这些特殊的离群值强烈干扰了几何平均数的稳定性。

调和平均为 $\dfrac{m}{\sum_{i=0}^{m-1}\frac1{2^{M_i}}}$因此，估算公式变成了：$\hat n=\dfrac{\alpha_m m^2}{\sum_{i=0}^{m-1}2^{-M_i}}$。

其中，$\alpha_m=\left(m\int_0^m\log_2^m\dfrac{2+u}{1+u}du\right)^{-1}$

具体而言：(参考 vldbss-learned-cost-estimation-guoliangli，出处是 vldb2022的ss)
$$
\alpha_m=\begin{cases}
0.673,&m=16\\
0.698,&m=32\\
0.709,&m=64\\
\dfrac{0.7213}{1+\dfrac{1.079}m},&m\ge128
\end{cases}
$$
是无偏估计，渐近标准差为 $SE_{hllc}(\dfrac{\hat n}n)=\dfrac{1.04}{\sqrt m}$

> 分段修正建议：设估计值为 $E$
>
> - $E\le\dfrac52m$ 时，LC 估计(Linear Counting，线性复杂度，类似 bitmap) [参考](https://zhuanlan.zhihu.com/p/77289303)
> - $\dfrac52m < E\le\dfrac1{30}2^{32}$ 时，HLLC 估计
> - $E > \dfrac1{30}2^{32}$ 时，$\hat n=-2^{32}\log(1-\dfrac E{2^{32}})$
>
> 基数估计的好处：可以并行化。因为可以很容易地合并结果。

> 横向对比其他模型：（参考 vldbss-learned-cost-estimation-guoliangli）
>
> - FM, PCSA, AMS, BJKST, LogLog, SuperLogLog, HyperLogLog, HyperLogLog++, MinCount, AKMV, LC, BF
> - 思想包括：计算尾 1/前导 0 / k-th 最小值 / 线性同义词(linear synoposes)
> - 方法包括：基于区间、基于桶、对数哈希、顺序统计、位统计



C++ 个人实验实现：

- 实践表明：对小元素个数或小取值范围(没做哈希)，误差很大
- 如果取值范围很大(即假设进行了良好哈希)，且 n 较大(如 1e6 或以上)，准确率接近 1.04 之内
- 桶数多少与元素个数、取值范围有关，较小时(小于 1e5)干涉较大，较大时越多越准确

```c++
#include <bits/stdc++.h>
using namespace std;
using db = double;
struct HyperLogLog {
    int k, m; // 选择低 k 位作为桶编号，桶数为 m
    db a; // 修正因子
    vector<int> M; // 桶，没有做进一步位域空间优化/常数优化
    HyperLogLog(int k) : k(k), m(1<<k) {
        a = 0.7213 / (1 + 1.079 / m);
        M.resize(m, 0);
    }
    void insert(int x) {
        int i = x & ((1<<k)-1);
        M[i] = max(M[i], __builtin_ffs(x>>k));
    }
    int size() {
        db ans = a * m * m, s = 0;
        for(int i=0;i<m;++i) 
            s += 1.0 / (1LL<<M[i]);
        return round(ans / s);
    }
};

int main() {
    mt19937 mt;
    uniform_int_distribution<int> dist(1, 1e5); // 任意调整
    set<int> s;
    int n=2e6; // 任意调整
    HyperLogLog h(min(int(log2(n)),16)); // 任意调整
    for(int i=0;i<n;++i) {
        int x = dist(mt);
        h.insert(x);
        s.insert(x);
    }
    cout << s.size() << ' ' << h.size() << ' ' << (1.*h.size()/s.size()) << '\n'; // 任意调整
    return 0;
}
```



###### HyperLogLog++

[论文 Hyperloglog in practice: Algorithmic engineering of a state of the art cardinality estimation algorithm](http://redisgate.kr/download/HyperLogLog-in-Practice.pdf) [论文解读参考](https://zhuanlan.zhihu.com/p/632565939)

改进：

- 将 32 位哈希函数优化为了 64 位哈希函数

- 将偏差纠正算法作修改，从 HllOrig 修改为 HllNoBias

  新算法不管估算数据集大小有多大，都使用同样的内存空间(桶数目)

##### GEO

直接使用了 Sorted Set 集合类型。

GEO 类型使用 GeoHash 编码方法实现了经纬度到 Sorted Set 中元素权重分数的转换，这其中的两个关键机制就是对二维地图做区间划分和对区间进行编码」。一组经纬度落在某个区间后，就用区间的编码值来表示，并把编码值作为 Sorted Set 元素的权重分数。

这样一来，我们就可以把经纬度保存到 Sorted Set 中，利用 Sorted Set 提供的“按权重进行有序范围查找”的特性，实现 LBS 服务中频繁使用的“搜索附近”的需求

[更多参考](https://blog.csdn.net/weixin_46307478/article/details/122965569)

#### 底层数据结构

##### 键值对

key 就是字符串对象，而 value 可以是字符串对象，也可以是集合数据类型的对象，比如 List 对象、Hash 对象、Set 对象和 Zset 对象

使用了一个哈希表保存所有键值对，是一个数组，存放指针。保存了 void * key 和 void * value 指针，分别指向了实际的键对象和值对象

与哈希表结构一致

<img src="img/f302fce6c92c0682024f47bf7579b44c.png" alt="img" style="zoom: 33%;" />

- edisDb 结构，表示 Redis 数据库的结构，结构体里存放了指向了 dict 结构的指针；
- dict 结构，结构体里存放了 2 个哈希表，正常情况下都是用哈希表1，哈希表2只有在 rehash 的时候才用，具体什么是 rehash，我在本文的哈希表数据结构会讲；
- ditctht 结构，表示哈希表的结构，结构里存放了哈希表数组，数组中的每个元素都是指向一个哈希表节点结构（dictEntry）的指针；
- dictEntry 结构，表示哈希表节点的结构，结构里存放了 void * key 和 void * value 指针， *key 指向的是 String 对象，而 \*value 则可以指向 String 对象，也可以指向集合类型的对象，比如 List 对象、Hash 对象、Set 对象和 Zset 对象

void * key 和 void * value 指针指向的是 Redis 对象，Redis 中的每个对象都由 redisObject 结构表示

<img src="img/58d3987af2af868dca965193fb27c464.png" alt="img" style="zoom:33%;" />

- type，标识该对象是什么类型的对象（String 对象、 List 对象、Hash 对象、Set 对象和 Zset 对象）；
- encoding，标识该对象使用了哪种底层的数据结构；
- ptr，指向底层数据结构的指针

##### SDS

char\* 缺陷：

- C 语言 strlen 线性复杂度
- 无法存储 \0 字符，因此不能保存二进制数据只能保存文本数据
- 标准库字符串操作函数不安全容易缓存区溢出，效率低下

SDS：

结构中的每个成员变量分别介绍下：

- len，记录了字符串长度。这样获取字符串长度的时候，只需要返回这个成员变量值就行，时间复杂度只需要 O（1）。

- alloc，分配给字符数组的空间长度。这样在修改字符串的时候，可以通过 `alloc - len` 计算出剩余的空间大小，可以用来判断空间是否满足修改需求，如果不满足的话，就会自动将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现前面所说的缓冲区溢出的问题。当判断出缓冲区大小不够用时，Redis 会自动将扩大 SDS 的空间大小

- flags，(unsigned char)，用来表示不同类型的 SDS。一共设计了 5 种类型，分别是 sdshdr5、sdshdr8、sdshdr16、sdshdr32 和 sdshdr64

  即使用哪种 int 存储 len, alloc，限制了最大长度

- buf[]，字符数组，用来保存实际数据。不仅可以保存字符串，也可以保存二进制数据

  SDS 为了兼容部分 C 语言标准库的函数， SDS 字符串结尾还是会加上 “\0” 字符

扩容规则：(目的：有效的减少内存分配次数) newlen 是目前长度+请求长度

- 如果所需的 sds 长度小于 1 MB，那么最后的扩容是按照翻倍扩容来执行的，即 2 倍的newlen
- 如果所需的 sds 长度超过 1 MB，那么最后的扩容长度应该是 newlen + 1MB

在 struct 声明了 `__attribute__ ((packed))` ，它的作用是：告诉编译器取消结构体在编译过程中的优化对齐，按照实际占用字节数进行对齐，如：

```c
struct __attribute__((packed)) test2  {
    char a;
    int b;
 } test2; // sizeof(test2)是5，不加是8
```

##### 链表

双向链表，链表节点有三个成员：两个指针和节点值是 void 指针

链表封装：

```c
typedef struct list {
    //链表头节点
    listNode *head;
    //链表尾节点
    listNode *tail;
    //节点值复制函数
    void *(*dup)(void *ptr);
    //节点值释放函数
    void (*free)(void *ptr);
    //节点值比较函数
    int (*match)(void *ptr, void *key);
    //链表节点数量
    unsigned long len;
} list;
```

无环链表。

缺陷：

- 内存不连续，无法很好利用 CPU 缓存
- 每个元素值需要额外包含两个指针，额外内存开销

> Redis 3.0 的 List 对象在数据量比较少的情况下，会采用压缩列表作为底层数据结构的实现，它的优势是节省内存空间，并且是内存紧凑型的数据结构
>
> 所以 Redis 在 3.2 版本设计了新的数据结构 quicklist，并将 List 对象的底层数据结构改由 quicklist 实现。
>
> 然后在 Redis 5.0 设计了新的数据结构 listpack，沿用了压缩列表紧凑型的内存布局，最终在最新的 Redis 版本，将 Hash 对象和 Zset 对象的底层数据结构实现之一的压缩列表，替换成由 listpack 实现

##### 压缩列表

一块连续的内存空间，针对不同长度的数据，进行相应编码

缺陷：

- 不能保存过多的元素，否则查询效率就会降低；
- 新增或修改某个元素时，压缩列表占用的内存空间需要重新分配，甚至可能引发连锁更新的问题

包含的元素数量较少，或者元素值不大的情况才会使用压缩列表作为底层数据结构

<img src="img/ab0b44f557f8b5bc7acb3a53d43ebfcb.png" alt="img" style="zoom:33%;" />

- zlbytes，记录整个压缩列表占用对内存字节数；
- zltail，记录压缩列表尾部节点距离起始地址由多少字节，也就是列表尾的偏移量；
- zllen，记录压缩列表包含的节点数量；
- zlend，标记压缩列表的结束点，固定值 0xFF

首尾是 O1 查找，其他是 ON。

entry 的三部分：

- prevlen，记录了前一个节点的长度，目的是为了实现从后向前遍历；
- encoding，记录了当前节点实际数据的类型和长度，类型主要有两种：字符串和整数。
- data，记录了当前节点的实际数据，类型和长度都由 `encoding` 决定

prvlen：

- 如果前一个节点的长度小于 254 字节，那么 prevlen 属性需要用 1 字节的空间来保存这个长度值；
- 如果前一个节点的长度大于等于 254 字节，那么 prevlen 属性需要用 5 字节的空间来保存这个长度值

encoding 属性的空间大小跟数据是字符串还是整数，以及字符串的长度有关，如下图（下图中的 content 表示的是实际数据，即本文的 data 字段）

<img src="img/压缩列表编码.png" alt="img" style="zoom: 33%;" />

- 如果当前节点的数据是整数，则 encoding 会使用 1 字节的空间进行编码，也就是 encoding 长度为 1 字节。通过 encoding 确认了整数类型，就可以确认整数数据的实际大小了，比如如果 encoding 编码确认了数据是 int16 整数，那么 data 的长度就是 int16 的大小。
- 如果当前节点的数据是字符串，根据字符串的长度大小，encoding 会使用 1 字节/2字节/5字节的空间进行编码，encoding 编码的前两个 bit 表示数据的类型，后续的其他 bit 标识字符串数据的实际长度，即 data 的长度

连锁更新：

压缩列表新增某个元素或修改某个元素时，如果空间不不够，压缩列表占用的内存空间就需要重新分配。而当新插入的元素较大时，可能会导致后续元素的 prevlen 占用空间都发生变化，从而引起连锁更新问题，导致每个元素的空间都要重新分配，造成访问压缩列表性能的下降

> 现在假设一个压缩列表中有多个连续的、长度在 250～253 之间的节点
>
> 因为这些节点长度值小于 254 字节，所以 prevlen 属性需要用 1 字节的空间来保存这个长度值。
>
> 这时，如果将一个长度大于等于 254 字节的新节点加入到压缩列表的表头节点，之后把 1 字节长度扩展为 5 字节，因为 prvlen 每个都恰好超过了 254，所以一直修改。

##### 哈希表

将 key 通过 Hash 函数的计算，就能定位数据在表中的位置

Redis 采用了链式哈希来解决哈希冲突，在不扩容哈希表的前提下，将具有相同哈希值的数据串起来

```c
typedef struct dictht {
    //哈希表数组
    dictEntry **table;
    //哈希表大小
    unsigned long size;  
    //哈希表大小掩码，用于计算索引值
    unsigned long sizemask;
    //该哈希表已有的节点数量
    unsigned long used;
} dictht;
```

节点值：

```c
typedef struct dictEntry {
    //键值对中的键
    void *key;
  
    //键值对中的值
    union {
        void *val;
        uint64_t u64;
        int64_t s64;
        double d;
    } v;
    //指向下一个哈希表节点，形成链表，解决哈希冲突
    struct dictEntry *next;
} dictEntry;
```

缺陷：链表长度变大会导致耗时增加到 On。解决：rehash 对哈希表扩展。

数据逐步增多，触发了 rehash 操作，这个过程分为三步：

- 给哈希表 2 分配空间，一般会比哈希表 1 翻倍；
- 将哈希表 1 的数据迁移到哈希表 2 中；
- 迁移完成后，哈希表 1 的空间会被释放，并把哈希表 2 设置为哈希表 1，然后在哈希表 2 新创建一个空白的哈希表，为下次 rehash 做准备

如果哈希表 1 的数据量非常大，那么在迁移至哈希表 2 的时候，因为会涉及大量的数据拷贝，此时可能会对 Redis 造成阻塞，无法服务其他请求

渐进式 rehash：数据的迁移的工作不再是一次性迁移完成，而是分多次迁移。

- 给哈希表 2 分配空间；
- 在 rehash 进行期间，每次哈希表元素进行新增、删除、查找或者更新操作时，Redis 除了会执行对应的操作之外，还会顺序将哈希表 1 中索引位置上的所有 key-value 迁移到哈希表 2 上；
- 随着处理客户端发起的哈希表操作请求数量越多，最终在某个时间点会把哈希表 1 的所有 key-value 迁移到哈希表 2，从而完成 rehash 操作

在渐进式 rehash 进行期间，哈希表元素的删除、查找、更新等操作都会在这两个哈希表进行

比如，查找一个 key 的值的话，先会在哈希表 1 里面进行查找，如果没找到，就会继续到哈希表 2 里面进行找到。

另外，在渐进式 rehash 进行期间，新增一个 key-value 时，会被保存到哈希表 2 里面，而哈希表 1 则不再进行任何添加操作，这样保证了哈希表 1 的 key-value 数量只会减少，随着 rehash 操作的完成，最终哈希表 1 就会变成空表

触发条件：负载因子 = 节点数 / 哈希表大小

- 当负载因子大于等于 1 ，并且 Redis 没有在执行 bgsave 命令或者 bgrewiteaof 命令，也就是没有执行 RDB 快照或没有进行 AOF 重写的时候，就会进行 rehash 操作。
- 当负载因子大于等于 5 时，此时说明哈希冲突非常严重了，不管有没有有在执行 RDB 快照或 AOF 重写，都会强制进行 rehash 操作

##### 整数集合

Set 对象只包含整数值元素，并且元素数量不大时，就会使用整数集

成员值：

- `uint32_t` `encoding` 为：`INTSET_ENC_INT16`， `INTSET_ENC_INT32`， `INTSET_ENC_INT64`，决定了元素的数据类型
- `uint32_t length` 元素数
- `contents[]` 元素数组

整数集合会有一个升级规则，就是当我们将一个新元素加入到整数集合里面，如果新元素的类型（int32_t）比整数集合现有所有元素的类型（int16_t）都要长时，整数集合需要先进行升级，也就是按新元素的类型（int32_t）扩展 contents 数组的空间大小，然后才能将新元素加入到整数集合里

整数集合升级的过程不会重新分配一个新类型的数组，而是在原本的数组上扩展空间，然后在将每个元素按间隔类型大小分割，如果 encoding 属性值为 INTSET_ENC_INT16，则每个元素的间隔就是 16 位

不支持降级操作，一旦对数组进行了升级，就会一直保持升级后的状态，删了不降

##### 跳表

[oiwiki](https://oi-wiki.org/ds/skiplist/)

支持平均 O(logN) 复杂度的节点查找

zset 结构体里有两个数据结构：一个是跳表，一个是哈希表。这样的好处是既能进行高效的范围查询，也能进行高效单点查询。同时维护二者。哈希表只是用于以常数复杂度获取元素权重，大部分操作都是跳表实现的

跳表就是多层有序链表，下图是一个三层跳表：

<img src="img/2ae0ed790c7e7403f215acb2bd82e884.png" alt="img" style="zoom: 33%;" />

- L0 层级共有 5 个节点，分别是节点1、2、3、4、5；
- L1 层级共有 3 个节点，分别是节点 2、3、5；
- L2 层级只有 1 个节点，也就是节点 3 

查找节点 4：L2->3->4，只需要跳两次

```c
typedef struct zskiplistNode {
    //Zset 对象的元素值
    sds ele;
    //元素权重值
    double score;
    //后向指针
    struct zskiplistNode *backward;
  
    //节点的level数组，保存每层上的前向指针和跨度
    struct zskiplistLevel {
        struct zskiplistNode *forward;
        unsigned long span;
    } level[];
} zskiplistNode;
```

level 从跨度最小(l0)的层开始，span 跨度是求节点在跳表的排位，计算头结点到当前节点的跨度和就可以求出排名

结构包含头尾节点，节点数量最大层数

```c
typedef struct zskiplist {
    struct zskiplistNode *header, *tail;
    unsigned long length;
    int level;
} zskiplist;
```

查找一个跳表节点的过程时，跳表会从头节点的最高层开始，逐一遍历每一层。在遍历某一层的跳表节点时，会用跳表节点中的 SDS 类型的元素和元素的权重来进行判断，共有两个判断条件：

- 如果当前节点的权重小于要查找的权重时，跳表就会访问该层上的下一个节点。
- 如果当前节点的权重等于要查找的权重时，并且当前节点的 SDS 类型数据小于要查找的数据时，跳表就会访问该层上的下一个节点。

如果上面两个条件都不满足，或者下一个节点为空时，跳表就会使用目前遍历到的节点的 level 数组里的下一层指针，然后沿着下一层指针继续查找，这就相当于跳到了下一层接着查找

<img src="img/3层跳表-跨度.drawio.png" alt="img" style="zoom:50%;" />

如查询元素 abcd，权重4，

- 先找到节点 3，其下一个节点是空，所以下一层，找到 4

跳表的相邻两层的节点数量最理想的比例是 2:1，查找复杂度可以降低到 O(logN)

如果采用新增节点或者删除节点时，来调整跳表节点以维持比例的方法的话，会带来额外的开销。

Redis 则采用一种巧妙的方法是，跳表在创建节点的时候，随机生成每个节点的层数，并没有严格维持相邻两层的节点数量比例为 2 : 1 的情况

具体的做法是，跳表在创建节点时候，会生成范围为[0-1]的一个随机数，如果这个随机数小于 0.25（相当于概率 25%），那么层数就增加 1 层，然后继续生成下一个随机数，直到随机数的结果大于 0.25 结束，最终确定该节点的层数。层高最大限制是 64

如果层高最大限制是 64，那么在创建跳表头节点的时候，就会直接创建 64 层高的头节点。

ZSKIPLIST_MAXLEVEL 定义的是最高的层数，Redis 7.0 定义为 32，Redis 5.0 定义为 64，Redis 3.0 定义为 32

使用跳表而不是平衡树的理由：[作者回答](https://news.ycombinator.com/item?id=1171423)

- 内存常数比平衡树低
- 范围查找使用频繁，其缓存局部性原理表现跟平衡树差不多
- 易于实现

补充：

- 平衡树每个节点 2 个指针，而跳表每个节点，设随机概率为 0.25，只需要
  $$
  \lim_{i\to\infty}(\dfrac14)^i=\dfrac{1-0.25^i}{1-0.25}=\dfrac43
  $$
  这么多个指针(`dp[i]=1+0.25dp[i]`)

- 范围查找比平衡树简单。找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点

##### quicklist

 Redis 3.2 的时候，List 对象的底层改由 quicklist 数据结构实现

 quicklist 就是一个链表，而链表中的每个元素又是一个压缩列表

通过控制每个链表节点中的压缩列表的大小或者元素个数，来规避连锁更新的问题。因为压缩列表元素越少或越小，连锁更新带来的影响就越小，从而提供了更好的访问性能

向 quicklist 添加一个元素的时候，不会像普通的链表那样，直接新建一个链表节点。而是会检查插入位置的压缩列表是否能容纳该元素，如果能容纳就直接保存到 quicklistNode 结构里的压缩列表，如果不能容纳，才会新建一个新的 quicklistNode 结构

quicklist 虽然通过控制 quicklistNode 结构里的压缩列表的大小或者元素个数，来减少连锁更新带来的性能影响，但是并没有完全解决连锁更新的问题

##### listpack

Redis 在 5.0 新设计一个数据结构叫 listpack，目的是替代压缩列表，它最大特点是 listpack 中每个节点不再包含前一个节点的长度了

listpack 头包含两个属性，分别记录了 listpack 总字节数和元素数量，然后 listpack 末尾也有个结尾标识

每个 listpack 节点结构如下：

- encoding，定义该元素的编码类型，会对不同长度的整数和字符串进行编码；
- data，实际存放的数据；
- len，encoding+data的总长度；

listpack 只记录当前节点的长度，当我们向 listpack 加入一个新元素的时候，不会影响其他节点的长度字段的变化

prevlen 主要是逆序遍历需要，而 listpack 的后序遍历：从当前列表项起始位置的指针开始，向左逐个字节解析，得到前一项的 entry-len 值 [源码lpDecodeBacklen](https://github.com/antirez/listpack/blob/master/listpack.c)

#### 布隆过滤器

[参考](https://zhuanlan.zhihu.com/p/684159014) [redis使用例子](https://blog.csdn.net/wuhuayangs/article/details/121830094) [参考2](https://openatomworkshop.csdn.net/6645ac60b12a9d168eb6c3b5.html?dp_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpZCI6MzQ0NzA0LCJleHAiOjE3MTg4ODgxOTAsImlhdCI6MTcxODI4MzM5MCwidXNlcm5hbWUiOiJxcV8zNDg0NDE1MCJ9.0o2PJBVbOh4yWE-UrydWSJibrwkr_pi2s3rVM6N6cbc)

##### 概述

Bloom Filter

空间高效的概率数据结构，用于判断一个元素是否存在于集合中

当布隆过滤器说，某个数据存在时，这个数据可能不存在；当布隆过滤器说，某个数据不存在时，那么这个数据一定不存在。

布隆过滤器只需要哈希表的 1/8 或 1/4 的空间复杂度就能完成同样的问题

##### 原理

使用一个固定大小的位数组，通常初始化为全0，用于存储元素的存在情况

使用多个哈希函数，通常使用不同的哈希函数来将元素映射到位数组的不同位置。这些哈希函数可以是任意的哈希算法，如MD5、SHA-1、SHA-256等

添加元素：当一个元素要被添加到布隆过滤器中时，该元素首先会被输入到多个哈希函数中，每个哈希函数计算出一个哈希值。这些哈希值会被映射到位数组的不同位置，将对应的位设置为1，表示该元素存在于集合中

判断元素存在：当需要判断一个元素是否存在于布隆过滤器中时，同样地，该元素会被输入到多个哈希函数中，计算出多个哈希值。然后布隆过滤器会检查对应的位数组位置上的位是否都为1。如果有任何一个位置上的位不为1，那么可以确定该元素一定不存在于集合中。如果所有位置上的位都为1，那么该元素可能存在于集合中，但仍可能是误判

误判率取决于哈希函数的数量、位数组的大小以及被插入元素的数量。随着插入的元素数量增加，误判率也会逐渐增加。为了降低误判率，可以增加位数组的大小和使用更多的哈希函数，但这也会增加布隆过滤器的空间复杂度

##### 优缺点

优点：

- 时空复杂度低
- 保密：不存储元素本身

缺点：

- 存在误判率
- 无法获取元素本身
- 难以删除元素

### 线程模型

[题目](https://www.xiaolincoding.com/redis/base/redis_interview.html)

##### 概述

单线程指：接收客户端请求->解析请求 ->进行数据读写等操作->发送数据给客户端，这个步骤由一个主线程来完成。

Redis 启动时会有后台线程 BIO(background I/O)：

- 2.6 版本：2 个，分别处理关闭文件、AOF 刷盘(append only file)

- 4.0 版本：新增异步释放 Redis 内存的 lazyfree 线程

  执行 unlink key / flushdb async / flushall async 等命令，会把这些删除操作交给后台线程来执行，好处是不会导致 Redis 主线程卡顿。因此，当要删除一个大 key 的时候，不要使用 del 命令删除，因为 del 是在主线程处理的，这样会导致 Redis 主线程卡顿，因此应该使用 unlink 命令来异步删除大key

多线程的理由：操作耗时，容易导致主线程阻塞

后台线程相当于一个消费者，生产者把耗时任务丢到任务队列中，消费者（BIO）不停轮询这个队列，拿出任务就去执行对应的方法即可

三个任务都有各自的任务队列：

- BIO_CLOSE_FILE，关闭文件任务队列：当队列有任务后，后台线程会调用 close(fd) ，将文件关闭；
- BIO_AOF_FSYNC，AOF刷盘任务队列：当 AOF 日志配置成 everysec 选项后，主线程会把 AOF 写日志操作封装成一个任务，也放到队列中。当发现队列有任务后，后台线程会调用 fsync(fd)，将 AOF 文件刷盘，
- BIO_LAZY_FREE，lazy free 任务队列：当队列有任务后，后台线程会 free(obj) 释放对象 / free(dict) 删除数据库所有对象 / free(skiplist) 释放跳表对象

##### 单线程模型

<img src="img/redis单线程模型.drawio.png" alt="img" style="zoom: 33%;" />

蓝色部分主事件循环是主线程，即命令处理和网络 I/O 是单线程。

初始化：

- 首先，调用 epoll_create() 创建一个 epoll 对象和调用 socket() 创建一个服务端 socket
- 然后，调用 bind() 绑定端口和调用 listen() 监听该 socket；
- 然后，将调用 epoll_ctl() 将 listen socket 加入到 epoll，同时注册连接事件处理函数。

事件循环函数，主要会做以下事情：

- 首先，先调用处理发送队列函数，看是发送队列里是否有任务，如果有发送任务，则通过 write 函数将客户端发送缓存区里的数据发送出去，如果这一轮数据没有发送完，就会注册写事件处理函数，等待 epoll_wait 发现可写后再处理 。
- 接着，调用 epoll_wait 函数等待事件的到来：
  - 如果是连接事件到来，则会调用连接事件处理函数，该函数会做这些事情：调用 accpet 获取已连接的 socket -> 调用 epoll_ctl 将已连接的 socket 加入到 epoll -> 注册读事件处理函数；
  - 如果是读事件到来，则会调用读事件处理函数，该函数会做这些事情：调用 read 获取客户端发送的数据 -> 解析命令 -> 处理
  - 
  - 命令 -> 将客户端对象添加到发送队列 -> 将执行结果写到发送缓存区等待发送；
  - 如果是写事件到来，则会调用写事件处理函数，该函数会做这些事情：通过 write 函数将客户端发送缓存区里的数据发送出去，如果这一轮数据没有发送完，就会继续注册写事件处理函数，等待 epoll_wait 发现可写后再处理

性能分析：单线程官方基准测试 Redis 吞吐量 10w/s

分析原因：

- 操作在内存，高效数据结构。故瓶颈是内存/带宽而不是 CPU；因为 CPU 不是瓶颈所以单线程就好了
- 单线程避免多线程竞争，省去切换线程的开销，也不会死锁
- I/O 多路复用机制(一个线程处理多个 I/O 流，即 select/epoll 机制)，使得单线程同时监听多个 Socket

单线程程序无法利用服务器多核 CPU，仍然用的理由 [官方解释](https://redis.io/docs/latest/develop/get-started/faq/) (How can Redis use multiple CPUs or cores)

> 使用了单线程后，可维护性高，多线程模型虽然在某些方面表现优异，但是它却引入了程序执行顺序的不确定性，带来了并发读写的一系列问题，增加了系统复杂度、同时可能存在线程切换、甚至加锁解锁、死锁造成的性能损耗。

##### 多线程的引入

Redis 6.0 版本之后，也采用了多个 I/O 线程来处理网络请求，这是因为随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在网络 I/O 的处理上。Redis 6.0 对于网络 I/O 采用多线程来处理。但是对于命令的执行，Redis 仍然使用单线程来处理。Redis 6.0 版本引入的多线程 I/O 特性对性能提升至少是一倍以上

Redis 6.0 版本支持的 I/O 多线程特性，默认情况下 I/O 多线程只针对发送响应数据（write client socket），并不会以多线程的方式处理读请求（read client socket）。要想开启多线程处理客户端读请求，就需要把 Redis.conf 配置文件中的 io-threads-do-reads 配置项设为 yes

```
io-threads-do-reads yes 
```

Redis.conf 配置文件中提供了 IO 多线程个数的配置项

```sh
io-threads 4 
```

官方的建议是如果为 4 核的 CPU，建议线程数设置为 2 或 3，如果为 8 核 CPU 建议线程数设置为 6，线程数一定要小于机器核数，线程数并不是越大越好

Redis 6.0 版本之后，Redis 在启动的时候，默认情况下会额外创建 6 个线程（*这里的线程数不包括主线程*）：

- Redis-server ： Redis的主线程，主要负责执行命令；
- bio_close_file、bio_aof_fsync、bio_lazy_free：三个后台线程，分别异步处理关闭文件任务、AOF刷盘任务、释放内存任务；
- io_thd_1、io_thd_2、io_thd_3：三个 I/O 线程，io-threads 默认是 4 ，所以会启动 3（4-1）个 I/O 多线程，用来分担 Redis 网络 I/O 的压力

### 持久化

[参考](https://www.xiaolincoding.com/redis/base/redis_interview.html#redis-%E6%8C%81%E4%B9%85%E5%8C%96)

##### 概述

把数据存储到磁盘，这样在 Redis 重启就能够从磁盘中恢复原有的数据，三种方式：

- AOF 日志：每执行一条写操作命令，就把该命令以追加的方式写入到一个文件里；
- RDB 快照：将某一时刻的内存数据，以二进制的方式写入磁盘；(Redis 默认开启)
- 混合持久化方式：Redis 4.0 新增的方式，集成了 AOF 和 RBD 的优点

##### AOF 日志

[参考](https://www.xiaolincoding.com/redis/storage/aof.html#aof-%E6%97%A5%E5%BF%97)

Redis 在执行完一条写操作命令后，就会把该命令以追加的方式写入到一个文件里(先到 `server.aof_buf` 缓冲区，再刷盘，如等 OS 决定啥时刷)，然后 Redis 重启时，会读取该文件记录的命令，然后逐一执行命令的方式来进行数据恢复。

<img src="img/337021a153944fd0f964ca834e34d0f2-20230309232243363.png" alt="img" style="zoom:50%;" />

`*3` 命令有三个部分，`$数字` 表示命令的字节长度，下面跟着命令。

Reids 是先执行写操作命令后，才将该命令记录到 AOF 日志里的，好处：

- 避免额外检查开销。如果反过来可能会写语法错误的命令。
- 不会阻塞当前写操作的执行。

风险：

- 数据可能丢失。
- 阻塞其他操作。AOF 日志也在主线程执行。

AOF 写回策略：

<img src="img/4eeef4dd1bedd2ffe0b84d4eaa0dbdea-20230309232249413.png" alt="img" style="zoom:50%;" />

Redis 提供了 3 种写回硬盘的策略，控制的就是上面说的第三步的过程。 在 Redis.conf 配置文件中的 appendfsync 配置项可以有以下 3 种参数可填：

- Always，所以它的意思是每次写操作命令执行完后，同步将 AOF 日志数据写回硬盘；可靠性高但性能开销大
- Everysec，所以它的意思是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，然后每隔一秒将缓冲区里的内容写回到硬盘；宕机丢失 1s 内数据
- No，意味着不由 Redis 控制写回硬盘的时机，转交给操作系统控制写回的时机，也就是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，再由操作系统决定何时将缓冲区内容写回硬盘。宕机可能丢失很多数据

AOF 日志文件过大就会带来性能问题，恢复过程慢。AOF 重写机制，当 AOF 文件的大小超过所设定的阈值后，Redis 就会启用 AOF 重写机制，来压缩 AOF 文件。

AOF 重写机制是在重写时，读取当前数据库中的所有键值对，然后将每一个键值对用一条命令记录到新的 AOF 文件，等到全部记录完后，就将新的 AOF 文件替换掉现有的 AOF 文件。

如分别对同一个键值对执行两次修改，只保留最新的一次修改。

如果 AOF 重写过程中失败了，现有的 AOF 文件就会造成污染，可能无法用于恢复使用。所以 AOF 重写过程，先重写到新的 AOF 文件，重写失败的话，就直接删除这个文件就好，不会对现有的 AOF 文件造成影响

重写 AOF 过程是由后台子进程 `bgrewriteaof` 来完成的：

- 子进程进行 AOF 重写期间，主进程可以继续处理命令请求，从而避免阻塞主进程
- 子进程带有主进程的数据副本，这里使用子进程而不是线程，因为如果是使用线程，多线程之间会共享内存，那么在修改共享内存数据的时候，需要通过加锁来保证数据的安全，而这样就会降低性能。而使用子进程，创建子进程时，父子进程是共享内存数据的，不过这个共享的内存只能以只读的方式，而当父子进程任意一方修改了该共享内存，就会发生写时复制，于是父子进程就有了独立的数据副本，就不用加锁来保证数据安全。

重写 AOF 日志过程中，如果主进程修改了已经存在 key-value，那么会发生写时复制，此时这个 key-value 数据在子进程的内存数据就跟主进程的内存数据不一致了，为了解决这种数据不一致问题，Redis 设置了一个 AOF 重写缓冲区，这个缓冲区在创建 bgrewriteaof 子进程之后开始使用。

重写 AOF 期间，当 Redis 执行完一个写命令之后，它会同时将这个写命令写入到 AOF 缓冲区和 AOF 重写缓冲区。

在 bgrewriteaof 子进程执行 AOF 重写期间，主进程需要执行以下三个工作:

- 执行客户端发来的命令；
- 将执行后的写命令追加到 AOF 缓冲区
- 将执行后的写命令追加到 AOF 重写缓冲区

当子进程完成 AOF 重写工作（扫描数据库中所有数据，逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志）后，会向主进程发送一条信号，信号是进程间通讯的一种方式，且是异步的。

主进程收到该信号后，会调用一个信号处理函数，该函数主要做以下工作：

- 将 AOF 重写缓冲区中的所有内容追加到新的 AOF 的文件中，使得新旧两个 AOF 文件所保存的数据库状态一致；
- 新的 AOF 的文件进行改名，覆盖现有的 AOF 文件。

信号函数执行完后，主进程就可以继续像往常一样处理命令了。

AOF 日志记录的是操作命令，不是实际的数据，所以用 AOF 方法做故障恢复时，需要全量把日志都执行一遍，一旦 AOF 日志非常多，势必会造成 Redis 的恢复操作缓慢。

##### RDB 快照

Redis Database [参考](https://www.xiaolincoding.com/redis/storage/rdb.html)

记录某一个瞬间的实际内存全量数据，恢复时直接将 RDB 文件读入内存就可以

Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave，他们的区别就在于是否在主线程里执行：

- 执行了 save 命令，就会在主线程生成 RDB 文件，由于和执行操作命令在同一个线程，所以如果写入 RDB 文件的时间太长，会阻塞主线程；
- 执行了 bgsave 命令，会创建一个子进程来生成 RDB 文件，这样可以避免主线程的阻塞；

Redis 还可以通过配置文件的选项来实现每隔一段时间自动执行一次 bgsave 命令，默认会提供以下配置：

```
save 900 1
save 300 10
save 60 10000
```

只要满足上面条件的任意一个，就会执行 bgsave，它们的意思分别是：

- 900 秒之内，对数据库进行了至少 1 次修改；
- 300 秒之内，对数据库进行了至少 10 次修改；
- 60 秒之内，对数据库进行了至少 10000 次修改

执行 bgsave 过程中，Redis 依然可以继续处理操作命令的，也就是数据是能被修改的，关键的技术就在于写时复制技术（Copy-On-Write, COW）

- 执行 bgsave 命令的时候，会通过 fork() 创建子进程，此时子进程和父进程是共享同一片内存数据的，因为创建子进程的时候，会复制父进程的页表，但是页表指向的物理内存还是一个，此时如果主线程执行读操作，则主线程和 bgsave 子进程互相不影响
- 如果主线程执行写操作，则被修改的数据会复制一份副本，然后 bgsave 子进程会把该副本数据写入 RDB 文件，在这个过程中，主线程仍然可以直接修改原来的数据

极端情况下，如果所有的共享内存都被修改，则此时的内存占用是原先的 2 倍。所以，针对写操作多的场景，要留意下快照过程中内存的变化，防止内存被占满了

##### 混合持久化

RDB 优点是数据恢复速度快，但是快照的频率不好把握。频率太低，丢失的数据就会比较多，频率太高，就会影响性能。

```
aof-use-rdb-preamble yes
```

AOF 优点是丢失数据少，但是数据恢复不快

Redis 4.0 提出了混合使用 AOF 日志和内存快照，也叫混合持久化

在 AOF 重写日志时，fork 出来的重写子进程会先将与主线程共享的内存数据以 RDB 方式写入到 AOF 文件，然后主线程处理的操作命令会被记录在重写缓冲区里，重写缓冲区里的增量命令会以 AOF 方式写入到 AOF 文件，写入完成后通知主进程将新的含有 RDB 格式和 AOF 格式的 AOF 文件替换旧的的 AOF 文件。此时 AOF 文件的前半部分是 RDB 格式的全量数据，后半部分是 AOF 格式的增量数据。

混合持久化优点：

- 混合持久化结合了 RDB 和 AOF 持久化的优点，开头为 RDB 的格式，使得 Redis 可以更快的启动，同时结合 AOF 的优点，减低了大量数据丢失的风险。

混合持久化缺点**：**

- AOF 文件中添加了 RDB 格式的内容，使得 AOF 文件的可读性变得很差；
- 兼容性差，如果开启混合持久化，那么此混合持久化 AOF 文件，就不能用在 Redis 4.0 之前版本了

##### 大key

[参考](https://www.xiaolincoding.com/redis/storage/bigkey_aof_rdb.html)

AOF 重写机制和 RDB 快照（bgsave 命令）的过程，都会分别通过 `fork()` 函数创建一个子进程来处理任务

在通过 `fork()` 函数创建子进程的时候，虽然不会复制父进程的物理内存，但是内核会把父进程的页表复制一份给子进程，如果页表很大，那么这个复制过程是会很耗时的，那么在执行 fork 函数的时候就会发生阻塞现象

fork 函数是由 Redis 主线程调用的，如果 fork 函数发生阻塞，那么意味着就会阻塞 Redis 主线程。由于 Redis 执行命令是在主线程处理的，所以当 Redis 主线程发生阻塞，就无法处理后续客户端发来的命令

执行 `info` 命令获取到 latest_fork_usec 指标，表示 Redis 最近一次 fork 操作耗时

```
# 最近一次 fork 操作耗时
latest_fork_usec:315
```

如果 fork 耗时很大，比如超过1秒，则需要做出优化调整：

- 单个实例的内存占用控制在 10 GB 以下，这样 fork 函数就能很快返回。
- 如果 Redis 只是当作纯缓存使用，不关心 Redis 数据安全性问题，可以考虑关闭 AOF 和 AOF 重写，这样就不会调用 fork 函数了
- 在主从架构中，要适当调大 repl-backlog-size，避免因为 repl_backlog_buffer 不够大，导致主节点频繁地使用全量同步的方式，全量同步的时候，是会创建 RDB 文件的，也就是会调用 fork 函数

当父进程或者子进程在向共享内存发起写操作时，CPU 就会触发写保护中断，然后 OS 进行物理内存的复制，并重新设置其内存映射关系，将父子进程的内存读写权限设置为可读写，最后才会对内存进行写操作：COW

有两个阶段会导致阻塞父进程：

- 创建子进程的途中，由于要复制父进程的页表等数据结构，阻塞的时间跟页表的大小有关，页表越大，阻塞的时间也越长；
- 创建完子进程后，如果子进程或者父进程修改了共享数据，就会发生写时复制，这期间会拷贝物理内存，如果内存越大，自然阻塞的时间也越长

Linux 内核从 2.6.38 开始支持内存大页机制，该机制支持 2MB 大小的内存页分配，而常规的内存页分配是按 4KB 的粒度来执行的。

如果采用了内存大页，那么即使客户端请求只修改 100B 的数据，在发生写时复制后，Redis 也需要拷贝 2MB 的大页。相反，如果是常规内存页机制，只用拷贝 4KB

关闭内存大页（默认是关闭的）：

```
echo never >  /sys/kernel/mm/transparent_hugepage/enabled
```



### 集群

[参考](https://www.xiaolincoding.com/redis/cluster/master_slave_replication.html) [参考](https://www.xiaolincoding.com/redis/base/redis_interview.html#redis-%E9%9B%86%E7%BE%A4)

#### 概述

##### 主从复制

主从复制是 Redis 高可用服务的最基础的保证，实现方案就是将从前的一台 Redis 服务器，同步数据到多台从 Redis 服务器上，即一主多从的模式，且主从服务器之间采用的是读写分离的方式。

主服务器可以进行读写操作，当发生写操作时自动将写操作同步给从服务器，而从服务器一般是只读，并接受主服务器同步过来写操作命令，然后执行这条命令

<img src="img/2b7231b6aabb9a9a2e2390ab3a280b2d.png" alt="img" style="zoom:50%;" />

主从服务器之间的命令复制是异步进行

在主从服务器命令传播阶段，主服务器收到新的写命令后，会发送给从服务器。但是，主服务器并不会等到从服务器实际执行完命令后，再把结果返回给客户端，而是主服务器自己在本地执行完命令后，就会向客户端返回结果了。如果从服务器还没有执行主服务器同步过来的命令，主从服务器间的数据就不一致了。所以，无法实现强一致性保证（主从数据时时刻刻保持一致），数据不一致是难以避免的

Redis 的主从服务器出现故障宕机时，需要手动进行恢复

##### 哨兵模式

Redis Sentinel

可以监控主从服务器，并且提供主从节点故障转移的功能

<img src="img/26f88373d8454682b9e0c1d4fd1611b4.png" alt="img" style="zoom: 33%;" />

##### 切片集群模式

缓存数据量大到一台服务器无法缓存时，就需要使用 Redis 切片集群（Redis Cluster ）方案，它将数据分布在不同的服务器上，以此来降低系统对单主节点的依赖，从而提高 Redis 服务的读写性能

采用哈希槽（Hash Slot），来处理数据和节点之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384 个哈希槽

每个键值对都会根据它的 key，被映射到一个哈希槽中，具体执行过程分为两大步：

- 根据键值对的 key，按照 CRC16 算法计算一个 16 bit 的值。
- 再用 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽

分配方法：

- 平均分配： 在使用 cluster create 命令创建 Redis 集群时，Redis 会自动把所有哈希槽平均分布到集群节点上。比如集群中有 9 个节点，则每个节点上槽的个数为 16384/9 个。
- 手动分配： 可以使用 cluster meet 命令手动建立节点间的连接，组成集群，再使用 cluster addslots 命令，指定每个节点上的哈希槽个数

比如节点 1 保存哈希槽 0 和 1，节点 2 保存哈希槽 2 和 3：

```sh
redis-cli -h 192.168.1.10 –p 6379 cluster addslots 0,1
redis-cli -h 192.168.1.11 –p 6379 cluster addslots 2,3
```

##### 集群脑裂

由于网络问题，集群节点之间失去联系。主从数据不同步；重新平衡选举，产生两个主服务。等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，由于会从节点会清空自己的缓冲区，所以导致之前客户端写入的数据丢失了

> 具体而言：
>
> - 主节点的网络突然发生了问题，它与所有的从节点都失联了，但是此时的主节点和客户端的网络是正常的，这个客户端并不知道 Redis 内部已经出现了问题，还在照样的向这个失联的主节点写数据，这些数据都是无法同步给从节点
> - 哨兵节点发现主节点失联了，它就认为主节点挂了，在从节点选举一个新的主节点，导致集群就有两个主节点
> - 网络恢复，原本的主节点降级为从节点，向新主节点请求数据同步。因为第一次同步是全量同步的方式，该节点清空自己原本的数据，于是客户端写给该节点的数据产生了丢失

解决方法：当主节点发现从节点下线或者通信超时的总数量小于阈值时，那么禁止主节点进行写数据，直接把错误返回给客户端，设置参数：

- min-slaves-to-write x，主节点必须要有至少 x 个从节点连接，如果小于这个数，主节点会禁止写数据。
- min-slaves-max-lag x，主从数据复制和同步的延迟不能超过 x 秒，如果超过，主节点会禁止写数据

组合参数，则两者需要同时满足。

回到上面的情景，主节点断网时，无法和从库进行 ACK 确认，无法响应哨兵心跳包，因此禁止写数据，当恢复网络时，它成为了从节点，没有数据丢失

> 假设我们将 min-slaves-to-write 设置为 1，把 min-slaves-max-lag 设置为 12s，把哨兵的 down-after-milliseconds 设置为 10s，主库因为某些原因卡住了 15s，导致哨兵判断主库客观下线，开始进行主从切换。
>
> 同时，因为原主库卡住了 15s，没有一个从库能和原主库在 12s 内进行数据复制，原主库也无法接收客户端请求了。
>
> 这样一来，主从切换完成后，也只有新主库能接收请求，不会发生脑裂，也就不会发生数据丢失的问题了

#### 主从复制实现

[参考](https://www.xiaolincoding.com/redis/cluster/master_slave_replication.html)

##### 第一次同步

`replicaof`（Redis 5.0 之前使用 slaveof）命令形成主服务器和从服务器的关系

```
# 服务器 B 执行这条命令
replicaof <服务器 A 的 IP 地址> <服务器 A 的 Redis 端口号>
```

服务器 B 就会变成服务器 A 的从服务器，然后与主服务器进行第一次同步

<img src="img/ea4f7e86baf2435af3999e5cd38b6a26.png" alt="图片" style="zoom:50%;" />

- runID，每个 Redis 服务器在启动时都会自动生产一个随机的 ID 来唯一标识自己。当从服务器和主服务器第一次同步时，因为不知道主服务器的 run ID，所以将其设置为 "?"。
- offset，表示复制的进度，第一次同步时，其值为 -1

第三阶段的命令：那么为了保证主从服务器的数据一致性，主服务器在下面这三个时间间隙中将收到的写操作命令，写入到 replication buffer 缓冲区里：

- 主服务器生成 RDB 文件期间；
- 主服务器发送 RDB 文件给从服务器期间；
- 从服务器加载 RDB 文件期间

从服务器载入完毕后发送确认消息给主服务器。

主从服务器在完成第一次同步后，双方之间就会维护一个 TCP 长连接。Redis 主节点每次收到写命令之后，先写到内部的缓冲区，然后异步发送给从节点

从服务器可以连成树，进而从服务器给其他从服务器作主服务器，减轻全量复制的压力

##### 增量复制

如果主从服务器间的网络连接断开了，那么就无法进行命令传播了，这时从服务器的数据就没办法和主服务器保持一致了，客户端就可能从从服务器读到旧的数据

在 Redis 2.8 之前，如果主从服务器在命令同步时出现了网络断开又恢复的情况，从服务器就会和主服务器重新进行一次全量复制。Redis 2.8 开始，网络断开又恢复后，从主从服务器会采用增量复制的方式继续同步

- 从服务器在恢复网络后，会发送 psync 命令给主服务器，此时的 psync 命令里的 offset 参数不是 -1；
- 主服务器收到该命令后，然后用 CONTINUE 响应命令告诉从服务器接下来采用增量复制的方式同步数据；
- 然后主服务将主从服务器断线期间，所执行的写命令发送给从服务器，然后从服务器执行这些命令

在主服务器进行命令传播时，不仅会将写命令发送给从服务器，还会将写命令写入到 repl_backlog_buffer 缓冲区里，因此 这个缓冲区里会保存着最近传播的写命令

- repl_backlog_buffer，是一个环形缓冲区，用于主从服务器断连后，从中找到差异的数据；

- replication offset，标记上面那个缓冲区的同步进度，主从服务器都有各自的偏移量，主服务器使用 master_repl_offset 来记录自己写到的位置，从服务器使用 slave_repl_offset 来记录自己读到的位置

- 如果判断出从服务器要读取的数据已经不存在 repl_backlog_buffer 缓冲区里，那么主服务器将采用全量同步的方式

- repl_backlog_buffer 缓行缓冲区的默认大小是 1M，并且由于它是一个环形缓冲区，所以当缓冲区写满后，主服务器继续写入的话，就会覆盖之前的数据，可以调整，公式：`second * write_size_per_second`

  second 是重连平均时间，第二个是每秒主服务器写数据平均大小

  为了应对一些突发的情况，可以将 repl_backlog_buffer 的大小设置为此基础上的 2 倍

  ```
  repl-backlog-size 1mb
  ```

##### 心跳

如果有一半以上的节点去 ping 一个节点的时候没有 pong 回应，集群就会认为这个节点挂掉了，会断开与这个节点的连接

- Redis 主节点默认每隔 10 秒对从节点发送 ping 命令，判断从节点的存活性和连接状态，可通过参数repl-ping-slave-period控制发送频率。
- Redis 从节点每隔 1 秒发送 replconf ack{offset} 命令，给主节点上报自身当前的复制偏移量，目的是为了：
  - 实时监测主从节点网络状态；
  - 上报自身复制偏移量， 检查复制数据是否丢失， 如果从节点数据丢失， 再从主节点的复制缓冲区中拉取丢失数据

##### buffer

复制缓冲区 replication buffer：主节点完成写指令后，将其写入按顺序发给所有从。确保所有新的数据更改都能被发送到从服务器，实现数据状态的一致性

回滚缓冲区 repl backlog buffer：支持断线后的部分重新同步，而不需要进行全量复制

replication buffer 、repl backlog buffer 区别如下：

- 出现的阶段不一样：
  - repl backlog buffer 是在增量复制阶段出现，一个主节点只分配一个 repl backlog buffer；
  - replication buffer 是在全量复制阶段和增量复制阶段都会出现，主节点会给每个新连接的从节点，分配一个 replication buffer；
- 这两个 Buffer 都有大小限制的，当缓冲区满了之后，发生的事情不一样：
  - 当 repl backlog buffer 满了，因为是环形结构，会直接覆盖起始位置数据;
  - 当 replication buffer 满了，会导致连接断开，删除缓存，从节点重新连接，重新开始全量复制

##### 数据一致

主节点并不会等到从节点实际执行完命令后，再把结果返回给客户端，而是主节点自己在本地执行完命令后，就会向客户端返回结果了。如果从节点还没有执行主节点同步过来的命令，主从节点间的数据就不一致了

- 尽量保证主从节点间的网络连接状况良好

- 开发一个外部程序来监控主从节点间的复制进度

  Redis 的 INFO replication 命令可以查看主节点接收写命令的进度信息（master_repl_offset）和从节点复制写命令的进度信息（slave_repl_offset），所以，我们就可以开发一个监控程序，先用 INFO replication 命令查到主、从节点的进度，然后，我们用 master_repl_offset 减去 slave_repl_offset，这样就能得到从节点和主节点间的复制进度差值了

  如果某个从节点的进度差值大于我们预设的阈值，我们可以让客户端不再和这个从节点连接进行数据读取，这样就可以减少读到不一致数据的情况。不过，为了避免出现客户端和所有从节点都不能连接的情况，我们需要把复制进度差值的阈值设置得大一些

不可能保证数据完全不丢失，只能做到使得尽量少的数据丢失

- 参数 min-slaves-max-lag，表示一旦所有的从节点数据复制和同步的延迟都超过了 min-slaves-max-lag 定义的值，那么主节点就会拒绝接收任何请求；这就是宕机主从最大数据不一致的秒数
- 当客户端发现 master 不可写后，可以采取降级措施，将数据暂时写入本地缓存和磁盘中，在一段时间（等 master 恢复正常）后重新写入 master 来保证数据不丢失，也可以将数据写入 kafka 消息队列，等 master 恢复正常，再隔一段时间去消费 kafka 中的数据，让将数据重新写入 master

还有集群脑裂的情况，见上文

#### 哨兵节点

##### 引入原因

主节点挂掉，要恢复服务的话，需要人工介入

哨兵节点主要负责三件事情：监控、选主、通知

##### 下线

哨兵每秒给所有主从发 ping，如果主节点或者从节点没有在规定的时间内响应哨兵的 PING 命令，哨兵就会将它们标记为主观下线。这个规定的时间是配置项 `down-after-milliseconds` 参数设定的，单位是毫秒

客观下线只适用于主节点。主节点可能其实并没有故障，可能只是因为主节点的系统压力比较大或者网络发送了拥塞，导致主节点没有在规定时间内响应哨兵的 PING 命令，为了减少误判的情况，哨兵在部署的时候不会只部署一个节点，而是用多个节点部署成哨兵集群（最少需要三台机器来部署哨兵集群，哨兵节点的数量应该是奇数），通过多个哨兵节点一起判断，就可以就可以避免单个哨兵因为自身网络状况不好，而误判主节点下线的情况，多个哨兵的网络同时不稳定的概率较小

当一个哨兵判断主节点为主观下线后，就会向其他哨兵发起命令，其他哨兵收到这个命令后，就会根据自身和主节点的网络状况，做出赞成投票或者拒绝投票的响应

当这个哨兵的赞同票数（包括哨兵自己的一张赞成票）达到哨兵配置文件中的 quorum 配置项设定的值后，这时主节点就会被该哨兵标记为客观下线，quorum 的值一般设置为哨兵个数的二分之一加 1，例如 3 个哨兵就设置 2

哨兵判断完主节点客观下线后，哨兵就要开始在多个从节点中，选出一个从节点来做新主节点。

##### 选举

在哨兵集群中选出一个 leader，让 leader 来执行主从切换

哪个哨兵节点判断主节点为客观下线，这个哨兵节点就是候选者，所谓的候选者就是想当 Leader 的哨兵

候选者会向其他哨兵发送命令，表明希望成为 Leader 来执行主从切换，并让所有其他哨兵对它进行投票。每个哨兵只有一次投票机会，如果用完后就不能参与投票了，可以投给自己或投给别人，但是只有候选者才能把票投给自己

要满足两个条件：

- 第一，拿到半数以上的赞成票；
- 第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值

不满足就重新选举

若有多个哨兵判断主节点客观下线，有多个候选者：每位候选者都会先给自己投一票，然后向其他哨兵发起投票请求。如果投票者先收到候选者 A 的投票请求，就会先投票给它，如果投票者用完投票机会后，收到候选者 B 的投票请求后，就会拒绝投票。这时，候选者 A 先满足了上面的那两个条件，所以候选者 A 就会被选举为 Leader

如果 3 个哨兵节点，挂了 1 个：仍然有机会选举成功。同理 5 个挂 2 个也可以

如果 3 个哨兵节点，挂了 2 个：人为介入

##### 主从故障转移

- 第一步：在已下线主节点（旧主节点）属下的所有从节点里面，挑选出一个从节点，并将其转换为主节点。
- 第二步：让已下线主节点属下的所有从节点修改复制目标，修改为复制新主节点；
- 第三步：将新主节点的 IP 地址和信息，通过发布者/订阅者机制通知给客户端；
- 第四步：继续监视旧主节点，当这个旧主节点重新上线时，将它设置为新主节点的从节点

###### 选新主节点

把网络状态不好的从节点给过滤掉。首先把已经下线的从节点过滤掉，然后把以往网络连接状态不好的从节点也给过滤掉

down-after-milliseconds * 10 配置项，其 down-after-milliseconds 是主从节点断连的最大连接超时时间。如果在 down-after-milliseconds 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连了。如果发生断连的次数超过了 10 次，就说明这个从节点的网络状况不好，不适合作为新主节点

对所有从节点进行三轮考察：优先级、复制进度、ID 号。在进行每一轮考察的时候，哪个从节点优先胜出，就选择其作为新主节点。

- 第一轮考察：哨兵首先会根据从节点的优先级来进行排序，优先级越小排名越靠前，
- 第二轮考察：如果优先级相同，则查看复制的下标，哪个从主节点接收的复制数据多，哪个就靠前。
- 第三轮考察：如果优先级和下标都相同，就选择从节点 ID 较小的那个

slave-priority 配置项，可以给从节点设置优先级。每一台从节点的服务器配置不一定是相同的，我们可以根据服务器性能配置来设置从节点的优先级

- 在选举出从节点后，哨兵 leader 向被选中的从节点发送 `SLAVEOF no one` 命令，让这个从节点解除从节点的身份，将其变为新主节点。
- 以每秒一次的频率向被升级的从节点发送 `INFO` 命令（没进行故障转移之前，`INFO` 命令的频率是每十秒一次），并观察命令回复中的角色信息，当被升级节点的角色信息从原来的 slave 变为 master 时，哨兵 leader 就知道被选中的从节点已经顺利升级为主节点了

###### 从节点指向

让已下线主节点属下的所有从节点指向新主节点，这一动作可以通过向从节点发送 `SLAVEOF` 命令来实现(`slaveof ip port`)

###### 通知客户

Redis 的发布者/订阅者机制来实现的。每个哨兵节点提供发布者/订阅者机制，客户端可以从哨兵订阅消息

哨兵提供的消息订阅频道有很多，不同频道包含了主从节点切换过程中的不同关键事件，几个常见的事件如下：

<img src="img/哨兵频道.webp" alt="img" style="zoom:33%;" />

客户端和哨兵建立连接后，客户端会订阅哨兵提供的频道。主从切换完成后，哨兵就会向 `+switch-master` 频道发布新主节点的 IP 地址和端口的消息，这个时候客户端就可以收到这条信息，然后用这里面的新主节点的 IP 地址和端口进行通信了

客户端不仅可以在主从切换后得到新主节点的连接信息，还可以监控到主从节点切换过程中发生的各个重要事件。这样，客户端就可以知道主从切换进行到哪一步了，有助于了解切换进度

###### 旧主转从

继续监视旧主节点，当旧主节点重新上线时，哨兵集群就会向它发送 `SLAVEOF` 命令，让它成为新主节点的从节点

##### 哨兵集群

设置主节点名字、主节点的 IP 地址和端口号以及 quorum 值

```
sentinel monitor <master-name> <ip> <redis-port> <quorum> 
```

哨兵节点之间是通过 Redis 的发布者/订阅者机制来相互发现的。

> Redis 的发布者/订阅者（pub/sub）机制是一种消息通信模式，支持发送者（发布者）和接收者（订阅者）之间的消息传递，而不需要发布者和订阅者直接相互了解。这种机制允许构建松耦合的、高度可扩展的交互系统
>
> 发布者：发布者发送消息到Redis的一个频道或者多个频道，但不关心谁在接收。
>
> 订阅者：订阅者订阅一个或多个频道，接收来自这些频道的消息。
>
> 频道：消息传递的媒介。发布者向频道发送消息，订阅了该频道的订阅者会接收到这些消息
>
> 特点：
>
> - 非持久性：所有通过发布者/订阅者机制发送的消息都不会被Redis持久化，也就是说，如果没有订阅者在接收消息，这些消息会被丢弃。
> - 无状态：Redis不维护任何关于已连接客户端订阅什么的状态信息。如果客户端断开连接，所有订阅信息都会丢失，需要重新订阅
> - 缺乏消息确认：发布的消息不会有任何确认机制，发布者不会知道哪些订阅者实际接收到了消息

在主从集群中，主节点上有一个名为`__sentinel__:hello`的频道，不同哨兵就是通过它来相互发现，实现互相通信的

哨兵 A 把自己的 IP 地址和端口的信息发布到`__sentinel__:hello` 频道上，哨兵 B 和 C 订阅了该频道。那么此时，哨兵 B 和 C 就可以从这个频道直接获取哨兵 A 的 IP 地址和端口号。然后，哨兵 B、C 可以和哨兵 A 建立网络连接

主节点知道所有从节点的信息，所以哨兵会每 10 秒一次的频率向主节点发送 INFO 命令来获取所有从节点的信息。哨兵就可以根据从节点列表中的连接信息，和每个从节点建立连接，并在这个连接上持续地对从节点进行监控

### 内存淘汰

[参考](https://www.xiaolincoding.com/redis/base/redis_interview.html#redis-%E8%BF%87%E6%9C%9F%E5%88%A0%E9%99%A4%E4%B8%8E%E5%86%85%E5%AD%98%E6%B7%98%E6%B1%B0) [参考](https://www.xiaolincoding.com/redis/module/strategy.html)

#### 过期删除

##### 概述

对一个 key 设置了过期时间时，Redis 会把该 key(键指针) 带上过期时间(long long值)存储到一个过期字典（expires dict）

查询一个 key 时，Redis 首先检查该 key 是否存在于过期字典中：

- 如果不在，则正常读取键值；
- 如果存在，则会获取该 key 的过期时间，然后与当前系统时间进行比对，如果比系统时间大，那就没有过期，否则判定该 key 已过期

策略：惰性删除+定期删除

- 惰性：不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key (源码 `db.c` 的 `expireIfNeeded`)

  根据 `lazyfree_lazy_expire` 参数配置决定异步还是同步删除（Redis 4.0版本开始提供参数）

  优点：使用很少的系统资源，因此，惰性删除策略对 CPU 时间最友好

  缺点：过期 key 内存不释放从而浪费，内存不友好

- 定期：每隔一段时间随机从数据库中取出一定数量的 key 进行检查，并删除其中的过期key

  具体而言：

  1. 从过期字典中随机抽取 20 个 key；
  2. 检查这 20 个 key 是否过期，并删除已过期的 key；
  3. 如果本轮检查的已过期 key 的数量，超过 5 个（20/4），也就是已过期 key 的数量占比随机抽取 key 的数量大于 25%，则继续重复步骤 1；如果已过期的 key 比例小于 25%，则停止继续删除过期 key，然后等待下一轮再检查

  保证定期删除不会出现循环过度，导致线程卡死现象，为此增加了定期删除循环流程的时间上限，默认不会超过 25ms

  优点：限制删除操作执行的时长和频率，来减少删除操作对 CPU 的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用

  缺点：难以确定删除操作执行的时长和频率。如果执行的太频繁，就会对 CPU 不友好；如果执行的太少，那又和惰性删除一样了，过期 key 占用的内存不会及时得到释放

##### 过期时间指令

设置 key 过期时间的命令一共有 4 个：

- `expire <key> <n>`：设置 key 在 n 秒后过期，比如 expire key 100 表示设置 key 在 100 秒后过期；
- `pexpire <key> <n>`：设置 key 在 n 毫秒后过期，比如 pexpire key2 100000 表示设置 key2 在 100000 毫秒（100 秒）后过期。
- `expireat <key> <n>`：设置 key 在某个时间戳（精确到秒）之后过期，比如 expireat key3 1655654400 表示 key3 在时间戳 1655654400 后过期（精确到秒）；
- `pexpireat <key> <n>`：设置 key 在某个时间戳（精确到毫秒）之后过期，比如 pexpireat key4 1655654400000 表示 key4 在时间戳 1655654400000 后过期（精确到毫秒）

当然，在设置字符串时，也可以同时对 key 设置过期时间，共有 3 种命令：

- `set <key> <value> ex <n>` ：设置键值对的时候，同时指定过期时间（精确到秒）；
- `set <key> <value> px <n>` ：设置键值对的时候，同时指定过期时间（精确到毫秒）；
- `setex <key> <n> <valule>` ：设置键值对的时候，同时指定过期时间（精确到秒）。

如果你想查看某个 key 剩余的存活时间，可以使用 `TTL <key>` 命令。

```bash
# 设置键值对的时候，同时指定过期时间位 60 秒
> setex key1 60 value1
OK

# 查看 key1 过期时间还剩多少
> ttl key1
(integer) 56
> ttl key1
(integer) 52
```

如果突然反悔，取消 key 的过期时间，则可以使用 `PERSIST <key>` 命令。

```bash
# 取消 key1 的过期时间
> persist key1
(integer) 1

# 使用完 persist 命令之后，
# 查下 key1 的存活时间结果是 -1，表明 key1 永不过期 
> ttl key1 
(integer) -1
```

##### 持久化

RDB：

- RDB 文件生成阶段：从内存状态持久化成 RDB（文件）的时候，会对 key 进行过期检查，过期的键不会被保存到新的 RDB 文件中，因此 Redis 中的过期键不会对生成新 RDB 文件产生任何影响。
- RDB 加载阶段：分别对应以下两种情况：
  - 如果 Redis 是主服务器运行模式的话，在载入 RDB 文件时，程序会对文件中保存的键进行检查，过期键不会被载入到数据库中。所以过期键不会对载入 RDB 文件的主服务器造成影响；
  - 如果 Redis 是从服务器运行模式的话，在载入 RDB 文件时，不论键是否过期都会被载入到数据库中。但由于主从服务器在进行数据同步时，从服务器的数据会被清空。所以一般来说，过期键对载入 RDB 文件的从服务器也不会造成影响。

AOF：

- AOF 文件写入阶段：当 Redis 以 AOF 模式持久化时，如果数据库某个过期键还没被删除，那么 AOF 文件会保留此过期键，当此过期键被删除后，Redis 会向 AOF 文件追加一条 DEL 命令来显式地删除该键值
- AOF 重写阶段：执行 AOF 重写时，会对 Redis 中的键值对进行检查，已过期的键不会被保存到重写后的 AOF 文件中，因此不会对 AOF 重写造成任何影响

##### 主从模式

当 Redis 运行在主从模式下时，从库不会进行过期扫描，从库对过期的处理是被动的。也就是即使从库中的 key 过期了，如果有客户端访问从库时，依然可以得到 key 对应的值，像未过期的键值对一样返回。

从库的过期键处理依靠主服务器控制，主库在 key 到期时，会在 AOF 文件里增加一条 del 指令，同步到所有的从库，从库通过执行这条 del 指令来删除过期的 key

#### 内存淘汰

##### 最大运行内存

`redis.conf` 的 `mamemory <bytes>`

- 在 64 位操作系统中，maxmemory 的默认值是 0，表示没有内存大小限制，那么不管用户存放多少数据到 Redis 中，Redis 也不会对可用内存进行检查，直到 Redis 实例因内存不足而崩溃也无作为。
- 在 32 位操作系统中，maxmemory 的默认值是 3G，因为 32 位的机器最大只支持 4GB 的内存

##### 概述

在 Redis 的运行内存达到了配置项为 maxmemory 设置的最大运行内存的阀值

八种策略：

1. noeviction Redis3.0之后，默认的内存淘汰策略

   超过最大设置内存时，不淘汰任何数据，而是不再提供服务，直接返回错误

2. volatile-random：随机淘汰设置了过期时间的任意键值

3. volatile-ttl：优先淘汰更早过期的键值

4. volatile-lru（Redis3.0 之前，默认的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最久未使用的键值

5. volatile-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最少使用的键值

6. allkeys-random：随机淘汰任意键值;

7. allkeys-lru：淘汰整个键值中最久未使用的键值；

8. allkeys-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰整个键值中最少使用的键值。

查看当前内存淘汰策略

```
127.0.0.1:6379> config get maxmemory-policy
1) "maxmemory-policy"
2) "noeviction"
```

修改：

- 临时生效，重启还原

  ```
  config set maxmemory-policy <策略>
  ```

- 配置文件 `maxmeory-policy <策略>`

##### LRU

Least Recently Used

传统 LRU 算法的实现是基于链表结构，链表中的元素按照操作顺序从前往后排列，最新操作的键会被移动到表头，当需要内存淘汰时，只需要删除链表尾部的元素即可，因为链表尾部的元素就代表最久未被使用的元素，缺点：

- 链表空间开销大
- 大量数据访问时链表移动耗时

Redis 实现近似 LRU 算法，更好节约内存：在 Redis 的对象结构体中添加一个额外的字段，用于记录此数据的最后一次访问时间

当 Redis 进行内存淘汰时，会使用随机采样的方式来淘汰数据，它是随机取 5 个值（此值可配置），然后淘汰最久没有使用的那个

缺点：无法解决缓存污染问题，比如应用一次读取了大量的数据，而这些数据只会被读取这一次，那么这些数据会留存在 Redis 缓存中很长一段时间，造成缓存污染

##### LFU

Least Frequently Used

核心思想是“如果数据过去被访问多次，那么将来被访问的频率也更高”

LFU 算法相比于 LRU 算法的实现，多记录了数据的访问频次

在 LRU 算法中，Redis 对象头的 24 bits 的 lru 字段是用来记录 key 的访问时间戳，因此在 LRU 模式下，Redis可以根据对象头中的 lru 字段记录的值，来比较最后一次 key 的访问时间长，从而淘汰最久未被使用的 key。

在 LFU 算法中，Redis对象头的 24 bits 的 lru 字段被分成两段来存储，高 16bit 存储 ldt(Last Decrement Time)，用来记录 key 的访问时间戳；低 8bit 存储 logc(Logistic Counter)，用来记录 key 的访问频次，它的值越小表示使用频率越低，越容易淘汰，每个新加入的 key 的logc 初始值为 5。

logc 并不是单纯的访问次数，而是访问频次（访问频率），每次 key 被访问时，会先对 logc 做一个衰减操作，衰减的值跟前后访问时间的差距有关系，如果上一次访问的时间与这一次访问的时间差距很大，那么衰减的值就越大。对 logc 做完衰减操作后，就开始对 logc 进行增加操作，增加操作并不是单纯的 + 1，而是根据概率增加，如果 logc 越大的 key，它的 logc 就越难再增加

两个配置项，用于调整 LFU 算法从而控制 logc 的增长和衰减：

- `lfu-decay-time` 用于调整 logc 的衰减速度，它是一个以分钟为单位的数值，默认值为1，lfu-decay-time 值越大，衰减越慢；
- `lfu-log-factor` 用于调整 logc 的增长速度，lfu-log-factor 值越大，logc 增长越慢

### 缓存设计

[参考](https://www.xiaolincoding.com/redis/base/redis_interview.html#redis-%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1)

#### 概念

缓存比数据库快，因为数据库结果有可能是底层数据表的聚合等复杂操作，且读盘等缘故。

##### 缓冲雪崩

大量缓存数据在同一时间过期（失效）时，如果此时有大量的用户请求，都无法在 Redis 中处理，于是全部请求都直接访问数据库，从而导致数据库的压力骤增，严重的会造成数据库宕机，从而形成一系列连锁反应，造成整个系统崩溃，这就是缓存雪崩

可能的原因：①大量数据同时过期；②宕机

解决策略：

- 将缓存失效时间随机打散： 我们可以在原有的失效时间基础上增加一个随机值（比如 1 到 10 分钟）

- 设置缓存不过期：后台服务来更新缓存数据

  当系统内存紧张的时候，有些缓存数据会被“淘汰”，而在缓存被“淘汰”到下一次后台定时更新缓存的这段时间内，业务线程读取缓存失败就返回空值，业务的视角就以为是数据丢失了

  解决：

  - 后台线程不仅负责定时更新缓存，而且也负责频繁地检测缓存是否有效，检测到缓存失效了，原因可能是系统紧张而被淘汰的，于是就要马上从数据库读取数据，并更新到缓存
  - 业务线程发现缓存数据失效后（缓存数据被淘汰），通过消息队列发送一条消息通知后台线程更新缓存，后台线程收到消息后，在更新缓存前可以判断缓存是否存在，存在就不执行更新缓存操作；不存在就读取数据库数据，并将数据加载到缓存。这种方式相比第一种方式缓存的更新会更及时，用户体验也比较好

- 互斥锁：见下文

在业务刚上线的时候，我们最好提前把数据缓起来，而不是等待用户访问才来触发缓存构建，这就是所谓的缓存预热，后台更新缓存的机制刚好也适合干这个事情

##### 缓存击穿

业务通常会有几个数据会被频繁地访问，比如秒杀活动，这类被频地访问的数据被称为热点数据。如果缓存中的某个热点数据过期了，此时大量的请求访问了该热点数据，就无法从缓存中读取，直接访问数据库，数据库很容易就被高并发的请求冲垮，这就是缓存击穿

缓存击穿是缓存雪崩的一个子集。应对缓存击穿可以采取前面说到两种方案：

- 互斥锁方案（Redis 中使用 setNX 方法设置一个状态位，表示这是一种锁定状态），保证同一时间只有一个业务线程请求缓存，未能获取互斥锁的请求，要么等待锁释放后重新读取缓存，要么就返回空值或者默认值

  实现互斥锁的时候，最好设置超时时间，不然第一个请求拿到了锁，然后这个请求发生了某种意外而一直阻塞，一直不释放锁，这时其他请求也一直拿不到锁，整个系统就会出现无响应的现象

- 不给热点数据设置过期时间，由后台异步更新缓存，或者在热点数据准备要过期前，提前通知后台线程更新缓存以及重新设置过期时间

##### 缓存穿透

当用户访问的数据，既不在缓存中，也不在数据库中，导致请求在访问缓存时，发现缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据，没办法构建缓存数据，来服务后续的请求。那么当有大量这样的请求到来时，数据库的压力骤增，这就是缓存穿透

缓存穿透的发生一般有这两种情况：

- 业务误操作，缓存中的数据和数据库中的数据都被误删除了，所以导致缓存和数据库中都没有数据；
- 黑客恶意攻击，故意大量访问某些读取不存在数据的业务

应对：

- 非法请求限制：在 API 入口处我们要判断求请求参数是否合理，请求参数是否含有非法值、请求字段是否存在，如果判断出是恶意请求就直接返回错误

- 设置空值或者默认值

- 使用布隆过滤器快速判断数据是否存在，避免通过查询数据库来判断数据是否存在

  写入数据库数据时，使用布隆过滤器做个标记，然后在用户请求到来时，业务线程确认缓存失效后，可以通过查询布隆过滤器快速判断数据是否存在，如果不存在，就不用通过查询数据库来判断数据是否存在，即使发生了缓存穿透，大量请求只会查询 Redis 和布隆过滤器，而不会查询数据库，保证了数据库能正常运行，Redis 自身也是支持布隆过滤器

##### 缓存策略

由于数据存储受限，系统并不是将所有数据都需要存放到缓存中的，而只是将其中一部分热点数据缓存起来

总体思路：通过数据最新访问时间来做排名，并过滤掉不常访问的数据，只留下经常访问的数据

> 以电商平台场景中的例子，现在要求只缓存用户经常访问的 Top 1000 的商品。具体细节如下：
>
> - 先通过缓存系统做一个排序队列（比如存放 1000 个商品），系统会根据商品的访问时间，更新队列信息，越是最近访问的商品排名越靠前；
> - 同时系统会定期过滤掉队列中排名最后的 200 个商品，然后再从数据库中随机读取出 200 个商品加入队列中；
> - 这样当请求每次到达的时候，会先从队列中获取商品 ID，如果命中，就根据 ID 再从另一个缓存数据结构中读取实际的商品信息，并返回。
>
> 在 Redis 中可以用 zadd 方法和 zrange 方法来完成排序队列和获取 200 个商品的操作

##### 服务熔断

Redis 故障宕机而引发的缓存雪崩问题，常见的应对方法有下面这几种：

- 服务熔断或请求限流机制；
- 构建 Redis 缓存高可靠集群

服务熔断机制，暂停业务应用对缓存服务的访问，直接返回错误，不用再继续访问数据库，从而降低对数据库的访问压力，保证数据库系统的正常运行，然后等到 Redis 恢复正常后，再允许业务应用访问缓存服务

请求限流机制，只将少部分请求发送到数据库进行处理，再多的请求就在入口直接拒绝服务，等到 Redis 恢复正常并把缓存预热完后，再解除请求限流的机制

如果 Redis 缓存的主节点故障宕机，从节点可以切换成为主节点，继续提供缓存服务，避免了由于 Redis 故障宕机而导致的缓存雪崩问题

#### 缓存更新策略

##### 概述

常见的缓存更新策略共有3种：

- Cache Aside（旁路缓存）策略；
- Read/Write Through（读穿 / 写穿）策略；
- Write Back（写回）策略

实际开发中，Redis 和 MySQL 的更新策略用的是 Cache Aside

##### 旁路缓存

最常用的，应用程序直接与数据库、缓存交互，并负责对缓存的维护，该策略又可以细分为读策略和写策略

<img src="img/6e3db3ba2f829ddc14237f5c7c00e7ce-20230309232338149.png" alt="img" style="zoom:50%;" />

写：先更新数据库中的数据，再删除缓存中的数据。

读：

- 如果读取的数据命中了缓存，则直接返回数据；
- 如果读取的数据没有命中缓存，则从数据库中读取数据，然后将数据写入到缓存，并且返回给用户。

写策略的步骤的顺序不能倒过来，即不能先删除缓存再更新数据库，原因是在读+写并发的时候，会出现缓存和数据库的数据不一致性的问题

> 举个例子，假设某个用户的年龄是 20，请求 A 要更新用户年龄为 21，所以它会删除缓存中的内容。这时，另一个请求 B 要读取这个用户的年龄，它查询缓存发现未命中后，会从数据库中读取到年龄为 20，并且写入到缓存中，然后请求 A 继续更改数据库，将用户的年龄更新为 21。最终，该用户年龄在缓存中是 20（旧值），在数据库中是 21（新值），缓存和数据库的数据不一致
>
> <img src="img/cc208c2931b4e889d1a58cb655537767-20230309232342573.png" alt="img" style="zoom:50%;" />
>
> 修改后：也会出现不一致的可能性
>
> <img src="img/1cc7401143e79383ead96582ac11b615-20230309232407419.png" alt="img" style="zoom:50%;" />
>
> 该可能性不大，因为缓存写入远快于数据库写入
>
> 而一旦请求 A 早于请求 B 删除缓存之前更新了缓存，那么接下来的请求就会因为缓存不命中而从数据库中重新读取数据，所以不会出现这种不一致的情况
>
> 如果写是先更数据库和更缓存的话：
>
> 无论是先更新数据库，再更新缓存，还是先更新缓存，再更新数据库，这两个方案都存在并发问题，当两个请求并发更新同一条数据的时候，可能会出现缓存和数据库中的数据不一致的现象，并且概率不低 [参考](https://www.xiaolincoding.com/redis/architecture/mysql_redis_consistency.html#%E5%85%88%E6%9B%B4%E6%96%B0%E6%95%B0%E6%8D%AE%E5%BA%93-%E8%BF%98%E6%98%AF%E5%85%88%E6%9B%B4%E6%96%B0%E7%BC%93%E5%AD%98)
>
> <img src="img/8febac10b14bed16cb96d1d944cd08da.png" alt="图片" style="zoom: 25%;" /><img src="https://cdn.xiaolincoding.com//mysql/other/454a8228a6549176ad7e0484fba3c92b.png" alt="图片" style="zoom:25%;" />
>
> 删除一个数据，相比更新一个数据更加轻量级，出问题的概率更小。删除缓存而不是更新缓存，就是懒加载思想的一个应用

Cache Aside 策略适合读多写少的场景，不适合写多的场景，因为当写入比较频繁时，缓存中的数据会被频繁地清理，这样会对缓存的命中率有一些影响。如果业务对缓存命中率有严格的要求，那么可以考虑两种解决方案：

- 一种做法是在更新数据时也更新缓存，只是在更新缓存前先加一个分布式锁，因为这样在同一时间只允许一个线程更新缓存，就不会产生并发问题了。当然这么做对于写入的性能会有一些影响；
- 另一种做法同样也是在更新数据时更新缓存，只是给缓存加一个较短的过期时间，这样即使出现缓存不一致的情况，缓存的数据也会很快过期，对业务的影响也是可以接受

如果反过来，即先删缓存，则使用延迟双删策略：

```
#删除缓存
redis.delKey(X)
#更新数据库
db.update(X)
#睡眠
Thread.sleep(N)
#再删除缓存
redis.delKey(X)
```

但是具体睡眠多久其实是个玄学，很难评估出来，所以这个方案也只是尽可能保证一致性而已，极端情况下，依然也会出现缓存不一致的现象。所以一般推荐先更新数据库。

如果数据库和缓存其中一个操作失败了：

- 重试机制

  将第二个操作（删除缓存）加入消息队列

  若删除缓存失败，消息队列可以读取到该操作并再次删除；多次失败就上报错误到业务层

  删除成功，移除出消息队列，避免重复操作

- 订阅 MySQL binlog 再操作缓存

  更新数据库成功，就会产生一条变更日志，记录在 binlog 里

  订阅 binlog 日志，拿到具体要操作的数据，然后再执行缓存删除，阿里巴巴开源的 Canal 中间件就是基于这个实现的

  Canal 模拟 MySQL 主从复制的交互协议，把自己伪装成一个 MySQL 的从节点，向 MySQL 主节点发送 dump 请求，MySQL 收到请求后，就会开始推送 Binlog 给 Canal，Canal 解析 Binlog 字节流之后，转换为便于读取的结构化数据，供下游程序订阅使用

这两种方法有一个共同的特点，都是采用异步操作缓存

##### 读穿/写穿策略

应用程序只和缓存交互，不再和数据库交互，而是由缓存和数据库交互，相当于更新数据库的操作由缓存自己代理了

读穿：先查询缓存中数据是否存在，如果存在则直接返回，如果不存在，则由缓存组件负责从数据库查询数据，并将结果写入到缓存组件，最后缓存组件将数据返回给应用

写穿：

- 如果缓存中数据已经存在，则更新缓存中的数据，并且由缓存组件同步更新到数据库中，然后缓存组件告知应用程序更新完成。
- 如果缓存中数据不存在，直接更新数据库，然后返回

无论是 Memcached 还是 Redis 都不提供写入数据库和自动加载数据库中的数据的功能

##### 写回策略

在更新数据的时候，只更新缓存，同时将缓存数据设置为脏的，然后立马返回，并不会更新数据库。对于数据库的更新，会通过批量异步更新的方式进行

Redis 并没有异步更新数据库的功能

Write Back 是计算机体系结构中的设计，比如 CPU 的缓存、操作系统中文件系统的缓存都采用了 Write Back（写回）策略

Write Back 策略特别适合写多的场景，因为发生写操作的时候， 只需要更新缓存，就立马返回了。比如，写文件的时候，实际上是写入到文件系统的缓存就返回了，并不会写磁盘

但是带来的问题是，数据不是强一致性的，而且会有数据丢失的风险，因为缓存一般使用内存，而内存是非持久化的，所以一旦缓存机器掉电，就会造成原本缓存中的脏数据丢失。所以你会发现系统在掉电之后，之前写入的文件会有部分丢失，就是因为 Page Cache 还没有来得及刷盘造成的

### 杂项

[参考](https://www.xiaolincoding.com/redis/base/redis_interview.html#redis-%E5%AE%9E%E6%88%98)

#### 延迟队列

延迟队列是指把当前要做的事情，往后推迟一段时间再做。延迟队列的常见使用场景有以下几种：

- 在淘宝、京东等购物平台上下单，超过一定时间未付款，订单会自动取消；（取消操作延迟一段时间做）
- 打车的时候，在规定时间没有车主接单，平台会取消你的单并提醒你暂时没有车主接单；
- 点外卖的时候，如果商家在10分钟还没接单，就会自动取消订单；

在 Redis 可以使用有序集合（ZSet）的方式来实现延迟消息队列的，ZSet 有一个 Score 属性可以用来存储延迟执行的时间

在 Redis 可以使用有序集合（ZSet）的方式来实现延迟消息队列的，ZSet 有一个 Score 属性可以用来存储延迟执行的时间。

使用 zadd score1 value1 命令就可以一直往内存中生产消息。再利用 zrangebysocre 查询符合条件的所有待处理的任务， 通过循环执行队列任务即可

<img src="img/延迟队列.png" alt="img" style="zoom: 67%;" />

#### 大key

##### 概念

一般而言，下面这两种情况被称为大 key：

- String 类型的值大于 10 KB；
- Hash、List、Set、ZSet 类型的元素的个数超过 5000个

大 key 会带来以下四种影响：

- 客户端超时阻塞。由于 Redis 执行命令是单线程处理，然后在操作大 key 时会比较耗时，那么就会阻塞 Redis，从客户端这一视角看，就是很久很久都没有响应。
- 引发网络阻塞。每次获取大 key 产生的网络流量较大，如果一个 key 的大小是 1 MB，每秒访问量为 1000，那么每秒会产生 1000MB 的流量，这对于普通千兆网卡的服务器来说是灾难性的。
- 阻塞工作线程。如果使用 del 删除大 key 时，会阻塞工作线程，这样就没办法处理后续的命令。
- 内存分布不均。集群模型在 slot 分片均匀情况下，会出现数据和查询倾斜情况，部分有大 key 的 Redis 节点占用内存多，QPS 也会比较大

最好在设计阶段，就把大 key 拆分成一个一个小 key。或者，定时检查 Redis 是否存在大 key ，如果该大 key 是可以删除的，不要使用 DEL 命令删除，因为该命令删除过程会阻塞主线程，而是用 unlink 命令（Redis 4.0+）删除大 key，因为该命令的删除过程是异步的，不会阻塞主线程。

##### 查询

###### cli

可以通过 redis-cli --bigkeys 命令查找大 key

```SH
redis-cli -h 127.0.0.1 -p6379 -a "password" -- bigkeys
```

使用的时候注意事项：

- 最好选择在从节点上执行该命令。因为主节点上执行时，会阻塞主节点；
- 如果没有从节点，那么可以选择在 Redis 实例业务压力的低峰阶段进行扫描查询，以免影响到实例的正常运行；或者可以使用 -i 参数控制扫描间隔，避免长时间扫描降低 Redis 实例的性能

该方式的不足之处：

- 这个方法只能返回每种类型中最大的那个 bigkey，无法得到大小排在前 N 位的 bigkey；
- 对于集合类型来说，这个方法只统计集合元素个数的多少，而不是实际占用的内存量。但是，一个集合中的元素个数多，并不一定占用的内存就多

###### SCAN

使用 SCAN 命令对数据库扫描，然后用 TYPE 命令获取返回的每一个 key 的类型。

对于 String 类型，可以直接使用 STRLEN 命令获取字符串的长度，也就是占用的内存空间字节数。

对于集合类型来说，有两种方法可以获得它占用的内存大小：

- 如果能够预先从业务层知道集合元素的平均大小，那么，可以使用下面的命令获取集合元素的个数，然后乘以集合元素的平均大小，这样就能获得集合占用的内存大小了。List 类型：`LLEN` 命令；Hash 类型：`HLEN` 命令；Set 类型：`SCARD` 命令；Sorted Set 类型：`ZCARD` 命令；
- 如果不能提前知道写入集合的元素大小，可以使用 `MEMORY USAGE` 命令（需要 Redis 4.0 及以上版本），查询一个键值对占用的内存空间

###### RdbTools

使用 RdbTools 第三方开源工具，可以用来解析 Redis 快照（RDB）文件，找到其中的大 key。

比如，下面这条命令，将大于 10 kb 的  key  输出到一个表格文件

```sh
rdb dump.rdb -c memory --bytes 10240 -f redis.csv
```

##### 删除

删除操作的本质是要释放键值对占用的内存空间

在应用程序释放内存时，操作系统需要把释放掉的内存块插入一个空闲内存块的链表，以便后续进行管理和再分配。这个过程本身需要一定时间，而且会阻塞当前释放内存的应用程序 -> 相应地就会造成 Redis 主线程的阻塞，如果主线程发生了阻塞，其他所有请求可能都会超时，超时越来越多，会造成 Redis 连接耗尽，产生各种异常

###### 分批次删除

对于删除大 Hash，使用 `hscan` 命令，每次获取 100 个字段，再用 `hdel` 命令，每次删除 1 个字段

```python
def del_large_hash():
  r = redis.StrictRedis(host='redis-host1', port=6379)
    large_hash_key ="xxx" #要删除的大hash键名
    cursor = '0'
    while cursor != 0:
        # 使用 hscan 命令，每次获取 100 个字段
        cursor, data = r.hscan(large_hash_key, cursor=cursor, count=100)
        for item in data.items():
                # 再用 hdel 命令，每次删除1个字段
                r.hdel(large_hash_key, item[0])
```

对于删除大 List，通过 `ltrim` 命令，每次删除少量元素。

```python
def del_large_list():
  r = redis.StrictRedis(host='redis-host1', port=6379)
  large_list_key = 'xxx'  #要删除的大list的键名
  while r.llen(large_list_key)>0:
      #每次只删除最右100个元素
      r.ltrim(large_list_key, 0, -101) 
```

对于删除大 Set，使用 `sscan` 命令，每次扫描集合中 100 个元素，再用 `srem` 命令每次删除一个键

```python
def del_large_set():
  r = redis.StrictRedis(host='redis-host1', port=6379)
  large_set_key = 'xxx'   # 要删除的大set的键名
  cursor = '0'
  while cursor != 0:
    # 使用 sscan 命令，每次扫描集合中 100 个元素
    cursor, data = r.sscan(large_set_key, cursor=cursor, count=100)
    for item in data:
      # 再用 srem 命令每次删除一个键
      r.srem(large_size_key, item)
```

对于删除大 ZSet，使用 `zremrangebyrank` 命令，每次删除 top 100个元素

```python
def del_large_sortedset():
  r = redis.StrictRedis(host='large_sortedset_key', port=6379)
  large_sortedset_key='xxx'
  while r.zcard(large_sortedset_key)>0:
    # 使用 zremrangebyrank 命令，每次删除 top 100个元素
    r.zremrangebyrank(large_sortedset_key,0,99) 
```

###### 异步删除

从 Redis 4.0 版本开始，可以采用异步删除法，用 unlink 命令代替 del 来删除。

这样 Redis 会将这个 key 放入到一个异步线程中进行删除，这样不会阻塞主线程。

除了主动调用 unlink 命令实现异步删除之外，我们还可以通过配置参数，达到某些条件的时候自动进行异步删除。

主要有 4 种场景，默认都是关闭的

- lazyfree-lazy-eviction：表示当 Redis 运行内存超过 maxmeory 时，是否开启 lazy free 机制删除；
- lazyfree-lazy-expire：表示设置了过期时间的键值，当过期之后是否开启 lazy free 机制删除；
- lazyfree-lazy-server-del：有些指令在处理已存在的键时，会带有一个隐式的 del 键的操作，比如 rename 命令，当目标键已存在，Redis 会先删除目标键，如果这些目标键是一个 big key，就会造成阻塞删除的问题，此配置表示在这种场景中是否开启 lazy free 机制删除；
- slave-lazy-flush：针对 slave (从节点) 进行全量数据同步，slave 在加载 master 的 RDB 文件前，会运行 flushall 来清理自己的数据，它表示此时是否开启 lazy free 机制删除

建议开启其中的 lazyfree-lazy-eviction、lazyfree-lazy-expire、lazyfree-lazy-server-del 等配置，这样就可以有效的提高主线程的执行效率



#### 管道

管道技术（Pipeline）是客户端提供的一种批处理技术，用于一次处理多个 Redis 命令，从而提高整个交互的性能

区别：(普通命令 vs 管道)

<img src="img/普通命令模式.jpg" alt="img" style="zoom:33%;" /> <img src="img/管道模式.jpg" alt="img" style="zoom:33%;" />

使用管道技术可以解决多个命令执行时的网络等待，它是把多个命令整合到一起发送给服务器端处理之后统一返回给客户端，这样就免去了每条命令执行后都要等待的情况，从而有效地提高了程序的执行效率。

但使用管道技术也要注意避免发送的命令过大，或管道内的数据太多而导致的网络阻塞。

要注意的是，管道技术本质上是客户端提供的功能，而非 Redis 服务器端的功能

#### 回滚

官方文档解释 [here](https://redis.io/docs/latest/develop/interact/transactions/) 不支持回滚（事务运行时错误的事务回滚）

- Redis 事务的执行时，错误通常都是编程错误造成的，这种错误通常只会出现在开发环境中，而很少会在实际的生产环境中出现，所以他认为没有必要为 Redis 开发事务回滚功能；
- 不支持事务回滚是因为这种复杂的功能和 Redis 追求的简单高效的设计主旨不符合

Redis 提供了 DISCARD 命令，但是这个命令只能用来主动放弃事务执行，把暂存的命令队列清空，起不到回滚的效果

```
#读取 count 的值4
127.0.0.1:6379> GET count
"1"
#开启事务
127.0.0.1:6379> MULTI 
OK
#发送事务的第一个操作，对count减1
127.0.0.1:6379> DECR count
QUEUED
#执行DISCARD命令，主动放弃事务
127.0.0.1:6379> DISCARD
OK
#再次读取a:stock的值，值没有被修改
127.0.0.1:6379> GET count
"1"
```

事务执行过程中，如果命令入队时没报错，而事务提交后，实际执行时报错了，正确的命令依然可以正常执行，所以这可以看出 Redis 并不一定保证原子性

```
#获取name原本的值
127.0.0.1:6379> GET name
"xiaolin"
#开启事务
127.0.0.1:6379> MULTI
OK
#设置新值
127.0.0.1:6379(TX)> SET name xialincoding
QUEUED
#注意，这条命令是错误的
# expire 过期时间正确来说是数字，并不是‘10s’字符串，但是还是入队成功了
127.0.0.1:6379(TX)> EXPIRE name 10s
QUEUED
#提交事务，执行报错
#可以看到 set 执行成功，而 expire 执行错误。
127.0.0.1:6379(TX)> EXEC
1) OK
2) (error) ERR value is not an integer or out of range
#可以看到，name 还是被设置为新值了
127.0.0.1:6379> GET name
"xialincoding"
```

#### 分布式锁

##### 概念

分布式锁是用于分布式环境下并发控制的一种机制，用于控制某个资源在同一时刻只能被一个应用所使用

可以用 redis 的 key-value 当成分布式锁

<img src="img/分布式锁.jpg" alt="img" style="zoom: 33%;" />

##### 实现

Redis 的 SET 命令有个 NX 参数可以实现key不存在才插入，所以可以用它来实现分布式锁：

- 如果 key 不存在，则显示插入成功，可以用来表示加锁成功；
- 如果 key 存在，则会显示插入失败，可以用来表示加锁失败

基于 Redis 节点实现分布式锁时，对于加锁操作，我们需要满足三个条件。

- 加锁包括了读取锁变量、检查锁变量值和设置锁变量值三个操作，但需要以原子操作的方式完成，所以，我们使用 SET 命令带上 NX 选项来实现加锁；
- 锁变量需要设置过期时间，以免客户端拿到锁后发生异常，导致锁一直无法释放，所以，我们在 SET 命令执行时加上 EX/PX 选项，设置其过期时间；
- 锁变量的值需要能区分来自不同客户端的加锁操作，以免在释放锁时，出现误释放操作，所以，我们使用 SET 命令设置锁变量值时，每个客户端设置的值是一个唯一值，用于标识客户端

```
SET lock_key unique_value NX PX 10000
```

- lock_key 就是 key 键；
- unique_value 是客户端生成的唯一的标识，区分来自不同客户端的锁操作；
- NX 代表只在 lock_key 不存在时，才对 lock_key 进行设置操作；
- PX 10000 表示设置 lock_key 的过期时间为 10s，这是为了避免客户端发生异常而无法释放锁

解锁的过程就是将 lock_key 键删除（del lock_key），但不能乱删，要保证执行操作的客户端就是加锁的客户端。所以，解锁的时候，我们要先判断锁的 unique_value 是否为加锁客户端，是的话，才将 lock_key 键删除

Redis 在执行 Lua 脚本时，可以以原子性的方式执行：

```lua
// 释放锁时，先比较 unique_value 是否相等，避免锁的误释放
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

##### 优缺点

优点：

1. 性能高效（这是选择缓存实现分布式锁最核心的出发点）。
2. 实现方便。Redis 提供了 setnx 方法
3. 避免单点故障（因为 Redis 是跨集群部署的，自然就避免了单点故障）

缺点：

- 超时时间不好设置。如果锁的超时时间设置过长，会影响性能，如果设置的超时时间过短会保护不到共享资源
- Redis 主从复制模式中的数据是异步复制的，这样导致分布式锁的不可靠性。如果在 Redis 主节点获取到锁后，在没有同步到其他节点时，Redis 主节点宕机了，此时新的 Redis 主节点依然可以获取锁，所以多个应用服务就可以同时获取到锁

##### 超时时间

可以基于续约的方式设置超时时间：先给锁设置一个超时时间，然后启动一个守护线程，让守护线程在一段时间后，重新设置这个锁的超时时间。实现方式就是：写一个守护线程，然后去判断锁的情况，当锁快失效的时候，再次进行续约加锁，当主线程执行完成后，销毁续约锁即可，不过这种方式实现起来相对复杂

##### 集群可靠性

官方已经设计了一个分布式锁算法 Redlock（红锁）

官方推荐是至少部署 5 个 Redis 节点，而且都是主节点，它们之间没有任何关系，都是一个个孤立的节点

让客户端和多个独立的 Redis 节点依次请求申请加锁，如果客户端能够和半数以上的节点成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁，否则加锁失败

Redlock 算法加锁三个过程：

- 第一步是，客户端获取当前时间（t1）。
- 第二步是，客户端按顺序依次向 N 个 Redis 节点执行加锁操作：
  - 加锁操作使用 SET 命令，带上 NX，EX/PX 选项，以及带上客户端的唯一标识。
  - 如果某个 Redis 节点发生故障了，为了保证在这种情况下，Redlock 算法能够继续运行，我们需要给加锁操作设置一个超时时间（不是对锁设置超时时间，而是对加锁操作设置超时时间），加锁操作的超时时间需要远远地小于锁的过期时间，一般也就是设置为几十毫秒。
- 第三步是，一旦客户端从超过半数（大于等于 N/2+1）的 Redis 节点上成功获取到了锁，就再次获取当前时间（t2），然后计算计算整个加锁过程的总耗时（t2-t1）。如果 t2-t1 < 锁的过期时间，此时，认为客户端加锁成功，否则认为加锁失败

加锁成功的条件：

- 条件一：客户端从超过半数（大于等于 N/2+1）的 Redis 节点上成功获取到了锁；
- 条件二：客户端从大多数节点获取锁的总耗时（t2-t1）小于锁设置的过期时间

加锁失败后，客户端向所有 Redis 节点发起释放锁的操作，释放锁的操作和在单节点上释放锁的操作一样，只要执行释放锁的 Lua 脚本就可以了

# NoSQL

## 概念

### 基本

将一批数据的共有特性经过提取后的有结构的数据就是结构化数据(例如一个国家的所有学生信息)

不能以同一个结构描述的一个数据集合,统称为非结构化数据(网页,日志是典型的非结构化数据)

存储结构化数据使用二维表;

典型的关系型数据库（MySQL、Oracle）,处理的是结构化数据，使用的操作语言sql(structured query language);

存储非结构化数据使用的常用结构就是key-value;

对于非结构化的海量数据处理(增删查改),使用nosql(not only structured query language);

典型的非关系型数据库redis,mongoDB



## MongoDB

### 安装

#### windows

[下载](https://www.runoob.com/mongodb/mongodb-window-install.html) [安装教程](http://www.ppmy.cn/news/3940.html)

安装后，设一下 path，在安装目录下的 `data`(不带空格等)，新建 `db\`，如果没有，在安装目录 `log\mongod.log` 新建，有就不理。安装目录新建 `mongo.config`，如果无权，改一下文件夹权限。内容为：

```yaml
# MongoDB 配置文件 示例

# 数据库路径
storage:
  dbPath: C:\temps\mongo\data\db #自己改

# 日志文件路径
systemLog:
  destination: file
  path: C:\temps\mongo\log\mongodb.log
  logAppend: true

# 监听的 IP 和端口
net:
  bindIp: 127.0.0.1
  port: 27017

# 是否启用安全认证（默认情况下关闭）
security:
  authorization: "disabled"

# 默认情况下，MongoDB 使用独占锁定，如果需要开启并发支持，请将下面的选项设置为 true
#wiredTigerConcurrentRead: (gpt刷的，感觉有错)
#  enabled: true

# 集群配置，如需要，请取消注释并根据实际情况进行修改
#replication:
#  replSetName: "rs0"

# 分片配置，如需要，请取消注释并根据实际情况进行修改
#sharding:
#  clusterRole: "shardsvr"
```

运行：

> --install 安装为服务后，MongoDB 将在 Windows 启动时自动启动，并且可以在服务管理器中进行管理。

```sh
mongod --config C:\temps\mongo\mongo.config --install
```

> ```sh
> mongod.exe --config C:\temps\mongo\mongo.config --serviceName "MongoDB" --serviceDisplayName "MongoDB" --install
> ```

管理员模式，启动关闭服务：

```sh
net start MongoDB
net stop MongoDB
```

删除服务：

```sh
mongod.exe --remove
```

如果访问 `http://localhost:27017/` 有：`It looks like you are trying to access MongoDB over HTTP on the native driver port.`，则证明启动成功了。

安装Mongoshell：[下载链接](https://www.mongodb.com/try/download/shell)，直接解压到安装目录下，并配置 path。进入 sh：

```sh
mongosh
```

> ```sh
> mongosh "mongodb://localhost:27017/mydatabase"
> ```

### 概念

#### 术语

| SQL术语/概念 | MongoDB术语/概念 | 解释/说明                           |
| :----------- | :--------------- | :---------------------------------- |
| database     | database         | 数据库                              |
| table        | collection       | 数据库表/集合                       |
| row          | document         | 数据记录行/文档                     |
| column       | field            | 数据字段/域                         |
| index        | index            | 索引                                |
| table joins  |                  | 表连接,MongoDB不支持                |
| primary key  | primary key      | 主键,MongoDB自动将_id字段设置为主键 |

|       RDBMS        | MongoDB                           |
| :----------------: | :-------------------------------- |
|       数据库       | 数据库                            |
|        表格        | 集合                              |
|         行         | 文档                              |
|         列         | 字段                              |
|       表联合       | 嵌入文档                          |
|        主键        | 主键 (MongoDB 提供了 key 为 _id ) |
| 数据库服务和客户端 |                                   |
|   Mysqld/Oracle    | mongod                            |
|   mysql/sqlplus    | mongo                             |

##### 文档

文档是一组键值(key-value)对(即 BSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。

```json
{"site":"www.runoob.com", "name":"菜鸟教程"}
```

需要注意的是：

1. 文档中的键/值对是有序的。
2. 文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。
3. MongoDB区分类型和大小写。
4. MongoDB的文档不能有重复的键。
5. 文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符。

文档键命名规范：

- 键不能含有\0 (空字符)。这个字符用来表示键的结尾。
- .和$有特别的意义，只有在特定环境下才能使用。
- 以下划线"_"开头的键是保留的(不是严格要求的)。

##### 集合

集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统：Relational Database Management System)中的表格。

集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。

比如，我们可以将以下不同数据结构的文档插入到集合中：

```json
{"site":"www.baidu.com"}
{"site":"www.google.com","name":"Google"}
{"site":"www.runoob.com","name":"菜鸟教程","num":5}
```

当第一个文档插入时，集合就会被创建。

- 集合名不能是空字符串""。
- 集合名不能含有\0字符（空字符)，这个字符表示集合名的结尾。
- 集合名不能以"system."开头，这是为系统集合保留的前缀。
- 用户创建的集合名字不能含有保留字符。有些驱动程序的确支持在集合名里面包含，这是因为某些系统生成的集合中包含该字符。除非你要访问这种系统创建的集合，否则千万不要在名字里出现$。　

> Capped collections 就是固定大小的collection。
>
> 它有很高的性能以及队列过期的特性(过期按照插入的顺序). 有点和 "RRD" 概念类似。
>
> Capped collections 是高性能自动的维护对象的插入顺序。它非常适合类似记录日志的功能和标准的 collection 不同，你必须要显式的创建一个capped collection，指定一个 collection 的大小，单位是字节。collection 的数据存储空间值提前分配的。
>
> Capped collections 可以按照文档的插入顺序保存到集合中，而且这些文档在磁盘上存放位置也是按照插入顺序来保存的，所以当我们更新Capped collections 中文档的时候，更新后的文档不可以超过之前文档的大小，这样话就可以确保所有文档在磁盘上的位置一直保持不变。
>
> 由于 Capped collection 是按照文档的插入顺序而不是使用索引确定插入位置，这样的话可以提高增添数据的效率。MongoDB 的操作日志文件 oplog.rs 就是利用 Capped Collection 来实现的。
>
> 要注意的是指定的存储大小包含了数据库的头信息。
>
> ```
> db.createCollection("mycoll", {capped:true, size:100000})
> ```
>
> - 在 capped collection 中，你能添加新的对象。
> - 能进行更新，然而，对象不会增加存储空间。如果增加，更新就会失败 。
> - 使用 Capped Collection 不能删除一个文档，可以使用 drop() 方法删除 collection 所有的行。
> - 删除之后，你必须显式的重新创建这个 collection。
> - 在32bit机器中，capped collection 最大存储为 1e9( 1X109)个字节。

#### 元数据

数据库的信息是存储在集合中。它们使用了系统的命名空间：

```
dbname.system.*
```

在MongoDB数据库中名字空间 <dbname>.system.* 是包含多种系统信息的特殊集合(Collection)，如下:

| 集合命名空间             | 描述                                      |
| :----------------------- | :---------------------------------------- |
| dbname.system.namespaces | 列出所有名字空间。                        |
| dbname.system.indexes    | 列出所有索引。                            |
| dbname.system.profile    | 包含数据库概要(profile)信息。             |
| dbname.system.users      | 列出所有可访问数据库的用户。              |
| dbname.local.sources     | 包含复制对端（slave）的服务器信息和状态。 |

对于修改系统集合中的对象有如下限制。

在{{system.indexes}}插入数据，可以创建索引。但除此之外该表信息是不可变的(特殊的drop index命令将自动更新相关信息)。

{{system.users}}是可修改的。 {{system.profile}}是可删除的。

#### 数据类型

表为MongoDB中常用的几种数据类型。

| 数据类型           | 描述                                                         |
| :----------------- | :----------------------------------------------------------- |
| String             | 字符串。存储数据常用的数据类型。在 MongoDB 中，UTF-8 编码的字符串才是合法的。 |
| Integer            | 整型数值。用于存储数值。根据你所采用的服务器，可分为 32 位或 64 位。 |
| Boolean            | 布尔值。用于存储布尔值（真/假）。                            |
| Double             | 双精度浮点值。用于存储浮点值。                               |
| Min/Max keys       | 将一个值与 BSON（二进制的 JSON）元素的最低值和最高值相对比。 |
| Array              | 用于将数组或列表或多个值存储为一个键。                       |
| Timestamp          | 时间戳。记录文档修改或添加的具体时间。                       |
| Object             | 用于内嵌文档。                                               |
| Null               | 用于创建空值。                                               |
| Symbol             | 符号。该数据类型基本上等同于字符串类型，但不同的是，它一般用于采用特殊符号类型的语言。 |
| Date               | 日期时间。用 UNIX 时间格式来存储当前日期或时间。你可以指定自己的日期时间：创建 Date 对象，传入年月日信息。 |
| Object ID          | 对象 ID。用于创建文档的 ID。                                 |
| Binary Data        | 二进制数据。用于存储二进制数据。                             |
| Code               | 代码类型。用于在文档中存储 JavaScript 代码。                 |
| Regular expression | 正则表达式类型。用于存储正则表达式                           |

##### ObjectId

ObjectId 类似唯一主键，可以很快的去生成和排序，包含 12 bytes，含义是：

- 前 4 个字节表示创建 **unix** 时间戳,格林尼治时间 **UTC** 时间，比北京时间晚了 8 个小时
- 接下来的 3 个字节是机器标识码
- 紧接的两个字节由进程 id 组成 PID
- 最后三个字节是随机数

MongoDB 中存储的文档必须有一个 _id 键。这个键的值可以是任何类型的，默认是个 ObjectId 对象

由于 ObjectId 中保存了创建的时间戳，所以你不需要为你的文档保存时间戳字段，你可以通过 getTimestamp 函数来获取文档的创建时间

##### 字符串

**BSON 字符串都是 UTF-8 编码**

##### 时间戳

BSON 有一个特殊的时间戳类型用于 MongoDB 内部使用，与普通的 日期 类型不相关。 时间戳值是一个 64 位的值。其中：

- 前32位是一个 time_t 值（与Unix新纪元相差的秒数）
- 后32位是在某秒中操作的一个递增的`序数`

在单个 mongod 实例中，时间戳值通常是唯一的。

在复制集中， oplog 有一个 ts 字段。这个字段中的值使用BSON时间戳表示了操作时间。

> BSON 时间戳类型主要用于 MongoDB 内部使用。在大多数情况下的应用开发中，你可以使用 BSON 日期类型。

##### 日期

表示当前距离 Unix新纪元（1970年1月1日）的毫秒数。日期类型是有符号的, 负数表示 1970 年之前的日期

#### 链接

```
mongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]
```

- **mongodb://** 这是固定的格式，必须要指定。
- **username:password@** 可选项，如果设置，在连接数据库服务器之后，驱动都会尝试登录这个数据库
- **host1** 必须的指定至少一个host, host1 是这个URI唯一要填写的。它指定了要连接服务器的地址。如果要连接复制集，请指定多个主机地址。
- **portX** 可选的指定端口，如果不填，默认为27017
- **/database** 如果指定username:password@，连接并验证登录指定数据库。若不指定，默认打开 test 数据库。
- **?options** 是连接选项。如果不使用/database，则前面需要加上/。所有连接选项都是键值对name=value，键值对之间通过&或;（分号）隔开

标准的连接格式包含了多个选项(options)，如下所示：

| 选项                | 描述                                                         |
| :------------------ | :----------------------------------------------------------- |
| replicaSet=name     | 验证replica set的名称。 Impliesconnect=replicaSet.           |
| slaveOk=true\|false | true:在connect=direct模式下，驱动会连接第一台机器，即使这台服务器不是主。在connect=replicaSet模式下，驱动会发送所有的写请求到主并且把读取操作分布在其他从服务器。false: 在 connect=direct模式下，驱动会自动找寻主服务器. 在connect=replicaSet 模式下，驱动仅仅连接主服务器，并且所有的读写命令都连接到主服务器。 |
| safe=true\|false    | true: 在执行更新操作之后，驱动都会发送getLastError命令来确保更新成功。(还要参考 wtimeoutMS).false: 在每次更新之后，驱动不会发送getLastError来确保更新成功。 |
| w=n                 | 驱动添加 { w : n } 到getLastError命令. 应用于safe=true。     |
| wtimeoutMS=ms       | 驱动添加 { wtimeout : ms } 到 getlasterror 命令. 应用于 safe=true. |
| fsync=true\|false   | true: 驱动添加 { fsync : true } 到 getlasterror 命令.应用于 safe=true.false: 驱动不会添加到getLastError命令中。 |
| journal=true\|false | 如果设置为 true, 同步到 journal (在提交到数据库前写入到实体中). 应用于 safe=true |
| connectTimeoutMS=ms | 可以打开连接的时间。                                         |
| socketTimeoutMS=ms  | 发送和接受sockets的时间。                                    |

### 指令

使用 `mongosh` 进入后输入指令。支持 js 语法

#### 数据库

查看所有数据库：

```mysql
show dbs
```

查看当前数据库对象/集合：

```
db
```

连接到指定数据库，如果不存在，新建：

```mysql
use 数据库名
```

> 数据库也通过名字来标识。数据库名可以是满足以下条件的任意UTF-8字符串。
>
> - 不能是空字符串（"")。
> - 不得含有' '（空格)、.、$、/、\和\0 (空字符)。
> - 应全部小写。
> - 最多64字节。
>
> 有一些数据库名是保留的，可以直接访问这些有特殊作用的数据库。
>
> - **admin**： 从权限的角度来看，这是"root"数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。
> - **local:** 这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合
> - **config**: 当Mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。

插入数据示例：

```mysql
use testmongo
db.testmongo.insertOne({"name":"你好"})
```

删除当前 use 的数据库：

```sh
db.dropDatabase()
```

#### 集合

创建：

```sh
db.createCollection(name, options)
```

> options 可以是如下参数：
>
> | 字段        | 类型 | 描述                                                         |
> | :---------- | :--- | :----------------------------------------------------------- |
> | capped      | 布尔 | （可选）如果为 true，则创建固定集合。固定集合是指有着固定大小的集合，当达到最大值时，它会自动覆盖最早的文档。 **当该值为 true 时，必须指定 size 参数。** |
> | autoIndexId | 布尔 | 3.2 之后不再支持该参数。（可选）如为 true，自动在 _id 字段创建索引。默认为 false。 |
> | size        | 数值 | （可选）为固定集合指定一个最大值，即字节数。 **如果 capped 为 true，也需要指定该字段。** |
> | max         | 数值 | （可选）指定固定集合中包含文档的最大数量。                   |
>
> 在插入文档时，MongoDB 首先检查固定集合的 size 字段，然后检查 max 字段。

```sh
db.createCollection("test1")
```

> 创建固定集合 mycol，整个集合空间大小 6142800 B, 文档最大个数为 10000 个。
>
> ```sh
> db.createCollection("mycol", { capped : true, autoIndexId : true, size : 6142800, max : 10000 } )
> ```

在 MongoDB 中，你不需要创建集合。当你插入一些文档时，MongoDB 会自动创建集合

```sh
db.mycol2.insertOne({"name" : "白茶"})
```

列出集合：

```sh
show collections
```

删除集合：`db.集合名.drop()`

```sh
db.mycol2.drop()
```

列出集合内的全部文档：

```sh
db.mycol2.find()
```



#### 文档

##### 插入

> MongoDB 使用 insert() 或 save() 方法向集合中插入文档，语法如下：
>
> ```
> db.COLLECTION_NAME.insert(document)
> 或
> db.COLLECTION_NAME.save(document)
> ```
>
> - save()：如果 _id 主键存在则更新数据，如果不存在就插入数据。该方法新版本中已废弃，可以使用 **db.collection.insertOne()** 或 **db.collection.replaceOne()** 来代替。
> - insert(): 若插入的数据主键已经存在，则会抛 **org.springframework.dao.DuplicateKeyException** 异常，提示主键重复，不保存当前数据。

db.collection.insertOne() 用于向集合插入一个新文档，语法格式如下：

```
db.collection.insertOne(
   <document>,
   {
      writeConcern: <document>
   }
)
```

db.collection.insertMany() 用于向集合插入一个多个文档，语法格式如下：

```
db.collection.insertMany(
   [ <document 1> , <document 2>, ... ],
   {
      writeConcern: <document>,
      ordered: <boolean>
   }
)
```

参数说明：

- document：要写入的文档。
- writeConcern：写入策略，默认为 1，即要求确认写操作，0 是不要求。
- ordered：指定是否按顺序写入，默认 true，按顺序写入。

如：

```json
db.col.insertOne({title: 'MongoDB 教程', 
    description: 'MongoDB 是一个 Nosql 数据库',
    by: '菜鸟教程',
    url: 'http://www.runoob.com',
    tags: ['mongodb', 'database', 'NoSQL'],
    likes: 100
})
```

变量定义与使用：

```js
document=({title: 'MongoDB 教程', 
    description: 'MongoDB 是一个 Nosql 数据库',
    by: '菜鸟教程',
    url: 'http://www.runoob.com',
    tags: ['mongodb', 'database', 'NoSQL'],
    likes: 100
});
db.col.insertOne(document);
```

```sh
var res = db.collection.insertMany([{"b": 3}, {'c': 4}])
res
```

##### 更新

```json
db.collection.update(
   <query>,
   <update>,
   {
     upsert: <boolean>,
     multi: <boolean>,
     writeConcern: <document>
   }
)
```

**参数说明：**

- **query** : update的查询条件，类似sql update查询内where后面的。
- **update** : update的对象和一些更新的操作符（如$,$inc...）等，也可以理解为sql update查询内set后面的
- **upsert** : 可选，这个参数的意思是，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入。
- **multi** : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。
- **writeConcern** :可选，抛出异常的级别。

```sh
db.col.updateOne({'title':'MongoDB 教程'},{$set:{'title':'MongoDB'}})
db.col.find().pretty()
```

以上语句只会修改第一条发现的文档，如果你要修改多条相同的文档，则需要设置 multi 参数为 true。

```sh
db.col.update({'title':'MongoDB 教程'},{$set:{'title':'MongoDB'}},{multi:true})
```

update 是修改指定的字段，replace 是完全覆盖。

##### 删除

```js
db.collection.remove(
   <query>,
   {
     justOne: <boolean>,
     writeConcern: <document>
   }
)
```

- **query** :（可选）删除的文档的条件。
- **justOne** : （可选）如果设为 true 或 1，则只删除一个文档，如果不设置该参数，或使用默认值 false，则删除所有匹配条件的文档。
- **writeConcern** :（可选）抛出异常的级别。

```sh
db.col.remove({'title':'MongoDB 教程'})
```

全删：

```sh
db.col.remove({})
```

如果你只想删除第一条找到的记录可以设置 justOne 为 1，如下所示：

```sh
db.COLLECTION_NAME.remove(DELETION_CRITERIA,1)
```

#### 查询

##### 常规

MongoDB 查询数据的语法格式如下：

```
db.collection.find(query, projection)
```

- **query** ：可选，使用查询操作符指定查询条件
- **projection** ：可选，使用投影操作符指定返回的键。查询时返回文档中所有键值， 只需省略该参数即可（默认省略）。

如果你需要以易读的方式来读取数据，可以使用 pretty() 方法，语法格式如下：

```
db.col.find().pretty()
```

pretty() 方法以格式化的方式来显示所有文档。

除了 find() 方法之外，还有一个 findOne() 方法，它只返回一个文档。

| 操作       | 格式                     | 范例                                        | RDBMS中的类似语句       |
| :--------- | :----------------------- | :------------------------------------------ | :---------------------- |
| 等于       | `{<key>:<value>`}        | `db.col.find({"by":"菜鸟教程"}).pretty()`   | `where by = '菜鸟教程'` |
| 小于       | `{<key>:{$lt:<value>}}`  | `db.col.find({"likes":{$lt:50}}).pretty()`  | `where likes < 50`      |
| 小于或等于 | `{<key>:{$lte:<value>}}` | `db.col.find({"likes":{$lte:50}}).pretty()` | `where likes <= 50`     |
| 大于       | `{<key>:{$gt:<value>}}`  | `db.col.find({"likes":{$gt:50}}).pretty()`  | `where likes > 50`      |
| 大于或等于 | `{<key>:{$gte:<value>}}` | `db.col.find({"likes":{$gte:50}}).pretty()` | `where likes >= 50`     |
| 不等于     | `{<key>:{$ne:<value>}}`  | `db.col.find({"likes":{$ne:50}}).pretty()`  | `where likes != 50`     |

MongoDB 的 find() 方法可以传入多个键(key)，每个键(key)以逗号隔开，即常规 SQL 的 AND 条件。语法格式如下：

```js
db.col.find({key1:value1, key2:value2}).pretty()
db.col.find({"by":"菜鸟教程", "title":"MongoDB 教程"}).pretty()
```

MongoDB OR 条件语句使用了关键字 **$or**,语法格式如下：

```sh
db.col.find({$or:[{"by":"菜鸟教程"},{"title": "MongoDB 教程"}]}).pretty()
db.col.find({"likes": {$gt:50}, $or: [{"by": "菜鸟教程"},{"title": "MongoDB 教程"}]}).pretty()
```

> `'where likes>50 AND (by = '菜鸟教程' OR title = 'MongoDB 教程')'`

```sh
db.col.find({likes : {$lt :200, $gt : 100}})
```

##### limit

```sh
db.COLLECTION_NAME.find().limit(NUMBER)
db.col.find({},{"title":1,_id:0}).limit(2)
```

使用skip()方法来跳过指定数量的数据，skip方法同样接受一个数字参数作为跳过的记录条数。

以下实例只会显示第二条文档数据

```sh
db.col.find({},{"title":1,_id:0}).limit(1).skip(1)
```

##### 排序

在 MongoDB 中使用 sort() 方法对数据进行排序，sort() 方法可以通过参数指定排序的字段，并使用 1 和 -1 来指定排序的方式，其中 1 为升序排列，而 -1 是用于降序排列。

```js
db.COLLECTION_NAME.find().sort({KEY:1})
db.col.find({},{"title":1,_id:0}).sort({"likes":-1})
```

skip(), limilt(), sort()三个放在一起执行的时候，执行的顺序是先 sort(), 然后是 skip()，最后是显示的 limit()。

##### 关系

MongoDB 的关系表示多个文档之间在逻辑上的相互联系。

文档间可以通过嵌入和引用来建立联系。

MongoDB 中的关系可以是：

- 1:1 (1对1)
- 1: N (1对多)
- N: 1 (多对1)
- N: N (多对多)

使用嵌入式方法，我们可以把用户地址嵌入到用户的文档中：

```json
{
   "_id":ObjectId("52ffc33cd85242f436000001"),
   "contact": "987654321",
   "dob": "01-01-1991",
   "name": "Tom Benzamin",
   "address": [
      {
         "building": "22 A, Indiana Apt",
         "pincode": 123456,
         "city": "Los Angeles",
         "state": "California"
      },
      {
         "building": "170 A, Acropolis Apt",
         "pincode": 456789,
         "city": "Chicago",
         "state": "Illinois"
      }]
} 
```

你可以这样查询用户的地址

```sh
db.users.findOne({"name":"Tom Benzamin"},{"address":1})
```

引用式关系是设计数据库时经常用到的方法，这种方法把用户数据文档和用户地址数据文档分开，通过引用文档的 **id** 字段来建立关系。

```sh
{
   "_id":ObjectId("52ffc33cd85242f436000001"),
   "contact": "987654321",
   "dob": "01-01-1991",
   "name": "Tom Benzamin",
   "address_ids": [
      ObjectId("52ffc4a5d85242602e000000"),
      ObjectId("52ffc4a5d85242602e000001")
   ]
}
```

```js
var result = db.users.findOne({"name":"Tom Benzamin"},{"address_ids":1})
var addresses = db.address.find({"_id":{"$in":result["address_ids"]}})
```

##### 引用

MongoDB 引用有两种：

- 手动引用（Manual References）
- DBRefs

考虑这样的一个场景，我们在不同的集合中 (address_home, address_office, address_mailing, 等)存储不同的地址（住址，办公室地址，邮件地址等）。

这样，我们在调用不同地址时，也需要指定集合，一个文档从多个集合引用文档，我们应该使用 DBRefs。

DBRef的形式：

```
{ $ref : , $id : , $db :  }
```

三个字段表示的意义为：

- $ref：集合名称
- $id：引用的id
- $db:数据库名称，可选参数

```js
{
   "_id":ObjectId("53402597d852426020000002"),
   "address": {
   "$ref": "address_home",
   "$id": ObjectId("534009e4d852427820000002"),
   "$db": "runoob"},
   "contact": "987654321",
   "dob": "01-01-1991",
   "name": "Tom Benzamin"
}
```

**address** DBRef 字段指定了引用的地址文档是在 runoob 数据库下的 address_home 集合，id 为 534009e4d852427820000002。

以下代码中，我们通过指定 $ref 参数（address_home 集合）来查找集合中指定id的用户地址信息：

```js
var user = db.users.findOne({"name":"Tom Benzamin"})
var dbRef = user.address
db[dbRef.$ref].findOne({"_id":(dbRef.$id)})
```

##### 覆盖索引

覆盖查询是以下的查询：

- 所有的查询字段是索引的一部分
- 所有的查询返回字段在同一个索引中

由于所有出现在查询中的字段是索引的一部分， MongoDB 无需在整个数据文档中检索匹配查询条件和返回使用相同索引的查询结果。

因为索引存在于RAM中，从索引中获取数据比通过扫描文档读取数据要快得多。

```js
{
   "_id": ObjectId("53402597d852426020000002"),
   "contact": "987654321",
   "dob": "01-01-1991",
   "gender": "M",
   "name": "Tom Benzamin",
   "user_name": "tombenzamin"
}
```

我们在 users 集合中创建联合索引，字段为 gender 和 user_name :

```js
db.users.createIndex({gender:1,user_name:1})
```

现在，该索引会覆盖以下查询：

```rs
db.users.find({gender:"M"},{user_name:1,_id:0})
```

也就是说，对于上述查询，MongoDB的不会去数据库文件中查找。相反，它会从索引中提取数据，这是非常快速的数据查询。

由于我们的索引中不包括 \_id 字段，\_id在查询中会默认返回，我们可以在MongoDB的查询结果集中排除它。

下面的实例没有排除_id，查询就不会被覆盖：

```rs
db.users.find({gender:"M"},{user_name:1})
```

最后，如果是以下的查询，不能使用覆盖索引查询：

- 所有索引字段是一个数组
- 所有索引字段是一个子文档

##### 分析

MongoDB 查询分析可以确保我们所建立的索引是否有效，是查询语句性能分析的重要工具。

MongoDB 查询分析常用函数有：explain() 和 hint()。

```sh
db.users.find({gender:"M"},{user_name:1,_id:0}).explain()
```

现在，我们看看这个结果集的字段：

- **indexOnly**: 字段为 true ，表示我们使用了索引。
- **cursor**：因为这个查询使用了索引，MongoDB 中索引存储在B树结构中，所以这是也使用了 BtreeCursor 类型的游标。如果没有使用索引，游标的类型是 BasicCursor。这个键还会给出你所使用的索引的名称，你通过这个名称可以查看当前数据库下的system.indexes集合（系统自动创建，由于存储索引信息，这个稍微会提到）来得到索引的详细信息。
- **n**：当前查询返回的文档数量。
- **nscanned/nscannedObjects**：表明当前这次查询一共扫描了集合中多少个文档，我们的目的是，让这个数值和返回文档的数量越接近越好。
- **millis**：当前查询所需时间，毫秒数。
- **indexBounds**：当前查询具体使用的索引。

虽然MongoDB查询优化器一般工作的很不错，但是也可以使用 hint 来强制 MongoDB 使用一个指定的索引。

这种方法某些情形下会提升性能。 一个有索引的 collection 并且执行一个多字段的查询(一些字段已经索引了)。

如下查询实例指定了使用 gender 和 user_name 索引字段来查询：

```js
db.users.find({gender:"M"},{user_name:1,_id:0}).hint({gender:1,user_name:1})
db.users.find({gender:"M"},{user_name:1,_id:0}).hint({gender:1,user_name:1}).explain()
```

#### 原子操作

mongodb不支持事务，所以，在你的项目中应用时，要注意这点。无论什么设计，都不要要求mongodb保证数据的完整性。

但是mongodb提供了许多原子操作，比如文档的保存，修改，删除等，都是原子操作。

所谓原子操作就是要么这个文档保存到Mongodb，要么没有保存到Mongodb，不会出现查询到的文档没有保存完整的情况。

##### $set

用来指定一个键并更新键值，若键不存在并创建。

```
{ $set : { field : value } }
```

##### $unset

用来删除一个键。

```
{ $unset : { field : 1} }
```

##### $inc

$inc可以对文档的某个值为数字型（只能为满足要求的数字）的键进行增减的操作。

```
{ $inc : { field : value } }
```

##### $push

用法：

```
{ $push : { field : value } }
```

把value追加到field里面去，field一定要是数组类型才行，如果field不存在，会新增一个数组类型加进去。

##### $pushAll

同$push,只是一次可以追加多个值到一个数组字段内。

```
{ $pushAll : { field : value_array } }
```

##### $pull

从数组field内删除一个等于value值。

```
{ $pull : { field : _value } }
```

##### $addToSet

增加一个值到数组内，而且只有当这个值不在数组内才增加。

##### $pop

删除数组的第一个或最后一个元素

```
{ $pop : { field : 1 } }
```

##### $rename

修改字段名称

```
{ $rename : { old_field_name : new_field_name } }
```

##### $bit

位操作，integer类型

```
{$bit : { field : {and : 5}}}
```

##### 偏移操作符

```
 t.find() { "_id" : ObjectId("4b97e62bf1d8c7152c9ccb74"), "title" : "ABC", "comments" : [ { "by" : "joe", "votes" : 3 }, { "by" : "jane", "votes" : 7 } ] }
 
> t.update( {'comments.by':'joe'}, {$inc:{'comments.$.votes':1}}, false, true )
 
> t.find() { "_id" : ObjectId("4b97e62bf1d8c7152c9ccb74"), "title" : "ABC", "comments" : [ { "by" : "joe", "votes" : 4 }, { "by" : "jane", "votes" : 7 } ] }
```



#### 其他

##### type

```sh
db.col.find({"title" : {$type : 'string'}})
db.col.find({"title" : {$type : 2}})
```

| **类型**                | **数字** | **备注**         |
| :---------------------- | :------- | :--------------- |
| Double                  | 1        |                  |
| String                  | 2        |                  |
| Object                  | 3        |                  |
| Array                   | 4        |                  |
| Binary data             | 5        |                  |
| Undefined               | 6        | 已废弃。         |
| Object id               | 7        |                  |
| Boolean                 | 8        |                  |
| Date                    | 9        |                  |
| Null                    | 10       |                  |
| Regular Expression      | 11       |                  |
| JavaScript              | 13       |                  |
| Symbol                  | 14       |                  |
| JavaScript (with scope) | 15       |                  |
| 32-bit integer          | 16       |                  |
| Timestamp               | 17       |                  |
| 64-bit integer          | 18       |                  |
| Min key                 | 255      | Query with `-1`. |
| Max key                 | 127      |                  |



## Redis

### 概念

#### 安装

##### windows

找个地方 [下载](https://pan.baidu.com/s/1ug6tN060YE9EYchxgdeW0w ) redis，解压后将 `redis-x64-3.2.100.zip` 解压，而 `desktop-manager` 是可视化客户端。检验方法：双击解压后的 `redis-server` 能跑。默认端口号 6379。安装客户端。打开后点右下角 connect to redis server，选 `127.0.0.1:6379`，在没关 server 时能连上。

> 提示说要更新，但是不更新也行，客户端。

##### linux

[下载](http://download.redis.io/releases/redis-3.2.11.tar.gz)，然后传到 linux 上 `/home/software`。解压：

```sh
tar -xf redis-3.2.11.tar.gz
```

安装插件：

```sh
yum -y install cmake gcc tcl*
```

cd 进解压的文件夹，执行编译：

```sh
make && make install
```

最后看到五个 install，表示安装成功。

或者：[apt ver 官网指导](https://redis.io/docs/latest/operate/oss_and_stack/install/install-redis/install-redis-on-linux/)

```sh
apt-get update
apt-get install redis
```



#### 概念

Redis（Remote Dictionary Server )，即远程字典服务

Redis 是一个支持nosql，key-value数据类型、内存、缓存、分布式的、可持久化、非关系型数据库。

redis在运行期间,使用的是内存资源作为存储介质存储数据;具有内存的致命缺陷(容量上限有限,断电丢失数据);优点是内存存储快

存储在redis内存中的数据,可以设置持久化策略,在断电内存数据丢失时,重启redis可以从磁盘文件重新读取数据(占据市场重要的技术点)

3.0之后的cluster技术,1000个节点，可以使用redis搭建传统结构的分布式,或者利用redis-cluster搭建集群

支持的功能：

1. 缓存,提升系统访问数据的性能(减少了层级之间的调用)

2. 低配版本的消息队列

3. 实现各种逻辑锁

4. session共享使用redis解决

### 指令

#### 启动

任意目录下，执行以下以启动单节点服务，要一直挂着，要么就丢后台。

```sh
redis-server
```

在挂着服务器的同时，另一个 bash 启动客户端：(默认 127.0.0.1 6379)

```sh
redis-cli
```

查看版本：

```sh
redis-cli --version
```

或进 `redis-cli` 后：`info server`

输入 `exit` 退出。

#### 基本

##### CRUD

- set key value
- get key
- del key

```redis
set myname baicha
get myname
del myname
```

第一行可以是C也可以是U，查无反馈 (nil)。

set 返回 OK 或 nil。重复 set 同一个值也 OK。

删除返回整数表示成功数，可以删不存在的返回0

key, value 都区分大小写，关键字不区分。



命令参数：

- EX 过期时间，单位为秒

  ```redis
  set myname baicha EX 10
  ```

  十秒内能查到，之后查反馈 nil

- PX 毫秒为单位的过期时间，同理

- NX 当且仅当值不存在时修改。如果存在，不管要改的跟之前一不一样，都返回 nil

  ```redis
  set myname guodong NX
  ```

- XX 当且仅当存在时修改



批量 CRUD 单节点批量操作（不支持多节点分布式，只能对本节点数据进行处理）(测试时确定当前节点拥有的数据)

```redis
mset n1 v1 n v2 n3 v3
mget n1 n2 n3 n4
```

即按顺序执行上述操作。



中文会编码，如下面 get 得 `"\xe7\x99\xbd\xe8\x8c\xb6"`

```redis
set name 白茶
get name
```



##### keys

keys，列表编号返回(疑似从新到远)每个 key，查无显示 empty list or set

```redis
set myname baicha
set myage 20
set hisname guodong
keys *
keys my*
keys *name
keys *ag*
```

##### select

默认存在 16 个数据分库，索引号为 0-15。默认是 0 号。可以换库。

```redis
set myname baicha
select 1
get myname
```

则找不到，select 回去找得到。

##### exists

是否存在某个 key。可以一下子查一堆。get也可以完成这个任务。redis在key-value结构的value存储容量是1G。使用get判断存在会先读再判断数据是否为空，浪费时间。

返回整数值表示个数。

```redis
exists myname urname hisname
exists hername
```

##### type

查看 key 的数据类型。

```redis
set myname lr580
type myname
hset user age 20
type user
lpush list01 10 20 50 80
type list
```

##### expire

对key添加超时过期的设定。

redis作为内存缓存数据库虽然速度快,还是有缺点的,内存容量,不可能比硬盘容量大的，可以通过expired key 设置数据的过期时间(秒)

`ttl key`，查看过期时间，-1 是无限，-2 是键不存在(或已经过期)。

`expire key sec`，设置某个键倒数 sec 秒后过期。只对存在的键有用。

`pttl key`，毫秒为单位查看过期时间。

`pexpire key msec`，同理，毫秒为单位设置到期。

redis最常见的保护内存的机制(热点数据保存,冷点数据超时清除)。访问次数多的数据，超时时间越长，访问次数少的数据，超时时间短。

##### flushall

清除所有分库的数据，包括清空 `dump.rdb` 文件。

##### save

当前内存数据保存到磁盘根目录 `dump.rdb`。关闭服务端还在。

##### flushdb

清空一个分库，不删除持久化文件内容。

##### append

对 key 对应的 value 追加数据，如字符串连接。返回追加后字符串长度。如果本来不存在 key 就新建。

```redis
set myname bai
append myname cha
get myname
```



##### 数字增减

`incr key`, `decr key`，如果字符串是不含前导零纯整数，可以自增自减，否则报错。

`incrby key num`, `decrby key num`。加减整数。

#### 其他

##### shutdown

关闭该服务器

##### 配置查询

`info replication`

##### slaveof

`slaveof IP port` 变成从节点

#### 集群

4.1 集群命令
cluster info ：打印集群的信息
cluster nodes ：列出集群当前已知的所有节点（ node），以及这些节点的相关信息。
4.2节点
cluster meet <ip> <port> ：将 ip 和 port 所指定的节点添加到集群当中，让它成为集群的一份子。
cluster forget <node_id> ：从集群中移除 node_id 指定的节点(保证空槽道)。
cluster replicate <node_id> ：将当前节点设置为 node_id 指定的节点的从节点。
cluster saveconfig ：将节点的配置文件保存到硬盘里面。
4.3槽(slot)
cluster addslots <slot> [slot ...] ：将一个或多个槽（ slot）指派（ assign）给当前节点。
cluster delslots <slot> [slot ...] ：移除一个或多个槽对当前节点的指派。
cluster flushslots ：移除指派给当前节点的所有槽，让当前节点变成一个没有指派任何槽的节点。
cluster setslot <slot> node <node_id> ：将槽 slot 指派给 node_id 指定的节点，如果槽已经指派给另一个节点，那么先让另一个节点删除该槽>，然后再进行指派。
cluster setslot <slot> migrating <node_id> ：将本节点的槽 slot 迁移到 node_id 指定的节点中。
cluster setslot <slot> importing <node_id> ：从 node_id 指定的节点中导入槽 slot 到本节点。
cluster setslot <slot> stable ：取消对槽 slot 的导入（ import）或者迁移（ migrate）。
4.5键
cluster keyslot <key> ：计算键 key 应该被放置在哪个槽上。
cluster countkeysinslot <slot> ：返回槽 slot 目前包含的键值对数量。
cluster getkeysinslot <slot> <count> ：返回 count 个 slot 槽中的键

### 数据类型

String, Hash, List, Set, Zset

#### String

字符串类型。基本操作如上所示。

#### Hash

> 对应value为hash结构的数据都是面向对象的数据结构

- `hset key field value`，key 是 map 名，field 是键。返回新增键数。
- `hget key field`
- `hexists key field`
- `hdel key field` 返回成功删除个数。

```redis
hset h1 k1 v1
hset h1 k1 v2
hget h1 k1
hexists h1 k1
```

得 `v2`。

批量设置和批量获取，不支持分布式：

```redis
hmset user name lr580 psw 123 age 17 location tj
hmget user name psw age location
```

输出所有值(不输出键)：`hvals key`

输出所有键(按插入顺序)：`hkeys key`

输出键值对数目：`hlen key`

直接端了整个 hash：`del key`

单增：`hincrby key field num`

```redis
hincrby user age -1
```

#### List

查看双向链表list和设置链表数据lpush（左表示上，右表示下）

- `lpush key v1 v2 ...` 依次插入到最左
- `rpush key v1 c2 ...` 依次插入到最右
- `lrange key l r` 展示范围从左往右 [l,r] -1 表示尾部(倒数第一个)。下标从 0 开始。
- `lrange key before/after v v2` 在从左找到的第一个 v 之前/后插入值
- `lset key index value` 从左的某个下标对应值进行修改
- `lrem key count value` 从左往右找 count 个 value 删了。若 count =0 全删，count > 0 从左往右找，否则从右往左找
- `ltrim key l r` 只保留给定范围的值
- `lpop key` 删掉最左元素
- `rpoplpush key1 key2` 将第一个链表最右元素删掉插入到第二个链表最左
- `llen key` 查询长度

```
lpush l1 7 6 5
lpush l1 4 3 2 1
lrange l1 0 -1
lrange l1 -3 -1
```

```redis
rpush l2 1 7 5 8 1
linsert l2 after 1 4
linsert l2 before 7 3
lset l2 6 0
lrange l2 0 -1
```



#### Set

无序，不重复。保存多个字符串元素。

- `sadd key member` 插入一到多个元素
- `smembers key` 遍历
- `srem key member` 删除
- `sismember key element` 查询某个元素是否在集合中
- `sinter key1 key2` 计算交集($S_1\cap S_2$)，不改变操作数
- `sunion key1 key2` 计算并集
- `sdiff key1 key2` 计算差集 s1-s2 (在前者不在后者)

```redis
sadd s1 pwp qwq owo PWP pwp
srem s1 owo ovo
smembers s1
scard s1
sismember s1 pwp
srandmember s1
```



#### ZSet

有序集合，绑定一个 score 作为排序依据。注意仍保证 member 不重。

- `zadd key score member`。返回插入时数，如已存在更新 score

- `zscore key member` 查询分数

- `zrank key member` 查询下标(先按score第一关键字，再按member字典序)

- `zrem key member` 删除

- `zincrby key increment member` score变动

- `zrange key l r` 查询排名范围的所有成员

  `zrange key l r withscores` 依次先输出 member 再输出 score

- `zrangebyscore key min max` 查询得分 [min,max] 的成员

- `zcount key min max` 得分区间的成员数目

- `zremrangebyrank key l r` 删除下标区间段

- `zremrangebyscore key min max` 删除得分区间段

- `zinterstore dest numkeys key1 key2 ...` 将若干个集合合并，score 相加，保存到 dest 这个集合里

```redis
zadd zs1 100 lr580
zadd zs1 0 lr580
zscore zs1 lr580
```

### 配置

#### 基本

Redis运行在保护模式下，保护模式默认是启用的，没有指定绑定地址，没有认证密码要求客户。在这种模式下，连接只接受环回接口。如果你想从外部电脑连接到Redis是不行的。远程链接不能进行 CRUD 等操作，进行CRUD操作会报错。

我们在Redis客户端可以查看Redis是否运行在保护模式下。也可以临时关闭保护模式。

查看当前模式：`config get protected-mode`

临时关闭：`config set protected-mode no`

永久关闭：找到 redis 根目录 `redis.conf`，删除或注释 `bind 127.0.0.1`。

一旦用#注释bind，没有任何限制，只要可以链接redis服务器，都允许使用。

> 如果这里绑定监听的ip（客户端只有通过被绑定的ip才可以连接服务器，利用 redis-cli -h ip地址链接服务器）
>
> 可以绑定多个ip，用空格分开：
>
> bing 127.0.0.1 106.75.101.218(外网可访问当前服务器的ip)
>
> 如果只是绑定127.0.0.1，则表示只有本机的客户端才能访问
>
> 启动一个客户端语法：
>
> redis-cli -p 连接的服务器端口(默认6379) -h连接的服务器ip地址（默认127.0.0.1）

接着，把 `protected-mode no` 修改，即关闭保护模式。

配置一个requirepass的授权密码,否则不安全，如 `requirepass 123456`。可以不配。配了就每次连接都要求提供登录密码。

授权密码在 cli 里：`auth 密码`。

端口号，对分布式其他 redis 示例需要修改。即 `port xxxx`。

当客户端空闲时间达到指定时间时,就会自动断开连接。此处可以指定为3600(1小时)默认为0秒表示不启用超时配置，即 `timeout 3600`。

是否开启后台运行模式，设置为yes,表示开启后台运行模式。

一旦开启后台运行模式，redis服务端启动时，控制台的提示内容会被放到一个日志文件中，而不需要占用一个控制台。

这个日志文件需要在指定地方配置，否则如果出错，错误提示信息没得查看

开启守护进程：`daemonize yes`。

每个进程在linux或者其他操作系统中都会占用pid号，当系统中的进程过多时，需要查找redis进程号可能比较麻烦，直接打开pid文件查看即可。

如果redis启动不成功，可直接查看日志文件。配置位置 `pidfile /var/run/redis_6379.pid`。如果把 redis 搬到根目录，则 `logfile "log6379.log"`。

save 持久化策略，如：

```
save 900 1 #900秒至少数据变动一次save1次
save 300 10
save 60 10000
```

制定 dump 的持久化文件，每个服务器单独指向一个文件 `dbfilename dump6379.rdb`。这样重启时，数据不会错乱。

同一个机子可以启动多个 redis，即：

```
redis-server redis01.conf
redis-server redis02.conf
```

> 复制粘贴配置文件，修改 `port,pidfile,logfile,dbfilename`。也可以批量替换端口号数字。

查看启动情况：

```sh
ps -ef | grep redis
```

或 cat 查看 log。登录：

```sh
redis-cli -p 6379
```

关闭节点：`kill pid`。ps 找 pid(第二列)。

### 集群

#### 理论

##### 基础

主从节点数据自动同步一致。

<img src="img/image-20230430183056448.png" alt="image-20230430183056448" style="zoom:67%;" />

实现集群节点的高可用,支持数据的备份功能是高可用的基础

redis支持数据的主从复制,支持多级主从,一主多从的复制结构,当结构过于复杂时,数据同步的效率就降低了.企业经验,最多2级主从（父、子、孙）,最多一个主6个从节点;

命令和配置文件 slaveOf 

##### 哨兵

哨兵集群模式 sentinel

redis-cluster(Redis3.0)出现之前,绝大部分的redis的使用是围绕哨兵集群展开的;利用单独的进程(特殊redis-server),开启对主从结构的监听;监听主节点,先从主节点调用命令info replication,拿到主从结构的所有信息,保存在内存中,后续每1秒钟发起一次心跳检测(rpc远程通信协议),一旦发现主节点宕机,哨兵集群发起投票选举,最终过半的票数确定事件的结果

<img src="img/image-20230430191349979.png" alt="image-20230430191349979" style="zoom:67%;" />

一个哨兵集群,只能维护一个数据分片的高可用结构,不能完成分布式的结构;

分布式结构如下:

<img src="img/image-20230430191609211.png" alt="image-20230430191609211" style="zoom:67%;" />

客户端连接的是哨兵，从哨兵那里知道哪个节点正在提供服务

为什么要投票?为什么过半?

哨兵监听的主从关系.是状态 ,每一次角色的变化,都由哨兵来监控--对所有的主从结构的决策都需要投票.

哨兵节点需要连接主从结构(涉及到网络通信),只有一个哨兵节点的管理结构,不可信，只有一个哨兵节点，当哨兵连接主从结构的网络异常时，会误判为主从结构的节点出问题。

所以必须引入哨兵集群，管理主从结构.毕竟超过一半的节点都误判了的几率极小;

- 集群容忍度(宕机容忍度)

哨兵集群容忍度:可以允许宕机的个数,就是集群的容忍度;

5个哨兵集群节点:为了达到过半选举至少3个节点;容忍度是2

6个哨兵集群节点:为了过半至少剩余4个节点,容忍度是2

2n-1个集群的容忍度和2n是一样的;一般过半机制的集群,选择都是奇数个;

##### cluster

Redis3.0版本出现的一个新的redis集群结构,引入了很多新的概念和逻辑.将之前学习的主从复制.哨兵监听的逻辑整合到了这个新的结构当中

![image-20230501144148542](img/image-20230501144148542.png)

- 集群节点两两互联,底层依赖二进制传输协议的,优化传输速度，内部之间传输很快.(集群的基础)
- 集群的监控监听机制,交给主节点投票(哨兵集成到了集群的主节点中),任意的主节点宕机,只要剩下的主节点满足过半.
- 客户端连接任意一个节点,都可以向集群传递命令存储和获取数据,数据会在集群内部做节点的连接跳转.
- 引入了新的分布式分片计算逻辑--hash取模(取余,CRC16()散列计算);取模结果[0,..16383],这个范围区间,称之为槽道号--hash slot，每个master负责管理一批槽道号，由于槽道的存在,key值做hash取模运算,根据key的取模结果判断是否归哪个节点管理,从而做到了解耦,key和节点的解耦key--slot--node.

redis 集群计算 key 的办法 [参考](https://segmentfault.com/a/1190000018094567?utm_source=tag-newest) 即对 key 做 CRC16 然后对 16384 取模。

Redis之间进行数据交换的时候消息头所占空间为CLUSTER_SLOTS/8，

也就是16384÷8÷1024=2kb

Redis Cluster要求一个集群最少需要3个主节点(最少的集群容忍度1,3个才能容忍宕一个),由于槽道的限制,理论上最多16384(官方版本提示1000节点左右)

![image-20230501151713303](img/image-20230501151713303.png)

每一个主节点都可以具备槽道管理权,也可以不具备槽道管理权(只具备投票权利)

- 计算CRC16()%16384,得到的整数，如何判断是否为当前节点的管理范围
- 如果不是当前节点管理的槽道范围,如何知道正确的槽道管理者是谁



根据后缀文件，两种redis的持久化策略对比

| rdb                                                      | aof                                                          |
| -------------------------------------------------------- | ------------------------------------------------------------ |
| 使用dump.rdb为默认的持久化文件,存储的是数据内容key-value | 一种追加模式的日志文件，存储的是命令的日志                   |
| 可以一次性存储多条数据(执行效率高)                       | 默认每秒同步执行的写命令(执行效率低)                         |
| save时间间隔长,对数据的备份能力要求高时不推荐使用rdb     | 备份数据时间比较短,最多最多,在极大的高并发先只会丢失1-2秒的数据 |
| 一次性存储数据过大时,有可能造成阻塞,影响使用             | 细水长流                                                     |
| 数据量并发少的时候rdb性能更高                            | 数据量并发到达一定程度aof反而性能高于rdb                     |

<img src="img/image-20230501232248649.png" alt="image-20230501232248649" style="zoom: 67%;" />

如果同时开启aof和rdb,aof优先级高于rdb;重启之后最先加载了

##### 槽道

槽道的组成部分：

- 16384位的二进制,本质上一个2048个元素的byte数组（1个8位，8*2048=16384）;通过对二进制的计算,获取(自定义的)下标,下标正好对应的槽道号,每个维护槽道的节点都会存储这样一个二进制的值,管理的和不管理的槽道号都会在二进制中体现,管理的槽道号对应的二进制值是 "1"/true,不管理的就是"0"/false,当一个key值传递到节点上做hash取模计算之后得到的0-16383的整数,到二进制判断结果1/0
- 16384个元素的数组,数组的元素值是一个引用的变量,该变量指向内存中保存的集群所有节点的信息的对象数据,每个节点都维护了一个完全一样的数组对象,只要判断槽道所属权是false将会调用这个数组的元素值(利用数组下标 0-16383),元素值的意义就是下标对应槽道号的管理节点信息的引用对象

![image-20230502141406854](img/image-20230502141406854.png)

二进制的值的特点**:**

主节点: 有槽道管理权的主节点,各自二进制不相同,自己管理的槽道的二进制是1,不是自己管理的槽道的二进制全是0;没有槽道管理权的节点.二进制值是16384位都是0

从节点:从节点没有槽道管理权，由于它备份了主节点的位序列，在key的槽道号计算完毕后,直接到索引数组中获取节点信息

创建集群时将该主节点管理的槽道号对应的二进制值从0变成1



索引数组/共享数组

节点相互握手的过程中,每个节点都会记录集群的所有节点的信息对象

<img src="img/image-20230502142251335.png" alt="image-20230502142251335" style="zoom:67%;" />

创建集群时,需要节点与节点进行握手meet,一旦meet成功相当于把对方的信息对象(ip,id,port,ms关系,time)抓取到了自己的内存管理保存

对槽道的相关操作,会在每个节点中的数组对象里保存一个引用值

<img src="img/image-20230502142608678.png" alt="image-20230502142608678" style="zoom: 67%;" />

集群最终创建完毕之后,每个节点内的数组结构

<img src="img/image-20230502142627585.png" alt="image-20230502142627585" style="zoom:67%;" />

<img src="img/image-20230502142828732.png" alt="image-20230502142828732" style="zoom:67%;" />

数组中每一个下标对应的都是槽道号,对应元素内容,除了有管理槽道的节点的对象引用外,还有其他数据(key值的对应关系)

#### Ruby

##### 安装

安装，先搞个 `tar.gz` 包解压到 `/usr/local`(其他路径最后 gem 会炸)，然后：

```
./configure
make && make install
```

查看版本：

```sh
ruby -v
```

安装 redis 接口：

```sh
gem install redis
```



#### 实例

##### 一主二从

复制上文 配置-基本 配好的一个 `redis.conf`，复制三份，批量换关键字，端口全改了(可能是6个关键字)。如端口改为 6382-6384。

默认情况下，所有的节点都是master。故从节点添加：(主节点ip和端口号)

```properties
slaveof 192.168.126.128 6382
```

启动各redis，`redis-server redis638x.conf`。CLI 登进各个去查看信息：

```redis
info replication
```

测试主节点写数据，然后主从都查数据，发现查到一样。从节点无权写数据。如果从节点能写(多级主从)，需要设置：

```properties
slave-read-only no
```

如果主节点宕机(如直接执行 shutdown)，则从节点不会变成主节点，什么也不做，只实现数据备份

##### 哨兵集群

根目录找到 `sentinel.conf` 哨兵模板配置文件，复制三份，修改：

- 保持注释 `# bind 127.0.0.1 192.168.1.1`

- 修改 `protected-mode no`

- 端口号 `port 26379` (其余两个为 `26380,26381`)是唯一哨兵间不同的地方

- 监听 `sentinel monitor mymaster 192.168.126.128 6382 2` (IP和端口根据上文)

  mymaster 自定义的名称,表示当前哨兵监听的一个主从结构的代号,也是其他哨兵判断是否是同一个集群的依据

  2 表示主观下限票数, 当集群哨兵不断宕机之后,最少剩下的节点数量

启动三个哨兵节点，每个用一个窗口，看日志信息：

```sh
redis-sentinel sentinel01.conf
```

查看输出，发现每个哨兵启动时，别的哨兵能马上监听到

如果Down掉其中一个哨兵节点。剩下两个也够投票

这时候关掉 master(6382)，经过一会儿，发现重新选举了一个主节点，如 6384

主节点一旦恢复,重新添加到集群以从节点角色继续提供服务。

查看哨兵的 conf，会发现自动追加了新东西，开头为：

```properties
# Generated by CONFIG REWRITE
```

如果主节点换掉了，上文的监听 sentinel monitor 也会自动改。

因此，每次重启哨兵节点的话，需要清理掉自动追加的内容，将监听端口号手动改回去。

一些信息解释

![image-20230430195727789](img/image-20230430195727789.png)

sentinel current-epoch 的作用：当 slave A 发现其所属的 master 下线时，就会试图发起故障转移流程。首先就是增加 current-epoch的值，这个增加后的 current-epoch的值是所有集群节点中最大的。然后slave A 向所有节点发起拉票请求，请求其他 节点投票给自己，使自己能成为新的 master。其他节点收到包后，发现发送者的 current-epoch比自己的 current-epoch 大，就会更新自己的 current-epoch，并在尚未投票的情况下，投票给 slave A，表示同意使其成为新的 master

哨兵节点开放防火墙：

```sh
firewall-cmd --query-port=26379/tcp
firewall-cmd --zone=public --add-port=26379/tcp --permanent
firewall-cmd --reload
firewall-cmd --query-port=26379/tcp
```

> 一行式子：
>
> ```sh
> firewall-cmd --query-port=26381/tcp;firewall-cmd --zone=public --add-port=26381/tcp --permanent;firewall-cmd --reload;firewall-cmd --query-port=26381/tcp
> ```

再把一主二从也开了(如果连不上)。

##### cluster

根目录配文件夹：

```sh
mkdir 8000 8001 8002 8003 8004 8005
```

复制一份根据上文 `配置-基本` 配好的 `redis.conf` 为 `redis-cluster.conf`，端口搞同一个，修改下述内容：

```properties
appendonly yes
appendfilename "appendonly6379.aof"
cluster-enabled yes
cluster-config-file nodes-6379.conf
```

将这份文件复制到上述文件夹里，端口号分别改为文件夹名

```sh
cp redis-cluster.conf 800x/
```

分别启动每个节点：

```sh
redis-server 800x/redis-cluster.conf
```

启动完后，发现无法操作任何一个 server 如 `redis-cli -p 800x` 进去不能 set

根据 `Ruby` 一节，安装好 `Ruby`，进到 `redis-3.2.11/src`，执行下面命令查看是否安装成功：

```sh
./redis-trib.rb
```

如果成功，搭建一个 3 主的结构，键入 yes：

```sh
./redis-trib.rb create 192.168.126.128:8000 192.168.126.128:8001 192.168.126.128:8002
```

现在登录 `redis-cli -c -p 8000`，键入：

```
cluster info
cluster nodes
```

信息内容解释：

| f73d56cfdcfe38e039f2e9dc97d37f494c6cba93 | 192.168.243.133:8002 | master | -                  | 0 1653911525764                                | 3    | connected | 10923-16383 |
| ---------------------------------------- | -------------------- | ------ | ------------------ | ---------------------------------------------- | ---- | --------- | ----------- |
| 节点id                                   | 节点host地址:端口    | 角色   | 节点对应的主节点id | 操作时间相关(时间戳，1970-1-1 0点到现在的秒数) | 序号 | 连接状态  | 管理槽道号  |

尝试 `set` 点什么，会先自动跳到对应的 server。

现在动态添加一个主节点，在 shell 在 `src/`

```sh
./redis-trib.rb add-node 192.168.126.128:8003 192.168.126.128:8000
```

看 `cluster nodes`，可以看到 8003 并没有获得任何槽道管理权，即不管理任何数据

挂从节点，先查询被挂主节点的 id，如 `5910513a54853005314120b4c2e1afbdf5a1ddef`，则：

```sh
./redis-trib.rb add-node --slave --master-id 5910513a54853005314120b4c2e1afbdf5a1ddef 192.168.126.128:8004 192.168.126.128:8003
```

即：以slave角色来添加8004节点，将8004节点挂接到8001（id: 5910513a54853005314120b4c2e1afbdf5a1ddef）,挂接或加入时通过8003来通知集群的其他节点

看 `cluster nodes` 检验。将主节点 8001 shutdown 掉，再次查询(需要等几秒)，发现成为了 master，取得了主节点的槽道。

再次启动主节点 8001，成为了 slave 且无槽道。

槽点转移，将 8002 的 200 个给 8003，先输入下面指令，然后依次输入① 200 和② 8003 的 id 和③ all(均分) 或 8002 的 id，④yes。

```
./redis-trib.rb reshard 192.168.126.128:8001
```

reshard不支持非空迁移(有数据的槽道),可以使用cluster集群提供的底层命令一步一步完成有数据的迁移(数据微调)

假设选了 all，发现每个 master 平均给了 60 多个槽道，否则，8002 给了 200 个

只能删除从节点或者没有槽道管理权的主节点,一旦删除,为了不影响集群使用,删除的节点自动关闭;如果删除的节点重新启动想重新添加到集群(做不到)

下面试图删除 8001，获取其 id，如：

```sh
./redis-trib.rb del-node 192.168.126.128:8001 5910513a54853005314120b4c2e1afbdf5a1ddef
```

8001不仅被删除，还被关闭，关闭的原因,是因为删除的节点不会修改配置文件node-8001.conf,8001自己以为节点启动,依然会以集群的角色出现,但是其他节点都不知道8001。

查看nodes-8001.conf，可以看到8001经过删除和关闭后，还认为自己在集群中。

尝试 up(即先 `redis-server` 再 `add-node`)，会发现 `[ERR]`。要解决，先关 8001 server，删掉根目录下述文件：

```
rm appendonly8001.aof nodes-8001.conf dump8001.rdb
```

再次启动和 add-node，发现成功。

如果集群坏了要修复，先 shutdown 或 shell kill 所有节点，然后删掉所有的持久化文件(如果是测试环境)：

```sh
rm -rf appendonly800* dump800* nodes-800*
```

启动：

```sh
redis-server 8000/redis-cluster.conf && redis-server 8001/redis-cluster.conf && redis-server 8002/redis-cluster.conf && redis-server 8003/redis-cluster.conf && redis-server 8004/redis-cluster.conf && redis-server 8005/redis-cluster.conf
```

重新搭建：

```sh
./redis-trib.rb create --replicas 1 192.168.126.128:8000 192.168.126.128:8001 192.168.126.128:8002 192.168.126.128:8003 192.168.126.128:8004 192.168.126.128:8005
```

预期看到的结果是3主3从，每个从对应不同的主。

记得开一开端口：

```sh
firewall-cmd --add-port=8000/tcp --permanent && firewall-cmd --reload && firewall-cmd --query-port=8000/tcp
```



##### 手动cluster

不借助 ruby。参考例子：

启动六个节点，通过 `redis-cli -c -p 8000 -h 192.168.126.128`，然后连五个集群：

```
cluster meet 192.168.126.128 800x
```

如果查询，会发现每个集群都没分配槽道，用一个 sh 脚本如 `8000slots.sh` 分配槽道：

```sh
for port in {0..5460};
	do redis-cli -h 192.168.126.128 -c -p 8000 cluster addslots $port;
done
```

执行：`sh 8000slots.sh`

如果权限不够，则先 `chmod u+x 8000slots.sh`，通过 `ll` 查看权限

同理，编写脚本对 8001 分配 5461-10922。8002 分配 10923-16383。

刚刚 meet 进来的都是 master。要改变某个节点为从节点，对该节点：

```
cluster replicate masterID
```

此时如果成功，在从想执行 set 就会自动跳到主

槽道迁移：(以8002 迁移 10923 给 8001 为例)

- 判断管理该槽道的节点的该槽道是否为空 \[被导出方如8002必须为空\]

  ```
  cluster getkeysinslot 槽道起始数字 槽道数量 
  ```

- 被导入方(8001)设置导入节点的id(8002)

  ```
  cluster setslot 槽道号 importing 被导出方ID
  ```

- 被导出方(8002)设置导出槽道

  ```
  cluster setslot 槽道号 migrating 被导入方ID
  ```

- 通知包括当事人的所有集群过半主节点，即过半主节点执行：

  ```
  cluster setslot 槽道号 node 被导入方ID
  ```

如果要迁移的槽道非空：

- 判断是否为空

- 被导入方执行：

  ```
  cluster setslot 槽道号 importing 被导出方
  ```

- 被导出方设置导出槽道

- 被导出方数据迁移

  ```
  migrate 被导出IP地址 被导出端口号 ""(key的匹配) 数据分区号(0-15) 超时毫秒(500) keys 键名1 键名2 ...
  ```

- 通知节点



# 分布式

## 理论

### 大数据

传统关系型数据库：`Oracle、openGauss、SQL Server、Mysql、DB2、SyBase、SqlLite`，容易理解、使用方便、易于维护。

缺点：

- 扩展性差（仅几十台），成本高；仅本地磁盘存储
- 难以满足海量数据（TB级）的高效率读写；
- 难以满足非常高的并发读写需求；

![image-20230228110559638](img/image-20230228110559638.png)

大数据：分布式存储，分布式计算； 普通服务器搭建；节点故障不影响集群

大数据：无法用常规工具对其内容进行分析处理的海量数据集合。

大数据技术：从多种类型的海量数据中快速获得有价值信息的技术

NoSQL（Not Only SQL）技术；对海量数据进行存储、管理、分析的系统：大数据库。大数据库+ 软件环境：大数据库系统

<img src="img/image-20230228110644593.png" alt="image-20230228110644593" style="zoom:50%;" />

<img src="img/image-20230228110655049.png" alt="image-20230228110655049" style="zoom: 50%;" />

NoSQL技术：根据CAP理论，遵循BASE原则设计。

**CAP理论**：一个分布式系统不能同时满足一致性(consistency)、可用性(availability)和分区容错性(partition tolerance)这三个需求，最多只能同时满足两个

> GPT4：
>
> 1. 一致性：多个副本之间能够保持一致的特性，最高级别的要求是任何读操作都能返回最新的写入结果
> 2. 可用性：所有的请求都能在有限的时间内返回结果。但是这个结果并不一定是最新的
> 3. 分区容错性：分区是因为故障导致的不连通的若干区域，即在存在分区时仍能保证对外提供服务
>
> 如：
>
> - 存在分区时，不可能同时保持一致性和可用性；
>   - 分区+一致，则拒绝部分请求故不可用
>   - 分区+可用，则一部分数据不一致
>   - 一致+可用，不能分区

> 分布式系统中，节点组成的网络应该是连通的。然而因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据散布在这些不连通的区域中，这就叫分区。
>
> 当一个数据项只在一个节点中，分区出现后，和这个节点不连通的部分就访问不到这个数据。这时分区是无法容忍的。
>
> 提高分区容忍性的办法：一个数据项复制到多个节点上，出现分区后，这一数据项就可能分布到各个区里。容忍性就提高了。
>
> 数据复制到多个节点：一致性问题，多个节点的数据可能不一致。要保证一致，每次写操作要等待全部节点写成功，等待会带来可用性问题。
>
> 数据存在的节点越多，分区容忍性越高，要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低

**BASE原则**是指Basically Available（基本可用）、Soft State（软状态）和Eventually Consistent（最终一致性）。

- 基本可用是指分布式系统在出现不可预知的故障的时候，允许损失部分可用性。 
   a、时间上的损失：正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户的查询结果，但由于出现故障，查询结果的响应时间增加到1-2秒。 
   b、功能上的损失：正常情况下，在一个电子商务网站上进行购物，消费者几乎能够顺利的完成每一笔订单；

  购物高峰（双十一），购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面

- 软状态: 不用完全符合ACID的原子性
  把订单状态改成已支付成功，告诉用户已经成功了；再异步发送消息通知积分服务和仓库服务，即使消息失败,MQ消息也会重新发送（重试）

  <img src="img/image-20230228110909914.png" alt="image-20230228110909914" style="zoom:50%;" />

  <img src="img/image-20230228110917075.png" alt="image-20230228110917075" style="zoom:50%;" />

  ACID原则：Atomicity原子性 Consistency一致性 Isolation隔离性 Durability持久性

- 最终一致性：系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一致的状态。

  不需要实时保证强一致性

BASE原则： 对CAP中一致性和可用性权衡的结果，大规模分布式系统实践的总结；

BASE原则的核心：无法做到强一致性，可以根据自身业务特点，采用适当的方式来使系统达到最终一致性



按数据存储方式和处理数据的类型分类：

![image-20230228111145543](img/image-20230228111145543.png)

<img src="img/image-20230228111219808.png" alt="image-20230228111219808" style="zoom: 50%;" />



### Hadoop

#### 简介

Hadoop是一个由Apache基金会所开发的[分布式系统](https://baike.baidu.com/item/分布式系统/4905336)基础架构

三大核心组件：

- 分布式文件系统 解决海量数据存储（HDFS）Hadoop Distributed File System
- 并行计算框架 海量数据分析（MapReduce）
- 资源管理平台 资源管理调度问题（YARN）Yet Another Resource Negotiator

<img src="img/image-20230228111448743.png" alt="image-20230228111448743" style="zoom:50%;" />

<img src="img/image-20230228111544670.png" alt="image-20230228111544670" style="zoom:50%;" />

#### HDFS

HDFS是根据Google的GFS来实现的

HDFS是主从结构的，有主节点（NameNode）和从节点（DataNode）。一个主节点可关联多个从节点，一个从节点也可关联多个主节点。从节点又称数据节点。每一个block会在多个DataNode上存储多份副本

<img src="img/image-20230228112314376.png" alt="image-20230228112314376" style="zoom:50%;" />

在存储数据的时候，会将数据进行切块。Block是HDFS的基本存储单位。可以通过dfs.blocksize来更改块的大小，在更改的时候，单位是字节。如果一个文件的大小不足128M，那么这个文件是多大，在HDFS上就占多大

在HDFS中，会对Block进行编号，我们称之为Block ID

分块的好处：便于存储超大文件，便于进行快速的备份

为了防止产生数据丢失，会将数据进行备份，备份称为副本。在Hadoop中，默认的副本数量为3

<img src="img/image-20230228112324388.png" alt="image-20230228112324388" style="zoom: 33%;" />

<img src="img/image-20230228112503917.png" alt="image-20230228112503917" style="zoom:33%;" />

namenode 管理DataNode和记录元数据Meta，元数据包括：①记录数据的虚拟存储路径、记录文件的切块数量、记录每一个数据块的存放位置、数据块的副本数量、文件权限。元数据的大小是在150B左右（130B-180B）

NameNode将元数据维系在内存以及磁盘中，在内存中的目的是为了快速查询，在磁盘中的目的是为了崩溃恢复。元数据在磁盘中是以edits文件和fsimage文件的形式存在。dits：记录写操作；fsimage:记录元数据，fsimage中的元数据和内存中的元数据并不是同步的

当NameNode接收到写请求之后，会将该请求记录到edits_inprogress文件中，如果记录成功，则将该请求同步更新到内存中，修改内存中的元数据，内存修改完成之后会给客户端返回一个ack表示成功

hadoop.tmp.dir属性：其他临时目录的基础，如果不配置，就是根目录下的/tmp

在HDFS中，会给每一个写操作分配一个编号叫做事务id，在HDFS中叫txid。当edits文件达到条件的时候会将操作更新到fsimage文件中，修改fsimage文件的元数据：①空间维度：当edit_inprogess文件达到指定大小的时候就会触发更新，默认是64M，大小可以由fs.checkpoint.size（core-site.xml）来进行指定，默认单位是字节；②时间维度：当距离上次更新时间达到指定间隔时间时就会触发更新，默认是1小时，时间大小由fs.checkpoint.period来指定，默认单位是秒；③重启更新：NameNode重启之后，会自动的将edits_inprogress中的操作更新到fsimage中；④强制更新：hadoop dfsadmin -rollEdits。

在更新的时候，会将edits_inprogress重命名为edits_xxxxxx-xxxxxx,同时会产生一个新的edits_inprogress。在Hadoop中，如果存在SecondaryNameNode，则更新过程是发生在SecondaryNameNode上。在HDFS中，最核心的节点是NameNode，但是在Hadoop1.0版本中只能有一个NameNode,在Hadoop2.0版本中，允许多设置一个NameNode，代价是丢掉SecondaryNameNode。

NameNode通过心跳机制来管理DataNode：DataNode每隔定长时间会给NameNode发送心跳信息，默认情况下，DataNode每隔3秒给NameNode发送一条信息，如果NameNode长时间（默认是10分钟）没有收到某个DataNode的心跳信息，则认为这个DataNode已经lost（丢失），此时NameNode会将这个DataNode中的数据再次备份，保证副本数量。心跳信息包括：当前节点的状态、当前节点中所存储的数据块信息。

NameNode重新启动的时候，将edits中的操作更新到fsimage中，将fsimage中的元数据加载到内存中，等待DataNode的心跳（如果DataNode一直没有心跳过来，则要重新备份保证副本数量，校验数据总量），这个过程称之为安全模式(safe mode)。如果所有的校验都成功则HDFS会自动退出安全模式。

退出安全模式的命令（极少用）：hadoop dfsadmin -safemode leave

因为安全模式问题，所以在伪分布下，副本数量必须为1-如果副本数量不是为1，则重启NameNode的时候，会导致HDFS一直处于安全模式

副本放置策略：

\1)  第一个副本：如果是外部客户端上传数据，则此时NameNode会选择一个相对空闲的节点，存放第一个副本；如果是内部上传，则第一个副本是放在本节点上。

\2)  第二个副本：在hadoop2.7以前，第二个副本要放在第一个副本的不同**机架**的节点上。从hadoop2.7以后的版本，第二个副本是放在第一个副本相同机架的节点上。

\3)  第三个副本：在hadoop2.7以前，第三个副本是放在和第二副本的同个机架上。从hadoop2.7以后的版本，第三个副本是放在第二个副本不同机架的节点上。

\4)  如果有更多副本：则随机挑选空闲节点存放。

在Hadoop所指的机架并不是物理结构，而是逻辑结构。

可以通过映射关系来指定节点对应的机架，也就意味着同一个物理机架上的节点可以映射到不同的逻辑机架上。

实际使用中，一般会将一个或者几个物理机架上的节点放在一个逻辑机架上

datanode

\1.  用于存储数据，注意数据是以Block形式存储

\2.  数据在DataNode上的存储位置由hadoop.tmp.dir属性决定。存储目录是：

/home/hadoop-2.7.3/tmp/hdfs/data/current/块池/current/finalized/subdir0/subdir0

\3.  DataNode会通过心跳机制（RPC方式）来向NameNode发送心跳信息

<img src="img/image-20230228112525278.png" alt="image-20230228112525278" style="zoom:33%;" />

#### MapReduce

分布式计算框架，基于它写出来的应用程序能够运行在Hadoop集群上。MapReduce采用“分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个从节点共同完成，然后通过整合各个节点的中间结果，得到最终结果。简单地说，MapReduce就是“任务的分解与结果的汇总”

<img src="img/image-20230228112556074.png" alt="image-20230228112556074" style="zoom:50%;" />

Map阶段负责数据的整理，Reduce阶段负责数据的汇总

Reducer中的迭代器只能遍历一次，如果Mapper和Reducer结果类型一致，可以只设置一个

果输入路径是一个文件，则MapReduce只处理这个文件，如果输入的是一个目录，则处理这个目录下的所有文件。注意：如果文件以“\_”开头，则该文件会被跳过，在MapReduce中，以“\_”开头的文件被认为是隐藏文件而不需要处理

mapper中的4个泛型：

`Mapper<LongWritable, Text, Text, Text>`

根据实际情况改变以上参数的类型

keyin：数据偏移量；长整型

valuein：输入的一行数据； 字符串

keyout：输出的健类型：

valueout：输出的值类型

传输的数据要能够被序列化



#### Yarn

资源管理、任务调度分开

| **组件**                | **功能**                                                     |
| ----------------------- | ------------------------------------------------------------ |
| ResourceManager(RM)     | 负责对各NM上的资源进行统一管理和调度。将AM分配空闲的Container运行并监控其运行状态。对AM申请的资源请求分配相应的空闲Container。 |
| NodeManager (NM)        | NM是每个节点上的资源和任务管理器。它会定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态；同时会接收并处理来自AM的Container 启动/停止等请求。 |
| ApplicationMaster  (AM) | 应用框架，它负责向ResourceManager协调资源，并且与NodeManager协同工作完成Task的执行和监控 |
| Container               | Container是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container 表示的。 |

#### 其他

| 其它Hadoop生态圈组件 | 功能                                                         |
| -------------------- | ------------------------------------------------------------ |
| HBase                | 一个建立在HDFS之上，面向列的针对结构化数据的可伸缩、高可靠、高性能、分布式和面向列的动态模式数据库。 |
| Hive                 | Hive定义了一种类似SQL的查询语言(HQL),将SQL转化为MapReduce任务在Hadoop上执行，通常用于离线分析。 |
| Spark                | 一种与  Hadoop 相似的开源集群计算环境，它基于内存计算，数据分析速度更快。 |
| Mahout               | 创建一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。 |
| Pig                  | 提供一种基于MapReduce的数据分析工具。                        |
| Zookeeper            | 解决分布式环境下的数据管理问题                               |
| Sqoop                | 主要用于传统数据库和Hadoop之间传输数据。                     |



### HBase

#### 简介

> BigTable是一个分布式存储系统，起初用于解决典型的互联网搜索问题
>
> 1 爬虫持续不断地抓取新页面，这些页面每页一行地存储到BigTable里
>
> 2 MapReduce计算作业运行在整张表上，生成索引，为网络搜索应用做准备
>
> •BigTable是一个分布式存储系统
>
> •利用谷歌提出的MapReduce分布式并行计算模型来处理海量数据
>
> •使用谷歌分布式文件系统GFS作为底层数据存储
>
> •可以扩展到PB级别的数据和上千台机器，具备广泛应用性、可扩展性、高性能和高可用性等特点
>
> •谷歌的许多项目都存储在BigTable中，包括搜索、地图、财经、打印、社交网站Orkut、视频共享网站YouTube和博客网站Blogger等

HBase是一个高可靠、高性能、面向列、可伸缩的分布式数据库，是谷歌BigTable的开源实现，主要用来存储非结构化和半结构化的松散数据。HBase的目标是处理非常庞大的表，可以通过水平扩展的方式，利用廉价计算机集群处理超过10亿行数据和数百万列元素组成的数据表 

![image-20230302163501835](img/image-20230302163501835.png)

|              | **BigTable** | **HBase**         |
| ------------ | ------------ | ----------------- |
| 文件存储系统 | GFS          | HDFS              |
| 海量数据处理 | MapReduce    | Hadoop  MapReduce |
| 协同服务管理 | Chubby       | Zookeeper         |

Hadoop：解决大规模数据的离线批量处理问题；MapReduce:高延迟数据处理机制，无法满足大规模数据实时处理应用的需求

HDFS面向批量访问模式，不是随机访问模式

传统的关系数据库无法应对数据规模剧增时导致的系统扩展性和性能问题（分库分表也不能很好解决）

传统关系数据库在数据结构变化时一般需要停机维护；空列浪费存储空间

HBase已成功应用于互联网服务领域、在线数据分析处理系统

HBase与传统关系数据库的区别：

（1）数据类型：关系数据库采用关系模型，具有丰富的数据类型和存储方式；HBase采用简单的数据模型，把数据存储为未经解释的字符串

（2）数据操作：关系数据库包含丰富的操作，涉及复杂的多表连接。HBase不存在复杂的表与表之间的关系，只有简单的插入、查询、删除、清空；HBase在设计上避免复杂的表和表之间的关系

（3）存储模式：关系数据库：基于行模式存储。

Hbase：基于列存储，每个列族都由几个文件保存，不同列族的文件是分离的

（4）数据索引：关系数据库：可以针对不同列构建复杂的多个索引，以提高数据访问性能；

 HBase只有一个行键索引，通过巧妙的设计，HBase中的所有访问方法，或者通过行键访问，或者通过行键扫描，从而使得整个系统不会慢下来

（5）数据维护：关系数据库：更新操作会用最新的当前值去替换记录中原来的旧值，旧值被覆盖后不会存在。

 HBase中执行更新操作时，不会删除旧版本，而是生成一个新的版本

（6）可伸缩性：关系数据库很难实现横向扩展，纵向扩展的空间也比较有限。

 HBase能轻易地通过在集群中增加或者减少硬件数量来实现性能的伸缩

<img src="img/image-20230302164008689.png" alt="image-20230302164008689" style="zoom:50%;" />

用户在表中存储数据，每一行都有一个可排序的行键和任意多的列

表在水平方向由一个或者多个列族组成，一个列族中可以包含任意多个列，同一个列族里面的数据存储在一起

列族支持动态扩展，可以很轻松地添加一个列族或列，无需预先定义列的数量以及类型，所有列均以字符串形式存储，用户需要自行进行数据类型转换

<img src="img/image-20230302164300173.png" alt="image-20230302164300173" style="zoom:67%;" />

表：HBase采用表来组织数据，表由行和列组成，列划分为若干个列族

行：每个HBase表都由若干行组成，每个行由行键（row key）来标识。

列族：一个HBase表被分组成许多“列族”（Column Family）的集合，它是基本的访问控制单元

列限定符：列族里的数据通过列限定符（或列）来定位

单元格：在HBase表中，通过行、列族和列限定符确定一个“单元格”（cell），单元格中存储的数据没有数据类型，总被视为字节数组byte[]

时间戳：每个单元格都保存着同一份数据的多个版本，这些版本采用时间戳进行索引

“四维坐标”，即[行键, 列族, 列限定符, 时间戳]，如：

| 键                                            | 值            |
| --------------------------------------------- | ------------- |
| [“201505003”, “Info”, “email”, 1174184619081] | “xie@qq.com”  |
| [“201505003”, “Info”, “email”, 1174184620720] | “you@163.com” |

<img src="img/image-20230302165311810.png" alt="image-20230302165311810" style="zoom:67%;" />

<img src="img/image-20230302165339030.png" alt="image-20230302165339030" style="zoom:50%;" />

•HBase包括2个主要组件：

–（1）一个Master主服务器

–（2）多个Region服务器

主服务器Master负责管理和维护HBase表的分区信息，维护Region服务器列表，分配Region，负载均衡

Region服务器负责存储和维护分配给自己的Region，处理来自客户端的读写请求

客户端不是直接从Master主服务器上读取数据，而是在获得Region的存储位置信息后，直接从Region服务器上读取数据

客户端不依赖Master，通过Zookeeper来获得Region位置信息，大多数客户端不和Master通信，这种设计方式使得Master负载很小 

一个HBase表被划分成多个Region，一个Region会分裂成多个新的Region。开始只有一个Region，后来不断分裂。Region拆分操作非常快，接近瞬间，因为拆分之后的Region读取的仍然是原存储文件，直到“合并”过程把存储文件异步地写到独立的文件之后，才会读取新文件

<img src="img/image-20230302165638571.png" alt="image-20230302165638571" style="zoom:50%;" />

<img src="img/image-20230302165704734.png" alt="image-20230302165704734" style="zoom:50%;" />

每个Region默认大小是100MB到200MB（2006年以前的硬件配置）

每个Region的最佳大小取决于单台服务器的有效处理能力

目前每个Region最佳大小建议1GB-2GB（2013年以后的硬件配置）

同一个Region不会被分拆到多个Region服务器

每个Region服务器存储10-1000个Region

<img src="img/image-20230302175216536.png" alt="image-20230302175216536" style="zoom:50%;" />

元数据表，.META.表，存储Region和Region服务器的映射关系

HBase表很大时， .META.表会被分裂成多个Region

根数据表，又名-ROOT-表，记录所有元数据的具体位置

-ROOT-表只有唯一一个Region

Zookeeper文件记录了-ROOT-表的位置

<img src="img/image-20230302175511827.png" alt="image-20230302175511827" style="zoom:50%;" />

| **层次** | **名称**      | **作用**                                                     |
| -------- | ------------- | ------------------------------------------------------------ |
| 第一层   | Zookeeper文件 | 记录了-ROOT-表的位置信息                                     |
| 第二层   | -ROOT-表      | 记录了.META.表的Region位置信息  -ROOT-表只能有一个Region。通过-ROOT-表，就可以访问.META.表中的数据 |
| 第三层   | .META.表      | 记录了用户数据表的Region位置信息，.META.表可以有多个Region，保存了HBase中所有用户数据表的Region位置信息 |

为了加快访问速度，.META.表的全部Region都会被保存在内存中

假设.META.表的每行（一个映射条目）在内存中大约占用1KB，并且每个Region限制为128MB，那么，上面的三层结构可以保存的用户数据表的Region数目的计算方法是：

（-ROOT-表能够寻址的.META.表的Region个数）×（每个.META.表的 Region可以寻址的用户数据表的Region个数）

一个-ROOT-表最多只能有一个Region，也就是最多只能有128MB，按照每行（一个映射条目）占用1KB内存计算，128MB空间可以容纳128MB/1KB=$2^{17}$行，也就是说，一个-ROOT-表可以寻址这么多个.META.表的Region。

同理，每个.META.表的 Region可以寻址的用户数据表的Region个数是128MB/1KB。

最终，三层结构可以保存的Region数目是(128MB/1KB) × (128MB/1KB) = $2^{34}$个Region

•为了加速寻址，客户端会缓存位置信息，同时，需要解决缓存失效问题

寻址过程客户端只需要询问Zookeeper服务器，不需要连接Master服务器

<img src="img/image-20230302175929565.png" alt="image-20230302175929565" style="zoom:50%;" />

客户端包含访问HBase的接口，同时在缓存中维护着已经访问过的Region位置信息，用来加快后续数据访问过程

Zookeeper可以帮助选举出一个Master作为集群的总管，并保证在任何时刻总有唯一一个Master在运行，这就避免了Master的“单点失效”问题

Zookeeper是一个很好的集群管理工具，被大量用于分布式计算，提供配置维护、域名服务、分布式同步、组服务等。

<img src="img/image-20230302180034139.png" alt="image-20230302180034139" style="zoom: 50%;" />

主服务器Master主要负责表和Region的管理工作：

–管理用户对表的增加、删除、修改、查询等操作

–实现不同Region服务器之间的负载均衡

–在Region分裂或合并后，负责重新调整Region的分布

–对发生故障失效的Region服务器上的Region进行迁移

. Region服务器

–Region服务器是HBase中最核心的模块，负责维护分配给自己的Region，并响应用户的读写请求

行键是按照字典序存储，因此，设计行键时，要充分利用这个排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。

例：如果最近写入HBase表中的数据是最可能被访问的，可以考虑将时间戳作为行键的一部分，由于是字典序排序，所以可以使用Long.MAX_VALUE - timestamp作为行键，这样能保证新写入的数据在读取时可以被快速命中



#### 配置理论

HBase有三种运行模式，单机模式、伪分布式模式、分布式模式。
 以下先决条件很重要，比如没有配置JAVA_HOME环境变量，就会报错。
 – JDK
 – Hadoop( 单机模式不需要，伪分布式模式和分布式模式需要)
 – SSH

启动关闭Hadoop和HBase的顺序一定是：
 启动Hadoop—>启动HBase—>关闭HBase—>关闭Hadoop

HBASE_MANAGES_ZK=true，则由HBase自己管理Zookeeper

否则，启动独立的Zookeeper

建议：单机版HBase，使用自带Zookeeper；集群安装HBase则采用单独Zookeeper集群



•[HBase](http://c.biancheng.net/hbase/) 解决不了所有的问题，但是针对某些特点的数据可以使用 HBase 高效地解决，如以下的应用场景。数据模式是动态的或者可变的，且支持半结构化和非结构化的数据。

•数据库中的很多列都包含了很多空字段，在 HBase 中的空字段不会像在关系型数据库中占用空间。

•需要很高的吞吐量，瞬间写入量很大。

•数据有很多版本需要维护，HBase 利用时间戳来区分不同版本的数据。

•具有高可扩展性，能动态地扩展整个存储系统。

•在实际应用中，有很多公司使用 Hbase。

Facebook 公司的 Social Inbox 系统，使用 HBase 作为消息服务的基础存储设施，每月可处理几千亿条的消息；



### Hive

#### 概述

Hive本质上是一个SQL解析引擎，它将SQL语句转译成MapReduce作业并在Hadoop上执行。Hive执行过程如下，其工作原理如图

（1）用户通过用户接口连接Hive，发布Hive QL。
（2）Hive解析查询并制定查询计划。
（3）Hive将查询转换成MapReduce作业。
（4）Hive在Hadoop上执行MapReduce作业

<img src="img/image-20230308145817830.png" alt="image-20230308145817830" style="zoom: 50%;" />

Hive的体系架构如图所示，按功能主要分5大模块：用户接口、Thrift服务器、解析器、MetaStore元数据和Hadoop集群

<img src="img/image-20230308150059030.png" alt="image-20230308150059030" style="zoom:50%;" />

Hive包含以下四种数据模型：内部表（Managed Table）、外部表（External Table）、分区（Partition）和桶（Bucket）。Hive中的数据存储如下图所示

<img src="img/image-20230308150111724.png" alt="image-20230308150111724" style="zoom:50%;" />

（1）内部表（Managed Table）。
每个Hive内部表在HDFS中都有对应目录用来存储表的数据。
内部表的创建过程和数据加载过程可以分别独立完成，也可以在同一个语句中完成。
删除内部表时，表中的数据和元数据会被同时删除。
（2）Hive外部表（External Table）
Hive外部表和内部表在元数据组织上是一样的，但在实际数据存储上有较大差异。
外部表数据不存储在自己表所属目录中，存储在LOCATION指定的HDFS路径中。
外部表仅有一个过程，创建表和数据加载过程同时进行和完成。
删除外部表仅仅是删除外部表对应的元数据，外部表所指向的数据不会被删除。
创建外部表使用EXTERNAL关键字。

（3）分区（Partition）
分区是表的部分列的集合。
一般为频繁使用的数据建立分区，在查找分区中数据时不用扫描全表，有利于提高查找效率。
Hive每个表有一个相应的目录存储数据，表中的的每一个分区对应表目录下的一个子目录，每个分区中的数据存储在对应子目录下的文件中。例如，表member（假定包含分区字段gender）在HDFS的路径为/user/hive/warehouse/member，分区gender=F对应的HDFS路径为/user/hive/warehouse/ member/gender=F，分区gender=M对应的HDFS路径为/user/hive/warehouse/member/gender=M，当导入数据到分区gender=F，则数据存储在/user/hive/warehouse/member/gender=F/000000_0文件中，当导入数据到分区gender=M，则数据存储在/user/hive/warehouse/member/gender=M/000000_0文件中

（4） 桶（Bucket）
桶是将表的列通过Hash算法进一步分解成不同的文件存储。
对指定列计算hash值，根据hash值切分数据，目的是为了并行。
每一个桶对应一个文件（注意和分区的区别）。分区是粗粒度的划分，桶是细粒度的划分，这样可以让查询发生在小范围的数据上，提高查询效率，适合进行表连接查询，适合用于采样分析。比如，要将member表的id列分散至32个桶中，首先对id列的值进行Hash值计算，其中对应Hash值是0的数据存储在/hive/warehouse/member/000000_0文件中，对应Hash值是1的数据存储在/hive/warehouse/ member/000001_0文件中，依次类推。

#### 部署

Hive将metastore元数据存储在RDBMS中，如MySQL、Derby。按metastore存储位置的不同，其部署模式分为内嵌模式、本地模式和完全远程模式三种

##### 内嵌模式

内嵌模式是安装时默认部署模式
元数据信息被存储在Hive自带的数据库Derby中
所示所有组件（如数据库、元数据服务）运行在同一个进程内。
只允许建立一个连接，意味着同一时刻只支持一个用户访问和操作Hive。

<img src="img/image-20230308150909041.png" alt="image-20230308150909041" style="zoom:50%;" />

Hive是基于Hadoop的数据仓库技术，因此部署Hive前要预先部署完成Hadoop稳定版本的集群环境。hive-2.1.1二进制包的安装步骤如下。
（1）下载Hive安装包到/root/tools目录。
（2）解压二进制包到安装目录/usr/cstor，解压后生成子目录apache-hive-2.1.1-bin。
（3）将目录apache-hive-2.1.1-bin重命名为hive，方便后续操作。
（4）为Hive配置Hadoop安装路径。 
（5）在HDFS里新建Hive存储目录。
（6）初始化元数据库，启动Hive，进入Hive运行时环境。
内嵌模式下，启动Hive指的是启动Hive运行时环境。出现“hive>”提示表示Hive正常启动，否则部署失败。

##### 本地模式

本地模式的元数据服务运行在Hive服务主进程中。
当涉及元数据操作时，Hive服务中的元数据服务模块通过JDBC和存储于DB里的元数据数据库进行交互。
本地模式下MySQL数据库与Hive运行在同一台物理机器上
可提供多用户并发访问metastore服务。

<img src="img/image-20230308151038984.png" alt="image-20230308151038984" style="zoom:50%;" />

Hive本地模式安装步骤。
（1）完成Hive基本安装。
（2）安装MySQL程序。
（3）启动MySQL。
（4）创建存储Hive元数据的数据库HiveDB。
（5）退出MySQL，切换到hive的bin目录编辑hive-config.sh文件。

（6）切换到hive的conf目录创建和编辑hive-site.xml文件。
（7）把Java连接MySQL的驱动程序文件mysql-connector-java-x.y.z-bin.jar复制到hive的lib目录。
（8）初始化元数据库，启动Hive，进入Hive运行时环境。
注意：要确保Hadoop集群已经启动和MySQL服务运行正常才可以启动Hive。

##### 远程模式

远程模式的元数据信息是被存储在独立数据库中
元数据可能存储在另一台物理机上，甚至另一种操作系统上。
元数据服务以独立进程运行。
远程模式允许创建多个连接，提供多用户同时访问并操作Hive。

<img src="img/image-20230308151057972.png" alt="image-20230308151057972" style="zoom:50%;" />

远程模式的安装与本地模式类似，最主要的区别是在hive-site.xml配置文件中修改localhost为远程MySQL数据库的IP地址。

#### 配置

Hive启动时会读取相关配置信息，Hive配置可以调优HQL代码执行效率，Hive配置文件在conf子目录下，经常需要根据需求进行修改。Hive配置文件主要有两个。

1. 文件hive-env.sh。指定Hadoop安装路径。
2. 文件hive-site.xml。保存Hive运行时所需要的相关配置参数。
    HIVE_HOME/conf目录中的文件hive-default.xml.template保存着各个配置参数的默认值，用户可以通过在conf目录中创建hive-site.xml并新增特定参数的值来覆盖默认值。
    更改hive-site.xml文件中的配置属性的方法。
    （1）直接用命令vim hive-site.xml编辑hive-site.xml文件。
    （2）使用带-hiveconf选项的Hive命令。
    （3）使用set命令。

#### 接口

Hive主要提供Hive Shell接口、Hive Web接口、Hive API接口、Hcatalog接口、Pig接口和Beeline接口等。

##### Shell

Hive Shell接口运行在Hadoop集群环境上，提供在Hive Shell下执行类SQL命令相关HiveQL操作环境，是Hive提供的标准接口，也是开发者最常用命令行接口。
在Hive提示符后输入HiveQL命令，Hive Shell把HiveQL查询转换为一系列MapReduce作业对任务进行并行处理，然后返回处理结果。通过Shell接口，程序员和分析师很容易编写HiveQL来实现新建表和查询表操作。
安装完Hive后，输入“<HIVE_HOME>/bin/hive”或者“hive --service cli”回车即可启动Hive Shell。初次在Hive Shell下执行命令，会在执行命令操作的机器上创建metastore数据库。

Hive Shell操作有如下特点。
（1）命令必须以分号“;”结束，通知Hive开始执行相应的操作。
（2）命令不要求大小写敏感（除了进行字符串比较相关操作），因此，命令“show table;”将会产生与“SHOW TABLE;”相同的输出结果。
（3）支持Tab键命令自动补全功能，如在hive>提示符后输入SH或SHO按下Tab键会自动补齐为show，输入show ta按下Tab键会显示所有可能的命令。
（4）默认会输出执行过程信息，如执行查询操作所用时间，通过指定-S选项可以禁止输出此类信息，只输出HiveQL执行结果。
Hive Shell的常用命令如下。
（1）清屏。                                            （4）查看表结构。
（2）查看数据库中的表。                        （5）查看HDFS上的文件
（3）查看数据库中的内置函数。              （6）执行操作系统的命令

##### Web

Hive Web接口简称HWI（Hive Web Interface），是Hive Shell接口的一个替代方案（图形化实现），提供了更直观的Web界面，适合数据分析或数据运营人员做即席查询。用户通过浏览器来访问和操作Hive服务端，可以查看Hive数据库模式，执行HiveQL相关操作命令。在浏览器的地址栏输入http://<IP地址>:9999/hwi/回车实现访问Hive Web管理接口。
（1）配置HWI的步骤
打包war文件。
copy相关jar文件到$HIVE_HOME/lib目录下，
修改hive-site.xml配置文件
（2）启动HWI服务，在命令行下输入“hive --service hwi”启动Hive Web管理方式

#### SQL

Hive定义了简单的类SQL查询语句，称HiveQL，与大部分SQL语法兼容，方便熟悉SQL的开发者使用Hive开发和处理复杂的分析工作，还可以用HQL进行查询。

Hive支持基本类型和复杂类型，基本类型包括如下几种。
（1）整数类型：tinyint/smallint/int/bigint。
（2）浮点数类型：float/double/dicemal。
（3）布尔类型：boolean。
（4）字符串类型：string/varchar/char。

复杂类型包括如下几种。
（1）Array：数组类型，由一系列相同数据类型的元素组成。
（2）Map：集合类型，包括Key-Value键值对，可以通过Key来访问元素。
（3）Struct：结构类型，可以包含不同数据类型的元素，这些元素可以通过“点语法”的方式来得到所需要的元素。

DDL操作（Data Definition Language，数据定义语言），常用DDL语句。
（1） create/drop/alter数据库
（2） create/drop/truncate表
（3） alter表/分区/列
（4） Create/Drop/Alter视图
（5） Hive函数
Hive中内置了许多函数，比如日期操作函数day()、year()和month()等，数值操作函数sum()、avg()、max()、min()和count()等。但在某些特殊场景下，可能还是需要自定义函数满足特定功能，这时要用用户自定义函数UDF。

DML（Data Manipulation Language，数据操作语言），常见的操作包括下面几个方面。
（1）将文件中的数据导入（load）到Hive表中
（2） select和filters
（3）数据表连接join操作
（4）将select查询结果导出到Hive的另一个表中
（5）将select查询结果写入文件
写入本地文件系统
写入分布式文件系统HDFS



### Spark

#### 简介

美国加州大学伯克利分校的AMP实验室在2010年发布的一个快速、通用的开源大数据处理引擎 

与Hadoop平台类似, 提供更高效、更快的数据处理，兼容Hadoop生态

当前主流的数据分析、数据流式处理、机器学习平台之一

特点：

- 高效、高性能的批处理

  高效利用内存处理数据；计算中间结果不需要存储到文件系统；作业调度的优化

- 丰富、灵活的编程接口

  编程语言：Java、Scala、Python、R、SQL

  交互式数据处理: Spark Shell、PySpark、Spark SQL CLI

- 灵活、易用的编程模型

  DAG编程模型丰富了map,reduce操作接口，增加了filter、flatMap、union等操作接口

- 多种类数据处理支持

  批处理、流失处理、迭代计算（机器学习、图计算）、交互式查询

- 多数据源支持

  HDFS、Hive、HBase、Parquet等

生态系统 BDAS

<img src="img/image-20230311182512066.png" alt="image-20230311182512066" style="zoom:67%;" />

<img src="img/image-20230311182555857.png" alt="image-20230311182555857" style="zoom:67%;" />

<img src="img/image-20230311182608196.png" alt="image-20230311182608196" style="zoom: 50%;" />

RDD 的转化：

A----groupBy-->B

C----map------>D

D,E--union----->F

B,F---join------>G



#### 部署

单点部署。

Spark集群点部署 —Standalone模式集群的架构

<img src="img/image-20230311183415425.png" alt="image-20230311183415425" style="zoom:50%;" />

Spark集群点部署 —高可用集群

<img src="img/image-20230311183454314.png" alt="image-20230311183454314" style="zoom:50%;" />

配置Master节点本地文件系统恢复

#### 配置

1）Spark属性    应用程序相关配置。优先级由低到高：

1. spark-defaults.conf 配置文件
2. 命令行参数
3. SparkConf对象

常用属性：

| **属性名**              | **默认值** | **含义**                                          |
| ----------------------- | ---------- | ------------------------------------------------- |
| spark.app.name          | (none)     | Spark应用程序的名称                               |
| spark.driver.cores      | 1          | 集群模式下driver所使用的core的数量                |
| spark.driver.memory     | 1G         | Driver进程所使用的内存大小                        |
| spark.executor.memory   | 1G         | 每个executor进程所使用的内存大小                  |
| spark.master            | (none)     | 集群管理器URL                                     |
| spark.submit.deployMode | (none)     | Driver程序的部署模式，取值为：”client”或”cluster” |

2）环境变量     机器相关配置

| **配置项**       | **含义**                  |
| ---------------- | ------------------------- |
| SPARK_LOCAL_IP   | 绑定的IP地址              |
| SPARK_PUBLIC_DNS | Driver程序使用的DNS服务器 |
| SPARK_CLASSPATH  | 额外追加的classpath       |

3）日志配置     日志开关、级别等

#### RDD

Spark Resilient Distributed Dataset

特性：

- 数据集合 与编程语言中的集合类似
- 弹性分布 数据的分片可以自定义
- 可持久化 可缓存，避免重复计算 
- 分布式存储 切分为多个数据块，分散存储在多个节点中
- 只读 一旦生成便不可修改，易于同步处理
- 可重新计算 在出现异常错误的情况下能够重新计算出

| RDD转换       | 含义                                                         |
| ------------- | ------------------------------------------------------------ |
| map(func)     | 通过函数func对数据集中的每个成员进行转换                     |
| filter(func)  | 通过函数func选择过滤数据集中的成员                           |
| flatMap(func) | 和map转换类似，但函数func可以把单个成员转换为多个成员。      |
| union(other)  | 返回当前集合与otherDataset集合的union操作                    |
| distinct      | 去掉集合中重复成员，使新的集合中成员各不相同                 |
| groupByKey    | 对键-值（key-value）对集合按照键（key）进行groupBy操作       |
| sortByKey     | 对键-值（key-value）对集合进行排序                           |
| join(other)   | 对两个键-值（key-value）对集合：（K,V），（K,W）进行连接操作，形成新的键-值对集合：（K，（V,W）） |

RDD依赖关系：RDD转换生成新的RDD, 新的RDD依赖于旧的形成依赖关系

按照RDD分区的依赖关系可分为两种类型：

- 窄依赖 父RDD的每个分区最多被一个子RDD分区所依赖

  <img src="img/image-20230311184213611.png" alt="image-20230311184213611" style="zoom:33%;" />

  <img src="img/image-20230311184205827.png" alt="image-20230311184205827" style="zoom:33%;" />

- 宽依赖 子RDD的每个分区都依赖于父RDD的所有分区或多个分区

  <img src="img/image-20230311184227983.png" alt="image-20230311184227983" style="zoom:33%;" />

RDD行动操作（Action）

| **Action**           | **含义**                                                     |
| -------------------- | ------------------------------------------------------------ |
| collect              | 返回RDD中的所有元素                                          |
| count                | 返回RDD中元素的数量                                          |
| countByKey           | 计算键-值对RDD每个键（key）对应的元素个数                    |
| first                | 返回RDD中第一个元素                                          |
| take(n)              | 返回RDD中前n个元素                                           |
| reduce(func)         | 通过函数func对RDD进行聚合操作                                |
| saveAsTextFile(path) | 把RDD保存为一个文本文件，可以选择保存在本地文件系统、HDFS等。文件中的一行为RDD中的一个元素 |
| foreach(func)        | 通过函数func对RDD中的每个元素进行计算，通常在更新累加器或者使用外部存储系统时用到 |

RDD作业的执行 ，RDD的转换是惰性的（lazy），遇到Action时才会执行

<img src="img/image-20230311184416694.png" alt="image-20230311184416694" style="zoom:50%;" />



### SparkSQL

#### 简介

Spark的结构化数据处理模块 

提供分布式SQL查询引擎

提供处理结构化数据的编程接口DataFrame

支持的结构化的数据源：

<img src="img/image-20230311211017807.png" alt="image-20230311211017807" style="zoom:50%;" />

使用场景：

JDBC/ODBC 

<img src="img/image-20230311211100770.png" alt="image-20230311211100770" style="zoom:50%;" />

命令行

<img src="img/image-20230311211109313.png" alt="image-20230311211109313" style="zoom:50%;" />

DataFrame编程接口的使用

<img src="img/image-20230311211218311.png" alt="image-20230311211218311" style="zoom:50%;" />

特性：

- 与Spark程序无缝集成

  在Spark程序中使用DataFrame API或SQL处理结构化数据

- 兼容Hive

  兼容Hive的数据、查询、用户定义函数

- 统一的数据访问方法

  DataFrame API与SQL类似，提供了一种操作结构化数据的标准方法

- 多数据源支持

  HDFS、Hive、HBase、Parquet等

架构：

<img src="img/image-20230311213144229.png" alt="image-20230311213144229" style="zoom:50%;" />

原理

<img src="img/image-20230311213438213.png" alt="image-20230311213438213" style="zoom:50%;" />



## 命令

### hadoop

#### 用户命令

| **命令**               | **功能**           |
| ---------------------- | ------------------ |
| bin/Hadoop             | 显示帮助           |
| bin/hadoop fs          | 文件操作           |
| bin/hadoop jar         | 运行MapReduce程序  |
| bin/hadoop version     | 查看Hadoop版本     |
| bin/hadoop checknative | 检查Hadoop的本地库 |

fs

| 命令                             | 功能                               |
| -------------------------------- | ---------------------------------- |
| -cat path/file                   | 输出文本文件的内容                 |
| -appendToFile 本地文件 集群文件  | 将本地文件的内容追加到集群文件结尾 |
| -copyFromLocal 本地文件 集群文件 | 将本地文件复制到集群               |
| -copyToLocal集群文件  本地文件   | 将集群文件复制到本地               |
| -cp 集群原文件 集群目标文件      | 复制集群文件                       |
| -mv 集群原文件 集群目标文件      | 移动或重命名文件                   |
| -ls 路径                         | 列出集群文件或者目录               |
| -mkdir 路径                      | 在集群中创建目录                   |
| -setrep [参数] [副本数] [路径]   | 设置文件副本数                     |



| 命令                                      | 功能                                                         |
| ----------------------------------------- | ------------------------------------------------------------ |
| hadoop fs  -mkdir /park                   | 在hdfs 的根目录下，创建 park目录                             |
| hadoop   fs -ls /                         | 查看hdfs根目录下有哪些目录                                   |
| hadoop fs  -put /root/1.txt /park         | 将linux操作系统root目录下的1.txt放在hdfs的park目录下(文件夹也行) |
| hadoop fs -get   /park/jdk /home          | 把hdfs文件系统下park目录的文件下载到linux的home目录下        |
| hadoop fs -rm  /park/文件名               | 删除hdfs  的park目录的指定文件                               |
| hadoop fs  -rmdir /park                   | 删除park目录，但是前提目录里没有文件                         |
| hadoop fs  -rmr /park                     | 删除park目录，即使目录里有文件                               |
| hadoop fs  -cat /park/a.txt               | 查看park目录下的a.txt文件                                    |
| hadoop fs  -tail /park/a.txt              | 查看park目录下a.txt文件末尾的数据                            |
| haddop  jar xxx.jar                       | 执行jar包                                                    |
| hadoop fs  -cat /park/result/part-r-00000 | 查看  /park/result/part-r-00000文件的内容                    |
| hadoop   fs  –mv  /park02 /park01         | 将HDFS上的park02目录重名为park01命令。                       |
| hadoop fs -mv /park02/1.txt /park01       | 将park02目录下的1.txt移动到/park01目录下                     |
| hadoop fs  -touchz /park/2.txt            | 创建一个空文件                                               |
| hadoop fs  -getmerge /park /root/tmp      | 将park目录下的所有文件合并成一个文件，并下载到linux的root目录下的tmp目录 |



#### 管理命令

|                                                           |                                                              |
| --------------------------------------------------------- | ------------------------------------------------------------ |
| hadoop  dfsadmin -safemode leave                          | 离开hadoop安全模式                                           |
| hadoop  dfsadmin -safemode enter                          | 进入安全模式                                                 |
| hadoop  dfsadmin -rollEdits                               | 手动执行fsimage文件和Edis文件合并元数据                      |
| hadoop  dfsadmin -report                                  | 查看存活的datanode节点信息                                   |
| hadoop fsck  /park                                        | 汇报/park目录  健康状况                                      |
| hadoop fsck  /park/1.txt -files -blocks -locations -racks | 查看1.txt  这个文件block信息以及机架信息      ![IMG_256](file:///C:/Users/lr580/AppData/Local/Temp/msohtmlclip1/01/clip_image002.jpg) |
| hadoop fs  -expunge                                       | 手动清空hdfs回收站                                           |

#### 日志

调整日志级别：

```sh
bin/hadoop daemonlog -getlevel <host:httpport> <classname>
bin/hadoop daemonlog -setlevel <host:httpport> <classname> <level>
```

`DEBUG < INFO < WARN < ERROR < FATAL`

`http://<hostname>:50070/logLevel`

#### 启动关闭

| **命令**                                     | **功能**                             |
| -------------------------------------------- | ------------------------------------ |
| \$sbin/start-all.sh  $sbin/stop-all.sh       | 启动集群所有服务/关闭集群所有服务    |
| \$sbin/start-dfs.sh $sbin/stop-dfs.sh        | 启动dfs;关闭dfs                      |
| \$sbin/start-yarn.sh  $sbin/stop-yarn.sh     | 启动YARN;关闭YARN                    |
| $sbin/hadoop-daemon.sh  [start\|stop] 服务名 | 单个Hadoop服务启动或者关闭           |
| $sbin/hadoop-daemons.sh [start\|stop] 服务名 | 全部slaves上的Hadoop服务启动或者关闭 |
| $sbin/yarn-daemon.sh [start\|stop] 服务名    | 单个yarn服务的启动或者关闭           |
| $sbin/yarn-daemons.sh [start\|stop] 服务名   | 全部slaves上的yarn服务启动或者关闭   |
| $bin/hdfs secondarynamenode                  | 以控制台的方式启动SecondaryNameNode  |
| $bin/hdfs namenode                           | 以控制台的方式启动NameNode           |
| $bin/hdfs datanode                           | 以控制台的方式启动DataNode           |



#### 其他

|                                              |                                                              |
| -------------------------------------------- | ------------------------------------------------------------ |
| hadoop fs -cp   /park01/1.txt /park02        | 将HDFS上 /park01下的1.txt拷贝一份到 /park02目录下。   目标路径可以有多个，用空格隔开，比如：hadoop fs -cp /park01/1.txt /park02   /park03…… |
| hadoop fs -du  /park/1.txt                   | 查看HDFS上某个文件的大小。也可以查看指定目录，如果是目录的话，则列出目录下所有的文件及其大小，比如：hadoop fs -du /park |
| hadoop fs -copyFromLocal /home/1.txt /park01 | 将本地文件1.txt上传到/park01目录下                           |
| hadoop fs -lsr   /                           | 递归查看指定目录下的所有内容                                 |
| hadoop fs  -copyToLocal /park01/1.txt /home  | 将HDFS上的1.txt 拷贝到本地文件系统                           |

### hdfs

#### 用户命令

| **命令**                     | **功能**                                 |
| ---------------------------- | ---------------------------------------- |
| $bin/hdfs                    | 显示帮助                                 |
| $bin/hdfs dfs [参数]         | 文件操作，与$bin/hadoop fs的参数完全一样 |
| $bin/hdfs namenode  -format  | 格式化NameNode                           |
| $bin/hdfs getconf            | 从配置文件中获取配置信息                 |
| $bin/hdfs fsck [路径] [参数] | 处理损坏的文件                           |

fsck 参数：

| 参数                      | 功能                                             |
| ------------------------- | ------------------------------------------------ |
| -list-corruptfileblocks   | 输出损坏的文件及丢失的块                         |
| -move                     | 将文件移动到/lost+found目录                      |
| -delete                   | 删除损坏的文件                                   |
| -openforwrite             | 输出以写方式打开的文件                           |
| -files                    | 输出该目录及子目录下所有文件的状态               |
| -files -blocks            | 输出该目录及子目录下所有文件的块信息             |
| -files –blocks -locations | 输出该目录及子目录下所有文件在DataNode的存储信息 |
| -files -blocks -racks     | 输出该目录及子目录下所有文件机架感知信息         |



#### 管理命令

| **命令**                                                     | **功能**                              |
| ------------------------------------------------------------ | ------------------------------------- |
| $bin/hdfs dfsadmin -report                                   | 查看HDFS的基本统计信息                |
| $bin/hdfs dfsadmin -safemode <enter \| leave \| get \| wait> | 配置安全模式                          |
| $bin/hdfs dfsadmin -saveNamespace                            | 将内存信息保存到磁盘，并重置edits文件 |
| $bin/hdfs dfsadmin - refreshNodes                            | 刷新节点和排除文件                    |
| $bin/hdfs dfsadmin – setBalancerBandwidth [byte per second]  | 设置负载均衡带宽                      |
| $bin/hdfs secondarynamenode [参数]                           | 操作SecondaryNameNode                 |
| $bin/hdfs balancer                                           | 平衡集群中DataNode的数据              |



### hbase

#### shell命令

进入：`hbase shell`。退出 `exit`。

查看帮助 `help '命令名'`，如 `help 'create'`。

列出所有表 `list`

> 列出所有命名空间 `list_namespace`

查看表的字段：`describe '表名'`

查看表是否存在 `exists '表名'`

查询表的信息 `scan '表名'`

查询数据 `get '表名', '行名', '列族:列名'`

创建表 `create`

- 创建一个表，只有一个列族：`create 'test', 'cf'`

添加数据 `put`。在添加数据时，HBase会自动为添加的数据添加一个时间戳，当然，也可以在添加数据时人工指定时间戳的值。

- `put 表名, 行id, 列族:列名, 值`。如 `put 't1', 'r1', 'f1:a', 'baicha'`

  新的 put 会覆盖掉旧的，所以 put 可以增改

- 为了存多个时间戳版本，先设置只保留最新的两个版本数据：

  `alter 't1', NAME=>'f1', VERSIONS=>2`

  再增加带时间戳的数据 `put 't1', 'r1', 'f1:a', 'guodong', 1677840899646`

  查询 `get 't1','r1', COLUMN=>'f1:a',VERSIONS=>2`

- 删除，put 改 delete

使表有效或无效 `enable/disable '表名'`

删除表 `drop '表名'`。删除前必须 disable。



### hive

#### 部署

hive不需要所有节点都安装

加载指令：

Load data local inpath '/home/stu.txt'  into table student;

Navicat连接到mysql：

对应数据库名称：HIveDB； hive-site.xml中设置

表DBS: 存放hive中有哪些数据库

#### 基础

| **命令**                                                     | **作用**                                     | **额外说明**                                                 |
| ------------------------------------------------------------ | -------------------------------------------- | ------------------------------------------------------------ |
| show databases;                                              | 查看都有哪些数据库                           |                                                              |
| create  database park;                                       | 创建park数据库                               | 创建的数据库，实际是在Hadoop的HDFS文件系统里创建一个目录节点，统一存在： /user/hive/warehouse 目录下 |
| use park;                                                    | 进入park数据库                               |                                                              |
| show  tables;                                                | 查看当前数据库下所有表                       |                                                              |
| create  table stu   (id int,name string);                    | 创建stu表，以及相关的两个字段                | 1. hive里，表示字符串用的是string，不用char和varchar  2. 所创建的表，也是HDFS里的一个目录节点 |
| insert into  stu values(1,'zhang')                           | 向stu表插入数据                              | 1.      HDFS不支持数据的修改和删除，因此已经插入的数据不能够再进行任何的改动  2.      在Hadoop2.0版本后支持了数据追加。实际上，insert into 语句执行的是追加操作  3.      hive支持查询，行级别的插入。不支持行级别的删除和修改  4.      hive的操作实际是执行一个job任务，调用的是Hadoop的MR  5.      插入完数据之后，发现HDFS stu目录节点下多了一个文件，文件里存了插入的数据，因此，hive存储的数据，是通过HDFS的文件来存储的。 |
| select *  from stu                                           | 查看表数据                                   | 也可以根据字段来查询，比如select id from stu                 |
| drop table  stu                                              | 删除表                                       |                                                              |
| select * from stu                                            | 查询stu表数据                                |                                                              |
| load data  local inpath '/home//stu.txt' into table stu;     | 通过加载文件数据到指定的表里                 | 1.      在执行完这个指令之后，发现hdfs stu目录下多了一个txt文件。由此可见，hive的工作原理实际上就是在管理hdfs上的文件，把文件里数据抽象成二维表结构，然后提供hql语句供程序员查询文件数据  2.      可以做这样的实验：不通过load 指令，而通过插件向stu目录下再上传一个文件，看下hive是否能将数据管理到stu表里。 |
| create  table stu1(id int,name string) row format delimited fields terminated by  ' '; | 创建stu1表，并指定分割符 空格。              |                                                              |
| desc stu                                                     | 查看 stu表结构                               |                                                              |
| create  table stu2 like stu                                  | 创建一张stu2表，表结构和stu表结构相同        | like只复制表结构，不复制数据                                 |
| insert overwrite   table stu2 select * from stu              | 把stu表数据插入到stu2表中                    |                                                              |
| insert  overwrite local directory '/home/stu' row format delimited fields terminated  by '   ' select * from stu; | 将stu表中查询的数据写到本地的/home/stu目录下 |                                                              |
| insert  overwrite directory '/stu' row format delimited fields terminated by '   ' select * from stu; | 将stu表中查询的数据写到HDFS的stu目录下       |                                                              |
| from stu  insert overwrite table stu1 select * insert overwrite table stu2 select *; | 将stu表中查询的数据写到stu1以及stu2两张表中  |                                                              |
| alter  table stu rename to stu2                              | 为表stu重命名为stu2                          |                                                              |
| alter table  stu add columns (age int);                      | 为表stu增加一个列字段age，类型为int          |                                                              |
| exit  或 quit                                                | 退出hive                                     |                                                              |

#### 表结构

##### 内外部表

\1)    内部表：先在hive里建一张表，然后向这个表插入数据（用insert可以插入数据，也可以通过加载外部文件方式来插入数据），这样的表称之为hive的内部表

\2)    外部表：HDFS里已经有数据了，然后，通过hive创建一张表来管理这个文件数据。则这样表称之为外部表。需要注意的是，hive外部表管理的是HDFS里的某一个目录下的文件数据

外部表创建

```hive
create external table stu (id int,name string) row format delimited fields terminated by  '  '  location   '/目录路径'
```

\1)    对于内部表，在删除该表的时候，HDFS对应的目录节点会被删除

\2)    对于外部表，在删除该表的时候，HDFS对应的目录节点不会删除

##### 分区表

\1)    分区表可以通过添加指定的字段来提高Hive的查询效率

\2)    在数据量较大的情况下，往往会添加分区表来避免**全表**查询

| 指令                                                         | 作用                                         | 额外说明                                                     |
| ------------------------------------------------------------ | -------------------------------------------- | ------------------------------------------------------------ |
| create table book (id int, name string) **partitioned** by (category  string)   row format delimited fields terminated by '\t'; | 创建book表，以category作为分区               | 在创建分区表时，partitioned字段可以不在字段列表中。生成的表中自动就会具有该字段。 |
| load data local inpath '/home/cn.txt' overwrite into table book  partition (category='cn'); | 将本地文件cn.txt添加到book表中，分区字段为cn | 在HDFS下生成category=cn目录                                  |
| select * from book where category='cn';                      | 查看分区为cn的数据                           |                                                              |
| ALTER TABLE book add PARTITION  (category = 'jp') location '/user/hive/warehouse/park.db/book/category=jp'; | 将指定的目录添加为分区字段                   |                                                              |
| show partitions iteblog;                                     | 查看分区                                     |                                                              |
| msck repair table book;                                      | 修复分区                                     |                                                              |
| alter table book drop partition(category='cn');              | 删除分区                                     |                                                              |
| alter table book partition(category='french') rename to partition  (category='hh'); | 修改分区的名字                               |                                                              |

##### 分桶表

\1)    分桶表是一种更细粒度的数据分配方式

\2)    一个表既可以分区也可以分桶

\3)    分桶的主要作用是实现数据的抽样，方便进行数据测试

\4)    分桶表通过hash分桶算法，将数据分放在不同的桶(hdfs中的文件)中，方便后续获取

\5)    分桶表机制默认是不开启的，需要手动开启：set hive.enforce.bucketing=true;

\6)    分桶表不允许以外部文件方式导入数据，只能从另外一张表数据导入

| 指令                                                         | 作用                                         | 额外说明                                                     |
| ------------------------------------------------------------ | -------------------------------------------- | ------------------------------------------------------------ |
| create table teacher(name string) clustered by (name) into 3 buckets **row format delimited  fields terminated by ' '**; | 创建teacher表，以name作为分桶机制，分为3个桶 |                                                              |
| insert overwrite table teacher select * from tmp;            | 将tmp表中的数据添加到teacher表中             | 实际上是产生了3个文件用于存储不分桶的数据                    |
| select * from teacher tablesample(bucket 1 out of 3 on name); | 进行抽样                                     | 抽样格式为：bucket x out of y on XXX  1. x表示抽样的起始桶，例如bucket 1 out of 3表示从第1 个桶开始抽取数据  2. y决定抽样的比例，要求必须是桶数的因子或者整数倍  3. 如果桶数为6，y为2，则表示抽取6/2=3个桶中的数据  4. 如果桶数为6，y为3，则表示抽取6/3=2个桶中的数据  5. 如果桶数为6，y为12，则表示抽取6/12=0.5个桶中的数据  6. 如果桶数为6，抽样为bucket 1 out of 3 on  id表示从第1个桶开始抽样，抽取2个桶的数据，所以抽取的样本为1和4桶中的数据 |

<img src="img/image-20230308151906975.png" alt="image-20230308151906975" style="zoom:50%;" />

#### 数据类型

| Hive中的类型 | Java中的类型 |
| ------------ | ------------ |
| tinyint      | byte         |
| smallint     | short        |
| int          | int          |
| bigint       | long         |
| boolean      | boolean      |
| float        | float        |
| double       | double       |
| string       | String       |
| timestamp    | TimeStamp    |
| binary       | byte[]       |

##### array

```
100,200,300 
200,300,500
```

```hive
create external table ex(vals array<int>) row format delimited fields terminated by '\t' collection items terminated by ',' location '/ex';
```

查询每行数组的个数，查询语句：

```hive
select size(vals) from ex;
```

注：hive 内置函数不具备查询某个具体行的数组元素。需要自定义函数来实现，但这样的需求在实际开发里很少，所以不需要在意。



```
100,200,300      tom,jary
200,300,500      rose,jack
```

```hive
create external table ex1(info1 array<int>,info2 array<string>) row format delimited fields terminated by '\t' collection items terminated by ',' location '/ex';
```



##### map

```
tom,23
rose,25
jary,28
```

```hive
create external table m1 (vals map<string,int>) row format delimited fields terminated by '\t' map keys terminated by ',' location '/map';
```

```hive
select vals['tom'] from m1;
```



```
tom 192.168.234.21
rose 192.168.234.21
tom 192.168.234.22
jary 192.168.234.21
tom 192.168.234.24
tom 192.168.234.21
rose 192.168.234.21
tom 192.168.234.22
jary 192.168.234.21
tom 192.168.234.22
tom 192.168.234.23
```

map类型，列的分割符必须是\t

```hive
create external table ex (vals map<string,string>) row format delimited fields terminated by '\t' map keys terminated by ' ' location '/ex';
```

```hive
select vals['tom'] from ex where vals['tom'] is not null;
```

去重：

```hive
select distinct(ip) from (select vals['tom'] ip from ex where vals['tom'] is not null)ex1;
```

```hive
select distinct(vals['tom']) from m2 where vals['tom'] is not null;
```



##### struct

```
tom 23
rose 22
jary 26
```

```hive
create external table ex (vals struct<name:string,age:int>)row format delimited collection items terminated by ' '  location '/ex';
```

```hive
select vals.age from ex where vals.name='tom';
```



#### 运算符

##### 关系

| 运算符        | 类型         | 说明                                                         |
| ------------- | ------------ | ------------------------------------------------------------ |
| A = B         | 所有原始类型 | 如果A与B相等，返回true，否则返回false                        |
| A == B        | 无           | 失败，因为无效的语法。 SQL使用”=”，不使用”==”。              |
| A <> B        | 所有原始类型 | 如果A不等于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。 |
| A < B         | 所有原始类型 | 如果A小于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。 |
| A <= B        | 所有原始类型 | 如果A小于等于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。 |
| A > B         | 所有原始类型 | 如果A大于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。 |
| A >= B        | 所有原始类型 | 如果A大于等于B返回TRUE,否则返回FALSE。如果A或B值为”NULL”，结果返回”NULL”。 |
| A IS NULL     | 所有类型     | 如果A值为”NULL”，返回TRUE,否则返回FALSE                      |
| A IS NOT NULL | 所有类型     | 如果A值不为”NULL”，返回TRUE,否则返回FALSE                    |
| A LIKE B      | 字符串       | 如果A或B值为”NULL”，结果返回”NULL”。字符串A与B通过sql进行匹配，如果相符返回TRUE，不符返回FALSE。B字符串中  的”_”代表任一字符，”%”则代表多个任意字符。例如： (‘foobar’ like ‘foo’)返回FALSE，（  ‘foobar’ like ‘foo_ _ _’或者 ‘foobar’ like ‘foo%’)则返回TURE |
| A RLIKE B     | 字符串       | 如果A或B值为”NULL”，结果返回”NULL”。字符串A与B通过java进行匹配，如果相符返回TRUE，不符返回FALSE。例如：（ ‘foobar’ rlike ‘foo’）返回FALSE，（’foobar’  rlike ‘^f.*r$’ ）返回TRUE。 |
| A REGEXP B    | 字符串       | 与RLIKE相同。                                                |

##### 算术

| 运算符 | 类型         | 说明                                                         |
| ------ | ------------ | ------------------------------------------------------------ |
| A + B  | 所有数字类型 | A和B相加。结果的与操作数值有共同类型。例如每一个整数是一个浮点数，浮点数包含整数。所以，一个浮点数和一个整数相加结果也是一个浮点数。 |
| A – B  | 所有数字类型 | A和B相减。结果的与操作数值有共同类型。                       |
| A * B  | 所有数字类型 | A和B相乘，结果的与操作数值有共同类型。需要说明的是，如果乘法造成溢出，将选择更高的类型。 |
| A / B  | 所有数字类型 | A和B相除，结果是一个double（双精度）类型的结果。             |
| A % B  | 所有数字类型 | A除以B余数与操作数值有共同类型。                             |
| A & B  | 所有数字类型 | 运算符查看两个参数的二进制表示法的值，并执行按位”与”操作。两个表达式的一位均为1时，则结果的该位为 1。否则，结果的该位为 0。 |
| A\|B   | 所有数字类型 | 运算符查看两个参数的二进制表示法的值，并执行按位”或”操作。只要任一表达式的一位为 1，则结果的该位为 1。否则，结果的该位为 0。 |
| A ^ B  | 所有数字类型 | 运算符查看两个参数的二进制表示法的值，并执行按位”异或”操作。当且仅当只有一个表达式的某位上为 1 时，结果的该位才为 1。否则结果的该位为 0。 |
| ~A     | 所有数字类型 | 对一个表达式执行按位”非”（取反）。                           |

##### 逻辑

| 运算符  | 类型   | 说明                                                         |
| ------- | ------ | ------------------------------------------------------------ |
| A AND B | 布尔值 | A和B同时正确时,返回TRUE,否则FALSE。如果A或B值为NULL，返回NULL。 |
| A && B  | 布尔值 | 与”A AND B”相同                                              |
| A OR B  | 布尔值 | A或B正确,或两者同时正确返返回TRUE,否则FALSE。如果A和B值同时为NULL，返回NULL。 |
| A \| B  | 布尔值 | 与”A OR B”相同                                               |
| NOT A   | 布尔值 | 如果A为NULL或错误的时候返回TURE，否则返回FALSE。             |
| ! A     | 布尔值 | 与”NOT A”相同                                                |



#### 函数

\1.   Hive实现了标准的sql，但在这之外，为了提升hive处理数据的能力，还额外提供了很多内置的函数，这些内置函数非常丰富，且可以直接使用，虽然不属于sql原生的语法，但大大的增强了hive处理数据的能力，是hive功能的重要组成部分

\2.   Hive除了有内置函数，还允许用户自定义函数

##### 数学

| 返回类型   | 函数                                               | 说明                                                         |
| ---------- | -------------------------------------------------- | ------------------------------------------------------------ |
| BIGINT     | round(double a)                                    | 四舍五入                                                     |
| DOUBLE     | round(double a, int  d)                            | 小数部分d位之后数字四舍五入，例如round(21.263,2),返回21.26   |
| BIGINT     | floor(double a)                                    | 对给定数据进行向下舍入最接近的整数。例如floor(21.2),返回21。 |
| BIGINT     | ceil(double a),  ceiling(double a)                 | 将参数向上舍入为最接近的整数。例如ceil(21.2),返回23.         |
| double     | rand(), rand(int  seed)                            | 返回大于或等于0且小于1的平均分布随机数（依重新计算而变）     |
| double     | exp(double a)                                      | 返回e的n次方                                                 |
| double     | ln(double a)                                       | 返回给定数值的自然对数                                       |
| double     | log10(double a)                                    | 返回给定数值的以10为底自然对数                               |
| double     | log2(double a)                                     | 返回给定数值的以2为底自然对数                                |
| double     | log(double base,  double a)                        | 返回给定底数及指数返回自然对数                               |
| double     | pow(double a, double  p) power(double a, double p) | 返回某数的乘幂                                               |
| double     | sqrt(double a)                                     | 返回数值的平方根                                             |
| string     | bin(BIGINT a)                                      | 返回二进制格式                                               |
| string     | hex(BIGINT a)  hex(string a)                       | 将整数或字符转换为十六进制格式                               |
| string     | unhex(string a)                                    | 十六进制字符转换由数字表示的字符。                           |
| string     | conv(BIGINT num, int  from_base, int to_base)      | 将指定数值，由原来的度量体系转换为指定的试题体系。例如CONV(‘a’,16,2),返回 |
| double     | abs(double a)                                      | 取绝对值                                                     |
| int double | pmod(int a, int b)  pmod(double a, double b)       | 返回a除b的余数的绝对值                                       |
| double     | sin(double a)                                      | 返回给定角度的正弦值                                         |
| double     | asin(double a)                                     | 返回x的反正弦，即是X。如果X是在-1到1的正弦值，返回NULL。     |
| double     | cos(double a)                                      | 返回余弦                                                     |
| double     | acos(double a)                                     | 返回X的反余弦，即余弦是X，，如果-1<= A <= 1，否则返回null.   |
| int double | positive(int a)  positive(double a)                | 返回A的值，例如positive(2)，返回2。                          |
| int double | negative(int a)  negative(double a)                | 返回A的相反数，例如negative(2),返回-2。                      |

##### 类型转换

| 返回类型    | 函数                  | 说明                                                         |
| ----------- | --------------------- | ------------------------------------------------------------ |
| 指定 “type” | cast(expr as  <type>) | 类型转换。例如将字符”1″转换为整数:cast(’1′ as  bigint)，如果转换失败返回NULL。 |

##### 日期

| 返回类型 | 函数                                             | 说明                                                         |
| -------- | ------------------------------------------------ | ------------------------------------------------------------ |
| string   | from_unixtime(bigint  unixtime[, string format]) | UNIX_TIMESTAMP参数表示返回一个值’YYYY- MM – DD HH：MM：SS’或YYYYMMDDHHMMSS.uuuuuu格式，这取决于是否是在一个字符串或数字语境中使用的功能。该值表示在当前的时区。 |
| bigint   | unix_timestamp()                                 | 如果不带参数的调用，返回一个Unix时间戳（从’1970- 01 – 0100:00:00′到现在的UTC秒数）为无符号整数。 |
| bigint   | unix_timestamp(string  date)                     | 指定日期参数调用UNIX_TIMESTAMP（），它返回参数值’1970- 01 – 0100:00:00′到指定日期的秒数。 |
| bigint   | unix_timestamp(string  date, string pattern)     | 指定时间输入格式，返回到1970年秒数：unix_timestamp(’2009-03-20′, ‘yyyy-MM-dd’) = 1237532400 |
| string   | to_date(string  timestamp)                       | 返回时间中的年月日：  to_date(“1970-01-01 00:00:00″)  = “1970-01-01″ |
| string   | to_dates(string  date)                           | 给定一个日期date，返回一个天数（0年以来的天数）              |
| int      | year(string date)                                | 返回指定时间的年份，范围在1000到9999，或为”零”日期的0。      |
| int      | month(string date)                               | 返回指定时间的月份，范围为1至12月，或0一个月的一部分，如’0000-00-00′或’2008-00-00′的日期。 |
| int      | day(string date)  dayofmonth(date)               | 返回指定时间的日期                                           |
| int      | hour(string date)                                | 返回指定时间的小时，范围为0到23。                            |
| int      | minute(string date)                              | 返回指定时间的分钟，范围为0到59。                            |
| int      | second(string date)                              | 返回指定时间的秒，范围为0到59。                              |
| int      | weekofyear(string  date)                         | 返回指定日期所在一年中的星期号，范围为0到53。                |
| int      | datediff(string  enddate, string startdate)      | 两个时间参数的日期之差。                                     |
| int      | date_add(string  startdate, int days)            | 给定时间，在此基础上加上指定的时间段。                       |
| int      | date_sub(string  startdate, int days)            | 给定时间，在此基础上减去指定的时间段。                       |

##### 条件

| 返回类型 | 函数                                                        | 说明                                                         |
| -------- | ----------------------------------------------------------- | ------------------------------------------------------------ |
| T        | if(boolean  testCondition, T valueTrue, T valueFalseOrNull) | 判断是否满足条件，如果满足返回一个值，如果不满足则返回另一个值。 |
| T        | COALESCE(T v1, T v2,  …)                                    | 返回一组数据中，第一个不为NULL的值，如果均为NULL,返回NULL。  |
| T        | CASE a WHEN b THEN c  [WHEN d THEN e]* [ELSE f] END         | 当a=b时,返回c；当a=d时，返回e，否则返回f。                   |
| T        | CASE WHEN a THEN b  [WHEN c THEN d]* [ELSE e] END           | 当值为a时返回b,当值为c时返回d。否则返回e。                   |

##### 字符串

| 返回类型                     | 函数                                                         | 说明                                                         |
| ---------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| int                          | length(string A)                                             | 返回字符串的长度                                             |
| string                       | reverse(string A)                                            | 返回倒序字符串                                               |
| string                       | concat(string A,  string B…)                                 | 连接多个字符串，合并为一个字符串，可以接受任意数量的输入字符串 |
| string                       | concat_ws(string  SEP, string A, string B…)                  | 链接多个字符串，字符串之间以指定的分隔符分开。               |
| string                       | substr(string A, int  start) substring(string A, int start)  | 从文本字符串中指定的起始位置后的字符。                       |
| string                       | substr(string A, int  start, int len) substring(string A, int start, int len) | 从文本字符串中指定的位置指定长度的字符。                     |
| string                       | upper(string A)  ucase(string A)                             | 将文本字符串转换成字母全部大写形式                           |
| string                       | lower(string A)  lcase(string A)                             | 将文本字符串转换成字母全部小写形式                           |
| string                       | trim(string A)                                               | 删除字符串两端的空格，字符之间的空格保留                     |
| string                       | ltrim(string A)                                              | 删除字符串左边的空格，其他的空格保留                         |
| string                       | rtrim(string A)                                              | 删除字符串右边的空格，其他的空格保留                         |
| string                       | regexp_replace(string  A, string B, string C)                | 字符串A中的B字符被C字符替代                                  |
| string                       | regexp_extract(string  subject, string pattern, int index)   | 通过下标返回正则表达式指定的部分。regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’,  2) returns ‘bar.’ |
| string                       | parse_url(string  urlString, string partToExtract [, string keyToExtract]) | 返回URL指定的部分。parse_url(‘http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1′, ‘HOST’) 返回：’facebook.com’ |
| string                       | get_json_object(string  json_string, string path)            | select a.timestamp,  get_json_object(a.appevents, ‘$.eventid’), get_json_object(a.appenvets, ‘$.eventname’) from log a; |
| string                       | space(int n)                                                 | 返回指定数量的空格                                           |
| string                       | repeat(string str,  int n)                                   | 重复N次字符串                                                |
| int                          | ascii(string str)                                            | 返回字符串中首字符的数字值                                   |
| string                       | lpad(string str, int  len, string pad)                       | 返回指定长度的字符串，给定字符串长度小于指定长度时，由指定字符从左侧填补。 |
| string                       | rpad(string str, int  len, string pad)                       | 返回指定长度的字符串，给定字符串长度小于指定长度时，由指定字符从右侧填补。 |
| array                        | split(string str,  string pat)                               | 将字符串转换为数组。                                         |
| int                          | find_in_set(string  str, string strList)                     | 返回字符串str第一次在strlist出现的位置。如果任一参数为NULL,返回NULL；如果第一个参数包含逗号，返回0。 |
| array<array<string>>         | sentences(string  str, string lang, string locale)           | 将字符串中内容按语句分组，每个单词间以逗号分隔，最后返回数组。 例如sentences(‘Hello there! How are you?’) 返回：( (“Hello”, “there”), (“How”, “are”, “you”) ) |
| array<struct<string,double>> | ngrams(array<array<string>>,  int N, int K, int pf)          | SELECT  ngrams(sentences(lower(tweet)), 2, 100 [, 1000]) FROM twitter; |
| array<struct<string,double>> | context_ngrams(array<array<string>>,  array<string>, int K, int pf) | SELECT context_ngrams(sentences(lower(tweet)),  array(null,null), 100, [, 1000]) FROM twitter; |

##### 聚合

| 返回类型                 | 函数                                                         | 说明                                                         |
| ------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| bigint                   | count(*) ,  count(expr), count(DISTINCT expr[, expr_., expr_.]) | 返回记录条数。                                               |
| double                   | sum(col),  sum(DISTINCT col)                                 | 求和                                                         |
| double                   | avg(col), avg(DISTINCT  col)                                 | 求平均值                                                     |
| double                   | min(col)                                                     | 返回指定列中最小值                                           |
| double                   | max(col)                                                     | 返回指定列中最大值                                           |
| double                   | var_pop(col)                                                 | 返回指定列的方差                                             |
| double                   | var_samp(col)                                                | 返回指定列的样本方差                                         |
| double                   | stddev_pop(col)                                              | 返回指定列的偏差                                             |
| double                   | stddev_samp(col)                                             | 返回指定列的样本偏差                                         |
| double                   | covar_pop(col1,  col2)                                       | 两列数值协方差                                               |
| double                   | covar_samp(col1,  col2)                                      | 两列数值样本协方差                                           |
| double                   | corr(col1, col2)                                             | 返回两列数值的相关系数                                       |
| double                   | percentile(col, p)                                           | 返回数值区域的百分比数值点。0<=P<=1,否则返回NULL,不支持浮点型数值。 |
| array<double>            | percentile(col,  array(p~1,,\ [, p,,2,,]…))                  | 返回数值区域的一组百分比值分别对应的数值点。0<=P<=1,否则返回NULL,不支持浮点型数值。 |
| double                   | percentile_approx(col,  p[, B])                              | Returns an  approximate p^th^ percentile of a numeric column (including floating point  types) in the group. The B parameter controls approximation accuracy at the  cost of memory. Higher values yield better approximations, and the default is  10,000. When the number of distinct values in col is smaller than B, this  gives an exact percentile value. |
| array<double>            | percentile_approx(col,  array(p~1,, [, p,,2_]…) [, B])       | Same as above, but  accepts and returns an array of percentile values instead of a single one. |
| array<struct\{‘x’,'y’\}> | histogram_numeric(col,  b)                                   | Computes a histogram  of a numeric column in the group using b non-uniformly spaced bins. The  output is an array of size b of double-valued (x,y) coordinates that  represent the bin centers and heights |
| array                    | collect_set(col)                                             | 返回无重复记录                                               |

##### explode

explode函数可以将一个array或者map展开，其中explode(array)使得结果中将array列表里的每个元素生成一行；explode(map)使得结果中将map里的每一对元素作为一行，key为一列，value为一列.

1．explode 命令可以将行数据，按指定规则切分出多行

2．用explode做行切分，注意表里只有一列，并且行数据是string类型，因为只有字符类型才能做切分

```
100,200,300
200,300,500
```

要将上面两行数据根据逗号拆分成多行（每个数字占一行）

上传HDFS，并创建对应的外部表，执行：

```hive
create external table ex1 (num string) location '/ex';
```

切分：

```hive
select explode(split(num,',')) from ex1;
```



##### UDF

自定义函数

\1.    如果hive的内置函数不够用，用户也可以自己定义函数来使用，这样的函数称为hive的用户自定义函数，简称UDF

\2.    UDF使得Hive的可扩展性增强



\1.    新建java工程，导入hive相关包，导入hive相关的lib

\2.    创建类继承UDF

\3.    自己编写一个evaluate方法，返回值和参数任意。

```java
import org.apache.hadoop.hive.ql.exec.UDF;
public class ToUpper extends UDF{
    public String evaluate(String str){
        return str.toUpperCase();
    }
}
```

\4.    为了能让mapreduce处理，String要用Text处理。

\5.    将写好的类打成jar包，上传到linux中

\6.    在hive命令行下，向hive注册UDF：add jar /xxxx/xxxx.jar

\7.    在hive命令行下，为当前udf起一个名字：create temporary function fname as '类的全路径名';

\8.    之后就可以在hql中使用该自定义函数了。`select fname('asdasdsa');`



### sparkSQL

| **常用数据操作** | **SQL**                                                |
| ---------------- | ------------------------------------------------------ |
| 创建数据库       | create  database mytestdb                              |
| 查看数据库       | show  databases                                        |
| 指定当前数据库   | use  mytestdb                                          |
| 创建表           | create  table test_tbl(id int, name string, value int) |
| 查看表定义       | desc test_tbl                                          |
| 列出所有表       | show  tables                                           |
| 插入数据         | insert  into table test_tbl values(0,"blue",10)        |
| 查询数据         | select  * from test_tbl  where  value>15               |
| 删除表           | drop  table  test_tbl                                  |
| 删除数据库       | drop  database  mytestdb                               |

## 数据库集群

### 理论

#### 基本

单机的数据库在一个高并发需求的环境,一定会受限于它的物理瓶颈(oracle 单表数据量亿级别,mysql单表数据量千万级别)

<img src="img/image-20230503215350129.png" alt="image-20230503215350129" style="zoom:80%;" />

数据库集群中的高可用也要基于数据的主从备份

以 mysql 集群为例。mysql也是支持多级主从.一主多从的主从结构搭建的，原理：

![image-20230503215420575](img/image-20230503215420575.png)

在一个主从结构中

主节点: 必须开启一个二进制的日志文件（log-bin）,以二进制格式记录所有的写操作命令,并且,会根据数据的写入,增加一个字段position的值,使得从节点通过判断position的值完成更新的判断和更新数据的抓取.例如主节点不断的写数据，position（指针）从100变化到1000，从节点不断监听（上次监听到的旧数据在100），发现主节点position已经从100变化到1000，从节点就知道自己要更新的范围是100-1000，就将要更新的数据抓取过去。

从节点:开启io线程,sql线程,本地的中继日志
Io线程:负责监听抓取主节点的更新内容(通过判断position的值是否变化);
relay.log:io线程将更新的内容存储在本地的一个日志文件（本地中继日志）
sql线程:负责监听本地中继日志,.判断更新(中继日志的position值判断),启动内部一个客户端执行所有更新的命令。

#### 双机热备

主节点宕机,使用从节点顶替主节点完成之前主节点的所有功能---故障转移

读写分离:将主从结构中的主,从节点充分利用起来,主节点即可以读也可以写,从节点可以分担一部分主节点读的压力,这种实现的逻辑--读写分离

<img src="img/image-20230505133455875.png" alt="image-20230505133455875" style="zoom:67%;" />

单机热备，很少使用，就是单方向的主从关系实现的故障转移

<img src="img/image-20230505133543668.png" alt="image-20230505133543668" style="zoom:67%;" />

双机热备(数据库集群单个分片的最小高可用单位)。双机热备的结构,就是以双向主从复制实现的故障转移

<img src="img/image-20230505133916274.png" alt="image-20230505133916274" style="zoom:67%;" />

#### mycat

##### 基本

数据库代理技术 

支持高性能的读写分离(计算实现)，最大的千亿别的大表水平分片并行计算，支持数据整合输入和整合输出

<img src="img/image-20230505135247578.png" alt="image-20230505135247578" style="zoom:67%;" />

实现的是对客户端发起的sql访问做拦截.计算之后连接后端数据库执行客户端具体的sql,根据连接的不同的数据库类型，在内部拦截后重新组织sql语句，所以支持不同后端数据库类型的方言

<img src="img/image-20230505135308198.png" alt="image-20230505135308198" style="zoom:67%;" />

(dn: database node (gpt))

mycat,包括各种数据库集群中间件的执行逻辑:

1 拦截计算

2 找到对应的分片位置

3 计算读写分离的逻辑

4 获取一个要执行sql语句的后端数据库连接

5 获取数据

6 返回客户端

7 还回连接资源。 



同类产品：

4.1同类产品

amoeba:变形虫

cobar:mycat前身

mycat:11年左右出现的最新的中间件

4.2amoeba

早期常用的数据库中间件,支持读写分离,支持跨库查询数据,一旦垮库性能受到极大影响.

现状：停更了,团队差不多解散了.

4.3cobar

cobar非常受欢迎的数据库中间件,在mycat之前就是一个常用的中间件技术,开发团队有很多是来自于amoeba.

cobar早期版本(官方发表解释已经解决这个问题)存在致命缺陷 后端数据库假死;

<img src="img/image-20230505135924196.png" alt="image-20230505135924196" style="zoom:67%;" />

后端数据库假死(无法响应请求，像崩溃了一样)

在高并发环境中,由于后端开发的技术基于阻塞线程的逻辑,极易出现由于线程都被占用,没有空闲线程处理请求sql逻辑,导致中间件判断本该访问的某个数据库后端节点宕机(实际没有宕机的)---假死



##### 逻辑表

###### 基本

核心概念逻辑表:逻辑库数据落地的具体体现方式;

逻辑库：不是真正的数据库,但是对于用户来讲,使用逻辑库的过程和真实库几乎没区别,而且逻辑库的数据来源对于客户端透明的(屏蔽,客户端也不关心)

![image-20230505142952287](img/image-20230505142952287.png)

###### 分片

分类:非分片表,分片表格,ER分片表,全局表

​	非分片表:mycat维护的逻辑表对应真实数据量不大 几万几十万,可以使用非分片表来管理后端数据

![image-20230505145517542](img/image-20230505145517542.png)

​	分片表:真实数据量整体庞大,无法在单独一个库,一个真实表中维护管理,将可以利用分片计算切分,对于用户客户端来讲,连接mycat看到一张-分片表

![image-20230505145529381](img/image-20230505145529381.png)

​	ER分片表:mycat处理垮库操作的首创的逻辑

​	全局表

###### 全局表

当某个表格数据不大,变化稳定时,本来应该在mycat配置成非分片的逻辑表,但是这种表格长期和业务表格(商品.用户)大量数据的分片表做关联查询使用,需要在mycat中将其配置为全局表.企业中常见的 工具字典表都是配置成全局表

工具字典表:和业务概念有关(如：分波器==15888,载波器=15889,拉普拉斯=88,傅里叶=89,麦克斯韦=90,http状态 200=成功,201,202,203,404=not found)

当一个逻辑表对应多个分片时,又不设置rule的分片计算规则,默认就是全局表

###### ER分片表

mycat没有直接解决垮库操作,影响效率;

ER分片表:分片表格多表关系的处理方式,mycat首创的.

定义的父级表格(1)t_order,定义子级表格(t_order_item),可以利用配置文件table包含childTable表示这种关系,所有表格的数据切分由父级表格统一维护(一种切分逻辑)

<img src="img/image-20230506165138949.png" alt="image-20230506165138949" style="zoom:67%;" />



##### 基本配置

`conf/` 下备份配置文件

```sh
cp server.xml server.xml.bck && cp schema.xml schema.xml.bck
```

###### server.xml

`server.xml` 的关键标签：

- system标签:定义的当前mycat占用的各种资源配置**,**端口,线程上限下限,缓存容量.

  - properties sqlParser=druidParser;使用阿里的的路由sql解析器;自定义代码实现其他

    ```xml
    <property name="defaultSqlParser">druidparser</property>
    ```

- user标签:定义连接登录mycat的所有用户的权限,信息等,可有多个

  属性name,可以登录mycat的用户名(user自己的属性，其他是property子标签)

  属性password,可以登录mycat的当前用户名的密码校验

  属性readOnly,true表示用户只读,false可读可写(默认)

  属性schemas,当前用户登录后可以在mycat看到的逻辑库.如果配置多个逻辑库,以","隔开,并且确保scehma.xml有同名的配置

  ```xml
  <user name="user">
      <property name="password">user</property>
      <property name="schemas">TESTDB</property>
      <property name="readOnly">true</property>
  </user>
  ```

- quarantine标签:安全标签,定义了可连接客户端白名单.sql黑名单

  - 标签whitehost:定义可以访问当前mycat的用户白名单ip地址,一旦定义,只有白名单ip的客户端服务器可以登录mycat

  - sql语句黑名单,可以实现登录的客户端们能够发送的sql语句可以在blacklist中做限制,必须开启

    下例禁止 select

    ```xml
    <blacklist check="false">
        <property name="selelctAllow">false</property>
    </blacklist>
    ```



###### schema.xml

schema标签:定义逻辑库,一个schema标签对应一个逻辑库

- name属性:定义一个逻辑库给客户端看到的名称

- checkSQLschema属性:自动检测schema的名称,false不检测,true检测;例如:select * from t_order,如果false的话mycat不做自动检测schema的处理,原样表格名称t_order做计算,如果是true的话,mycat会自动根据逻辑库名称(mstest),拼接在t_order mstest.t_order;用法,如果schema逻辑库名称和真实库一致,建议配置false/true无所谓,如果不一致,必须配置false,否则无法连接使用后端数据库;

- sqlMaxLimit属性:最大查询数量,在sql语句中没有拼接limit关键字,mycat自动拼接,查询前100条;保护机制,防止客户端错误使用select语句导致mycat性能耗费;如果sql语句中客户端存在limit则不用加了。

table标签:定义逻辑表,一个schema中可以对应多个table

- name属性:逻辑表名称,无论分片非分片,对应真实表格必须一模一样的名称;

- primaryKey属性:逻辑表中的主键字段名称,也要和真实表一致

- dataNode属性:定义一个逻辑表示分片还是非分片,如果其值对应一个dataNode一定是非分片.对应多个就是分片表，也可以是全局表

- rule属性:当前表格分片表的计算分片的逻辑（算法）,不同的名称对应不同的执行算法;auto-sharding-long,根据整数id主键计算范围 0-500w 对应第一个分片,500w-1000w对应第二个分片,1000w-1500w对应第三个分片

childTable标签:与逻辑表ER分片表有关

- 与ER分片表格,主从表分片计算有关

dataNode标签:定义分片,绑定分片表格设置,绑定dataHost设置

- name:分片名称,用来计算分片的关键字(redis计算分片节点名称)
- dataHost:绑定一个dataHost标签,用来维护当前分片对应的具体数据库结构（可以是主从辅助，双击热备等结构）
- database:对应分片中数据库真实库的名称

dataHost标签:定义某个分片绑定的具体数据库结构

一个分片对应的数据库集群到低是双机热备还是单机热备.还是单节点完全取决于和dataNode绑定的dataHost决定

- name属性:dataHost标签的名称,可以用来和其他标签绑定(两个标签关联)

- maxCon属性:当前dataHost维护的所有数据库的连接池最大连接

- minCon属性:当前dataHost维护的所有数据库的连接池最小连接

- dbType属性:dataHost对应的数据库集群类型,必须全部一致

- dbDriver属性:如果是mysql类型可以使用native(本土)代替,如果是其他类型数据库,驱动类必须全路径名称

heartbeat标签:每个dataHost用来检测心跳,发现宕机的节点机制

writeHost标签:一个dataHost用来主要负责写操作的节点(主节点)

- name:名称

- url:真实库的连接地址 ip:port

- user:登录用户名

- password:登录密码

readHost标签:一个dataHost中某个writeHost包含的只能用来读的节点(可以是从节点,可以是主节点)

- name:名称

- url:真实库的连接地址 ip:port

- user:登录用户名

- password:登录密码

###### 黑名单

配置项      缺省值    描述

selelctAllow    true    是否允许执行 SELECT 语句

selectAllColumnAllow    true    是否允许执行 SELECT * FROM T 这样的语句。如果设置为 false，不允许执行 select * from t，但可以select * from (select id, name from t) a。这个选项是防御程序通过调用 select * 获得数据表的结构信息。

selectIntoAllow    true    SELECT 查询中是否允许 INTO 字句

deleteAllow    true    是否允许执行 DELETE 语句

updateAllow    true    是否允许执行 UPDATE 语句

insertAllow    true    是否允许执行 INSERT 语句

replaceAllow    true    是否允许执行 REPLACE 语句

mergeAllow    true    是否允许执行 MERGE 语句，这个只在 Oracle 中有用

callAllow    true    是否允许通过 jdbc 的 call 语法调用存储过程

setAllow    true    是否允许使用 SET 语法

truncateAllow    true    truncate 语句是危险，缺省打开，若需要自行关闭

createTableAllow    true    是否允许创建表

alterTableAllow    true    是否允许执行 Alter Table 语句

dropTableAllow    true    是否允许修改表

commentAllow    false    是否允许语句中存在注释，Oracle 的用户不用担心，Wall 能够识别 hints和注释的区别

noneBaseStatementAllow    false    是否允许非以上基本语句的其他语句，缺省关闭，通过这个选项就能够屏蔽 DDL。

multiStatementAllow    false    是否允许一次执行多条语句，缺省关闭

useAllow    true    是否允许执行 mysql 的 use 语句，缺省打开

describeAllow    true    是否允许执行 mysql 的 describe 语句，缺省打开

showAllow    true    是否允许执行 mysql 的 show 语句，缺省打开

commitAllow    true    是否允许执行 commit 操作

rollbackAllow    true    是否允许执行 roll back 操作

\##如果把 selectIntoAllow、deleteAllow、updateAllow、insertAllow、mergeAllow 都设置为 false，这就是一个只读数据源了。##

拦截配置－永真条件

selectWhereAlwayTrueCheck    true    检查 SELECT 语句的 WHERE 子句是否是一个永真条件

selectHavingAlwayTrueCheck    true    检查 SELECT 语句的 HAVING 子句是否是一个永真条件

deleteWhereAlwayTrueCheck    true    检查 DELETE 语句的 WHERE 子句是否是一个永真条件

deleteWhereNoneCheck    false    检查 DELETE 语句是否无 where 条件，这是有风险的，但不是 SQL 注入类型的风险

updateWhereAlayTrueCheck    true    检查 UPDATE 语句的 WHERE 子句是否是一个永真条件

updateWhereNoneCheck    false    检查 UPDATE 语句是否无 where 条件，这是有风险的，但不是SQL 注入类型的风险

conditionAndAlwayTrueAllow    false    检查查询条件(WHERE/HAVING 子句)中是否包含 AND 永真条件

conditionAndAlwayFalseAllow    false    检查查询条件(WHERE/HAVING 子句)中是否包含 AND 永假条件

conditionLikeTrueAllow    true    检查查询条件(WHERE/HAVING 子句)中是否包含 LIKE 永真条件

其他拦截配置

selectIntoOutfileAllow    false    SELECT ... INTO OUTFILE 是否允许，这个是 mysql 注入攻击的常见手段，缺省是禁止的

selectUnionCheck    true    检测 SELECT UNION

selectMinusCheck    true    检测 SELECT MINUS

selectExceptCheck    true    检测 SELECT EXCEPT

selectIntersectCheck    true    检测 SELECT INTERSECT

mustParameterized    false    是否必须参数化，如果为 True，则不允许类似 WHERE ID = 1 这种不参数化的 SQL

strictSyntaxCheck    true    是否进行严格的语法检测，Druid SQL Parser 在某些场景不能覆盖所有的SQL 语法，出现解析 SQL 出错，可以临时把这个选项设置为 false，同时把 SQL 反馈给 Druid 的开发者。

conditionOpXorAllow    false    查询条件中是否允许有 XOR 条件。XOR 不常用，很难判断永真或者永假，缺省不允许。

conditionOpBitwseAllow    true    查询条件中是否允许有"&"、"~"、"|"、"^"运算符。

conditionDoubleConstAllow    false    查询条件中是否允许连续两个常量运算表达式

minusAllow    true    是否允许 SELECT * FROM A MINUS SELECT * FROM B 这样的语句

intersectAllow    true    是否允许 SELECT * FROM A INTERSECT SELECT * FROM B 这样的语句

constArithmeticAllow    true    拦截常量运算的条件，比如说 WHERE FID = 3 - 1，其中"3 - 1"是常量运算表达式。

limitZeroAllow    false    是否允许 limit 0 这样的语句

禁用对象检测配置

tableCheck    true    检测是否使用了禁用的表

schemaCheck    true    检测是否使用了禁用的 Schema

functionCheck    true     检测是否使用了禁用的函数

objectCheck    true    检测是否使用了“禁用对对象”

variantCheck    true    检测是否使用了“禁用的变量”

readOnlyTables    空    指定的表只读，不能够在 SELECT INTO、DELETE、UPDATE、INSERT、MERGE 中作

### 例子

#### mysql一主一从

##### 基础操作

主从复制。准备两台虚拟机，配好网能 ping 通互联网。数据库开一下用户和防火墙，保证能连进去。

主节点(以 `192.168.126.128` 为例)，配置文件 `/etc/my.cnf` 追加：

```properties
#定义的是集群中mysql的id值,不能和其他mysql节点的id重复
server-id=1
#定义了一个开启的二进制日志文件的名称
log-bin=mysql-bin
```

> 可以查询，如 `show variables like 'server_id';`

重启，然后登进去查询 `show master status\G;`。信息解读：

- File 后缀是自增数字，从 1 开始，当服务器重启时,当文件数据量超过阈值64M/128M,会滚动,数字自增1。重启也会自增1
- Position 判断更新

mysql-bin的上一个滚动文件的指针指向下一个新文件的开始的

从节点(以 `192.168.126.129` 为例)，添加行：

```properties
server-id=2
#双机热备
log-bin=mysql-bin
```

重启，登进去 `show slave status\G;` 发现为空。

进行挂接操作，执行指令：

```mysql
CHANGE MASTER TO MASTER_HOST="192.168.126.128",
MASTER_PORT=3306,
MASTER_USER="root",
MASTER_PASSWORD="root",
MASTER_LOG_FILE="mysql-bin.000001",
MASTER_LOG_POS=376;
```

如果成功，再次查询 `show slave status\G;` 有记录。发现有：

```
Slave_IO_Running: No
Slave_SQL_Running: No
```

故，对从节点，启动：`start slave;`(关闭则 `stop slave;`)

查看 `datadir`：`show variables like 'datadir';`，用于下行查询

查看主从节点的 `cat /var/lib/mysql/auto.cnf`，发现两者 `uuid` 一致。查询 slave status 应该有：

```
Slave_IO_Running: Yes
Slave_SQL_Running: Yes
Seconds_Behind_Master: 0
```

> 如果关掉从节点，且删掉 `auto.cnf`，然后再启动(不需挂 `start slave`，会自动挂)，会得到新的 `uuid`。这种情况下，发现SQL线程能正常工作，IO线程不能正常工作，需要：
>
> 1. 从节点重启
> 2. 主节点 `flush logs;`
> 3. 从节点 `stop slave;`
> 4. 从节点重新执行 `CHANGE MASTER TO...`
> 5. 从节点 `start slave;`

主从测试：(先检查主从链接是否正常，否则不同步操作会出错)

- 主节点创建数据库表写入数据，查看从节点是否发生变化
- 从节点应当保证不写数据(如果硬要写，能写，但不会同步，导致一致性问题)
- 用户也会同步

##### 双机热备

在上文基础上(即129是128的slave)，让128再挂上129(change master to, start slave)，即互为master-slave。

主从测试：

- 无论在哪个节点更新数据，都能复制到另一个节点。
- 宕机测试，对129先关掉，然后128写数据，然后开129，期望数据同步。

#### mycat基本

##### 安装

一个下载 [release](http://dl.mycat.org.cn/1.6.7.6/20210303094759/) [链接](http://dl.mycat.org.cn/1.6.7.6/20201104174609/)

安装到任意一台虚拟机

安装mycat1.5版本需要jdk1.8支持和mysql的支持，查看支持：

```sh
java -version
```

解压 `mycat-server` 发布版 `.tar.gz` 到 `/home/software` 或别的目录

目录结构：

- bin mycat的命令脚本文件都在bin

- catlet 插件资源(默认是空)

- conf (配置文件所在路径,server.xml,schema.xml rule.xml等都是我们需要使用)

- logs:分两个

- - wrapper.log:启动日志
  - mycat.log:运行日志

启动，在 `bin` 下：(需要等待可能半分钟)

```sh
#./mycat start /后台启动,可以查看wrapper.log启动日志
#./mycat console /控制台，也可以去wrapper.log启动日志中查看 ctrl+c 可关闭
```

##### 配置

对上面的主从，假设数据库账号为 root，密码为 psw，数据库名位 mstest，有数据表 `t_user`。且 mycat 账号也设为 root，密码也设为 root(即 `server.xml`)

`server.xml`

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE mycat:server SYSTEM "server.dtd">
<mycat:server xmlns:mycat="http://io.mycat/">
    <system>
        <property name="defaultSqlParser">druidparser</property>
    </system>
    <user name="root">
        <property name="password">root</property>
        <property name="schemas">mstest</property>
    </user>
</mycat:server>
```

> 注：这里用的是自己装的版本，见上文链接，1.6 release，如果用课堂的，改成 `xmlns:mycat=http://org.opencloudb/`

`schema.xml`

```xml
<?xml version="1.0"?>
<!DOCTYPE mycat:schema SYSTEM "schema.dtd">
<mycat:schema xmlns:mycat="http://io.mycat/" >
	<schema name="mstest" checkSQLschema="false" sqlMaxLimit="100">
		<table name="t_user" dataNode="dn1" primaryKey="ID" />
	</schema>
	<dataNode name="dn1" dataHost="localhost1" database="mstest" />
	<dataHost name="localhost1" maxCon="1000" minCon="10" balance="0"
		writeType="0" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">
		<heartbeat>select user()</heartbeat>
		<writeHost host="hostM1" url="192.168.126.128:3306" user="root" password="psw"/>
	</dataHost>
</mycat:schema>
```

> 注：如果本机就是 `192.168.126.128` 必须用 `localhost`，对其他机子就用 IP 即可。不能省略端口号，不能添加前后缀如 `jdbc://` 和 `?useSSL=false`。`select user()` 含义即输出当前链接的用户@IP。

试图运行，发现成功。

默认开放 8066 端口，所以防火墙开开。

```sh
firewall-cmd --add-port=8066/tcp --permanent && firewall-cmd --reload
```

> mysql5开一下中文编码，配置项：
>
> ```properties
> character-set-server = utf8mb4
> collation-server = utf8mb4_unicode_ci
> ```
>
> 检验：
>
> ```mysql
> SHOW VARIABLES LIKE 'character_set_database';
> ```
>
> 检验数据表的：
>
> ```mysql
> SHOW CREATE TABLE t_user\G;
> ```

如果没配过，128配一下数据库：

```mysql
create database mstest;
use mstest;
create table t_user (id int, user varchar(50));
ALTER TABLE t_user CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
insert into t_user values (580,'白茶');
```

充分测试主从一致且能显示中文。

到 `bin` 去，启动：

```
./mycat console
```



> ##### mysql8适配
>
> 对 mysql8 支持不良，要自己调配：
>
> 1. 下一个 [mysql-connector-java](https://dev.mysql.com/downloads/file/?id=477058) 8 的 jar 包，复制到 `lib/`
>
>    ```sh
>    scp mysql-connector-java-8.0.11.jar root@192.168.126.128:/home/software/mycat/lib
>    ```
>
>    提权：
>
>    ```sh
>    chmod 777 mysql-connector-java-8.0.11.jar
>    ```
>
>    如果本来有 `mysql-connector-java-5.1.35.jar` 删了。
>
> 2. 数据库添加配置并重启
>
>    ```properties
>    default_authentication_plugin=mysql_native_password
>    ```
>
> 3. `server.xml` system 子节点添加：
>
>    ```xml
>    <property name="useHandshakeV10">1</property>
>    ```
>
> 没成功……

##### 基本指令

在上述配好后，随便找台机子连进去，如：

```sh
mysql -uroot -proot -h192.168.126.128 -P8066
```

必须完整测试下面的指令：

```mysql
show databases;
use mstest;
show tables;
select * from t_user;
```

如果出现虚拟机 mysql5 能连，但本机(windows) 的 mysql8 客户端无论怎么改参数都说密码错误连不上，可以考虑本机使用 python 测试：

```python
import pymysql as sql
oj = sql.connect(host='192.168.126.128', port=8066, user='root', password='root', database='mstest', charset='UTF8')
cur = oj.cursor()
cmd = 'select * from t_user'
print(cur.execute(cmd))
print(cur.fetchall())
```

#### mycat单分片

`dataHost` 的 `switchType` 若：

- -1 表示不开启当前dataHost中的高可用故障转移
- 1 表示开启故障转移

配置的dataHost中writeHost可以配置多个,按照顺序,分别在内存中给定index序号,0,1,2,3,4,5...

一旦开启故障转移,0号进行心跳检测如果发生问题,将会把1号上移

修改 schema 为双机热备结构：

```xml
<?xml version="1.0"?>
<!DOCTYPE mycat:schema SYSTEM "schema.dtd">
<mycat:schema xmlns:mycat="http://io.mycat/" >
	<schema name="mstest" checkSQLschema="false" sqlMaxLimit="100">
		<table name="t_user" dataNode="dn1" primaryKey="ID" />
	</schema>
	<dataNode name="dn1" dataHost="localhost1" database="mstest" />
	<dataHost name="localhost1" maxCon="1000" minCon="10" balance="0"
		writeType="0" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">
		<heartbeat>select user()</heartbeat>
		<writeHost host="hostM1" url="localhost:3306" user="root" password="root"/>
		<writeHost host="hostM2" url="192.168.126.129:3306" user="root" password="root"/>
	</dataHost>
</mycat:schema>
```

验证高可用：

- 将第一个writeHost宕机，mysql服务停止  mycat客户端读写正常

- 将第一个宕机重启,将第二个writeHost宕机 mycat依然支持读写正常

参考测试：

```python
import pymysql as sql
from random import randint

oj = sql.connect(host='192.168.126.128', port=8066, user='root', password='root', database='mstest', charset='UTF8')

with oj.cursor() as cur:
    sql = "INSERT INTO t_user VALUES (%s, %s)"
    values = (randint(1, 65535), '白茶')
    cur.execute(sql, values)
    oj.commit()
with oj.cursor() as cur:
    cmd = 'SELECT * FROM t_user'
    cur.execute(cmd)
    print(cur.fetchall())
```



![image-20230506145102099](img/image-20230506145102099.png)

#### mycat读写分离

##### 单分片

先清空数据库，关掉主从：

```mysql
delete from t_user;
stop slave;
```

然后给两个数据库构造不同的数据：

```mysql
insert into mstest.t_user values (580,'白茶'),(581,'果冻'); #128
insert into mstest.t_user values (580,'锦乐'),(581,'桑泽'); #129
```

schema.xml写控制dataHost的writeType：

0:表示所有的当前dataHost的写操作都在第一个writeHost中完成(index最小)

1:老版本的遗留逻辑(1.5以上的mycat中不推荐配置),对当前的dataHost中所有writeHost进行读写随机

测试配置文件直接使用高可用故障转移 保证writeType=0/或者writeType=1



dataHost标签中balance属性

0:不开启读的分离,所有的读都在第一个writeHost进行

1:开启读写分离,绝大部分读都在除了第一个writeHost以外的其他writeHost,和所有readHost中完成,当并发超过一定量时,第一个writeHost也负责一部分读工作;

2:所有的读操作，都随机的在所有的writeHost和readHost中进行

3:所有的读操作，都在readHost中完成,在集群中没有配置ReadHost的情况下,读都到第一个writeHost完成



<img src="img/image-20230506152803026.png" alt="image-20230506152803026" style="zoom:67%;" />

在2个writeHost的双机热备结构下实现读写分离,一旦某一个writeHost宕机,不会到它下面的readHost读操作

对上述单分片修改，设 switchType=1 writeType=0 balance=1，重启，执行查询，发现查到是锦乐桑泽。(写也到了129)



##### 多分片

```xml
<?xml version="1.0"?>
<!DOCTYPE mycat:schema SYSTEM "schema.dtd">
<mycat:schema xmlns:mycat="http://io.mycat/" >
	<schema name="mstest" checkSQLschema="false" sqlMaxLimit="100">
		<table name="t_user" dataNode="dn1" primaryKey="ID" />
        <table name="t_product" dataNode="dn1,dn2" primaryKey="ID" rule="auto-sharding-long"/>
	</schema>
	<dataNode name="dn1" dataHost="localhost1" database="mstest" />
    <dataNode name="dn2" dataHost="localhost2" database="mstest" />
	<dataHost name="localhost1" maxCon="1000" minCon="10" balance="0"
		writeType="0" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">
		<heartbeat>select user()</heartbeat>
		<writeHost host="hostM1" url="localhost:3306" user="root" password="root"/>
	</dataHost>
    <dataHost name="localhost2" maxCon="1000" minCon="10" balance="0"
		writeType="0" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">
		<heartbeat>select user()</heartbeat>
		<writeHost host="hostM1" url="192.168.126.129:3306" user="root" password="root"/>
	</dataHost>
</mycat:schema>
```

同目录下找 `rule.xml`，找到 `auto-sharding-long` 这一个 `tableRule`，进而找到 `function`，进而找到 `mapFile`，进而找到同目录下 `autopartition-long.txt`，删掉最后一行及 `1000M-15002M=2`，因为只有两个 DN。

两个数据库都建表：

```mysql
CREATE TABLE t_product (
    id int(11) NOT NULL AUTO_INCREMENT,
    name varchar(100) DEFAULT NULL,
    cid int(11) DEFAULT NULL,
    PRIMARY KEY (id)
) ENGINE=MyISAM DEFAULT CHARSET=utf8;
```

mycat 执行：

```mysql
insert into t_product(id,name,cid) values('1', '长虹空调', '1');
insert into t_product(id,name,cid) values('5000001', '华为手机', '2');
insert into t_product(id,name,cid) values('10000001', '三星手机', '3');#fail
select * from t_product; #都能查到，但每个数据库只存了一个
```

新增一条商品数据,观察最终新增到哪个分片

dn1,dn2;auto-sharding-long分片规则决定

dn1对应 index=0

dn2对应 index=1

auto-sharding-long默认配置中以id字段做参考计算

0-500w=index0  500w-1000w=index1  1000w-1500w=index2

也就是前 500w 自动存第一个库，后 500w 自动存第二个库。



如果计算分片的是其他数据类型 时间,字符串等等;可以使用hash一致性，如：

```mysql
CREATE TABLE es_user (
    user_id char(36) NOT NULL,
    name varchar(100) DEFAULT NULL,
    PRIMARY KEY (user_id)
) ENGINE= MyISAM DEFAULT CHARSET=utf8;
```

使用：

```xml
<table name="es_user" primaryKey="user_id" dataNode="dn1,dn2" rule="custom-by-murmur"/>
```

修改其 `rule.xml` 的 `tableRule` 对应 murmur 的 `rule` 下的 `columns` 为 `user_id`。如无添加，即：

```xml
<tableRule name="custom-by-murmur">
    <rule>
        <columns>id</columns>
        <algorithm>murmur</algorithm>
    </rule>
</tableRule>
```

看到对应 `Function` 的源码解释：

```xml
<function name="murmur"
          class="io.mycat.route.function.PartitionByMurmurHash">
    <property name="seed">0</property><!-- 默认是0 -->
    <property name="count">2</property><!-- 要分片的数据库节点数量，必须指定，否则没法分片 -->
    <property name="virtualBucketTimes">160</property><!-- 一个实际的数据库节点被映射为这么多虚拟节点，默认是160倍，也就是虚拟节点数是物理节点数的160倍 -->
    <!-- <property name="weightMapFile">weightMapFile</property> 节点的权重，没有指定权重的节点默认是1。以properties文件的格式填写，以从0开始到count-1的整数值也就是节点索引为key，以节点权重值为值。所有权重值必须是正整数，否则以1代替 -->
    <!-- <property name="bucketMapPath">/etc/mycat/bucketMapPath</property>
   用于测试时观察各物理节点与虚拟节点的分布情况，如果指定了这个属性，会把虚拟节点的murmur hash值与物理节点的映射按行输出到这个文件，没有默认值，如果不指定，就不会输出任何东西 -->
</function>
```



#### mycat全局表

两数据库建：

```mysql
CREATE TABLE t_category(
    id int(10) unsigned NOT NULL,
    name char(50) DEFAULT NULL,
    PRIMARY KEY(id)
) ENGINE=MyISAM DEFAULT CHARSET=utf8;
```

两数据库都插入同样的数据。

使用mycat维护2个逻辑表,

t_product表格:数据量大-分片表

t_category表格:数据量不大,变化不大，稳定-非分片表

mycat不支持底层跨数据库操作(不代表别的中间件不支持)

![image-20230506162742135](img/image-20230506162742135.png)

当一个逻辑表对应多个分片时,又不设置rule的分片计算规则,默认就是全局表，即：

```xml
<table name="t_category" primaryKey="id" dataNode="dn1,dn2" type="global"/>
```

测试：mycat 不能写，但是能联表查询。

```mysql
select * from t_product left jion t_category on t_product.cid=t_category.id;
```



#### mycatER表

真实数据库创建：

```mysql
CREATE TABLE  t_order (
    id varchar(100) NOT NULL,
    money int(11) DEFAULT NULL,
    PRIMARY KEY (id)
) ENGINE= InnoDB DEFAULT CHARSET=utf8;
CREATE TABLE t_order_item (
  id int(11) NOT NULL AUTO_INCREMENT,
  name varchar(100) DEFAULT NULL,
  oid varchar(100) DEFAULT NULL,
  PRIMARY KEY (id)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

配置：

```xml
<table name="t_order_item" primaryKey="id" dataNode="dn1,dn2" rule="auto-sharding-long"/>
<table name="t_order" primaryKey="id" dataNode="dn1,dn2" rule="sharding-by-murmur"/>
```

mycat 测试写入：

```mysql
insert into t_order(id,money) values('da57776f-9fae-431d-9359-18492ae13858',8000);
Insert into t_order(id,money) values('153b198e-228d-435b-91ab-58e6498c00f9',777);
Insert into t_order(id,money) values('3bca1c96-2a0f-4b0d-a684-106924823aa2',9327);
insert into t_order(id,money) values('ty57776f-9fae-431d-9359-18492ae13233',8880);
Insert into t_order_item(id,name,oid) values('1','电视','da57776f-9fae-431d-9359-18492ae13858');
Insert into t_order_item(id,name,oid) values('2','华为平板','da57776f-9fae-431d-9359-18492ae13858');
Insert into t_order_item(id,name,oid) values('300001','薯片','153b198e-228d-435b-91ab-58e6498c00f9');
Insert into t_order_item(id,name,oid) values('300002','华为手机','153b198e-228d-435b-91ab-58e6498c00f9');
Insert into t_order_item(id,name,oid) values('500001','海尔冰箱','3bca1c96-2a0f-4b0d-a684-106924823aa2');
Insert into t_order_item(id,name,oid) values('500002','格力空调','3bca1c96-2a0f-4b0d-a684-106924823aa2');
Insert into t_order_item(id,name,oid) values('200001','小米手机','ty57776f-9fae-431d-9359-18492ae13233');
Insert into t_order_item(id,name,oid) values('200002','vivo手机','ty57776f-9fae-431d-9359-18492ae13233');
```

如果执行：

```mysql
select * from t_order_item left join t_order on t_order.id=t_order_item.oid;
```

可能会报错：`ERROR 1064 (HY000): NullPointerException`

所以配置主从表格：

able标签:定义主表(父表 t_order),按照正常的分片表格配置

childTable标签:定义从表,不能指定分片,因为不能定义分片规则

joinKey属性:定义的是从表与主表的外键关联字段名称

parentKey:定义从表的外键字段在主表的名称

删掉 xml 里的 `<table name="t_order_item"` 一行，加上：

```xml
<table name="t_order" primaryKey="id" dataNode="dn1,dn2" rule="sharding-by-murmur">
    <childTable name="t_order_item" primaryKey="id" joinKey="oid" parentKey="id"/>
</table>
```

先在两数据库分别清空两表数据，然后重启，重新测试上述代码，预期正确。



# 其他数据库

## SQLite

轻量级数据

### 安装使用

以 ubuntu 为例。安装：

```sh
sudo apt-get install sqlite3
```

选定一个数据库文件做存储并启动：

```sh
sqlite3 test.sqlite
sqlite3 test.db # 多种输入格式支持
```

[教程](https://www.runoob.com/sqlite/sqlite-tutorial.html)

### 配置指令

#### 指令

`.quit` 或 `.exit`

`.help`

`.tables` 

`.schema 数据表名`

`PRAGMA foreign_keys=ON;` 开启外键约束报错



`.header on` 加上显示表头

`.mode column` 使用格式化显示(否则默认是`|`分隔和`\n`换行)；默认是 list

> 假设有四列，要求每列在长度15截断：`.width 15 15 15 15`

> 参考：(假设已经 header on)
>
> before:
>
> ```
> sid|name|surname|age|gpa
> 1|Alicia|Shan|20|3.5
> ```
>
> after:
>
> ```
> sid  name    surname  age  gpa
> ---  ------  -------  ---  ---
> 1    Alicia  Shan     20   3.5
> ```

`.mode csv` 逗号格式

`.import filename tablename` 以当前 mode 读取文件到指定表格

> 如：`.import flights-small.csv FLIGHTS`

`.nullvalue NULL` 显示 null 而不是对 null 显示空格



`.read 文件名.sql` 导入 SQL 文件指令(/数据)

#### 永久化

在 `/home/用户名` 新建 `.sqliterc` 逐行增加配置内容。

### SQL指令

只描述跟一般 SQL 不一样的地方。

可以使用单双引号、反引号(`)。可以使用方括号标注列名(兼容微软语法)

group by 对非聚合值/非聚合键不会报错，其他数据库会

整数除法：`select 5/2=2`。

字符串类型转换：`select '100'+10=110`。

> #### 数据类型

#### 表达式

字符串拼接，如 `name || ',' || score`。

> #### 函数

## PostgreSQL

### 基本

#### 安装使用

[参考 多平台安装教程](https://s3.us-west-2.amazonaws.com/ucsd.dsc100/guides/Setting+Up+PostgreSQL.pdf)

安装：

```sh
sudo apt-get update
sudo apt-get install postgresql posgresql-contrib
```

启动：

```sh
sudo service postgresql start
```

以用户 postgres 身份启动 psql：

```sh
sudo -u postgres psql
```

> ```sh
> sudo su
> su - postgres
> psql
> ```

指定数据库：

```sh
sudo -u postgres psql -U postgres -d 数据库名
```

默认端口 5432

图形化界面(pgadmin) [官网](https://www.pgadmin.org/)

### 命令

#### 基本

`\q` 退出

`\?` 帮助 或 `\h 指令` 如 `\h SELECT`

`\dt` 列出数据表)(也可 `\dt 表名` 看存在性)

`\conninfo` 查看当前连接的是哪个数据库

`\c 数据库名` 切换数据库

`\d 表名` 查看表信息

`\l` 列出所有数据库 (`\list`)

`\g` 或 `;` 执行前一个命令

`\s` 显示命令历史

`\e` 文本编辑器打开上一条命令，编辑后可以再次执行

`\i 路径` 执行该路径 SQL 脚本命令

`\timing on` 显示语句执行所花费的时间

#### SQL

删除级联：`drop table 数据表名 cascade`

#### 功能函数

having 聚合：`bool_and` 所有组内满足表达式且非 null 返回 true，否则 false

filter 子句：对聚合函数进行条件过滤

```sql
SELECT product_id,
       SUM(sales_amount) AS total_sales,
       SUM(sales_amount) FILTER (WHERE is_promotion = TRUE) AS promotion_sales
FROM sales
GROUP BY product_id;
```



#### 导入

##### sql

要导入的 `.sql` 放到诸如 `/tmp` 这样的目录，否则可能即便 `chmod 777` 了还是没法导，导之前要 create 对应的数据库

```sh
sudo -u postgres psql -d 数据库名 -f 完整文件路径
```

##### csv

[参考](https://stackoverflow.com/questions/2987433/how-to-import-csv-file-data-into-a-postgresql-table)

`copy 数据表 from '路径' with (format csv, header)` 如果第一行是数据，省去 `,header`，如果分割符不是逗号，使用 `delimiter 'xxx'` 代替

或者使用 `\copy`

### 数据类型

int

serial 自动递增的整数列从 1 开始。

boolean

varchar

## Oracle

> 研究生当助教实验课内容

### 安装与介绍

> 师兄说不推荐安装，会让自己电脑越来越卡，建议找个生产环境什么的去装

下载服务器 Oracle Database Express Edition (XE)  [here](https://www.oracle.com/database/technologies/xe-downloads.html)

> 第一个商用关系数据库管理系统，是使用最广泛的商业 DBMS 之一

安装结束，自动运行，遇到什么问题百度解决。没运行的话去服务里启动一下。

自带叫 SQL*Plus 的命令行工具作为客户端执行操作。

Oracle 使用 PL/SQL

> [PL/SQL](https://baike.baidu.com/item/PL%2FSQL/8564979)也是一种[程序语言](https://baike.baidu.com/item/程序语言/10696489)，叫做过程化[SQL](https://baike.baidu.com/item/SQL语言/4801972)[语言](https://baike.baidu.com/item/SQL语言/4801972)（Procedural Language/SQL）。PL/SQL是[Oracle](https://baike.baidu.com/item/Oracle数据库/3710800)[数据库](https://baike.baidu.com/item/Oracle数据库/3710800)对SQL语句的扩展。在普通SQL语句的使用上增加了编程语言的特点，所以PL/SQL把数据操作和查询语句组织在PL/SQL代码的过程性单元中，通过逻辑判断、循环等操作实现复杂的功能或者计算

Oracle SQL Developer 是图形化的客户端。[下载](https://www.oracle.com/tools/downloads/sqldev-downloads.html)

默认配置一般不用修改，如果要的话自行查百度修改。导航栏绿色+号连接到服务器。用户名sys，即安装时输入了密码的那个用户名。test是测试连接但不连进去。

可以调字体大小，工具首选项代码编辑器字体。

可以拖拽sql文件到工作区执行，其他所见即所得很简单不用多介绍。

### 语法

这里讲区别于 MySQL / SQL 的部分。

#### 基础语法

##### 通用

创建表格不能用 `if exists` 语法(mysql专用)，但是 exist 时会报错，会继续执行下去(即忽略create若存在)。

同目录下的文件可以直接调用，如有文件名 `abc.sql`，执行代码 `@abc` 就可以执行这个文件的全部代码。类似 python exec。文件名不区分大小写。

新建用户，用户名以 `C##` 或 `c##` 开头 [参见]([https://docs.oracle.com/database/121/SQLRF/statements_8003.htm#SQLRF01503](https://docs.oracle.com/database/121/SQLRF/statements_8003.htm) ) [权限组]([https://www.cnblogs.com/yank/p/5088621.html ](https://www.cnblogs.com/yank/p/5088621.html))

> 由于 Oracle 数据库总是在 char 类型的属性值后面加上与字符串长度相同的尾部空格，因此可以使用 rtrim 函数删除这些尾部空格。将 char 类型的属性值与正则表达式进行比较时需要使用该函数。（课件
>
> (试验 length 发现长度还是正确的)，已知 length 会统计空格

##### select

仅表达与 mysql 不同的部分：

单引号文本，dual 是用来专门执行类似 print 输出常量的库。

```sql
select '文本' from dual -- mysql 的话不用 from dual且mysql单双引号都行
-- 也可以是表达式如 select 3*4 from dual; select 1 from dual;
```

`||` 运算符连接两列(mysql没有)

as 列别名只能双引号，不能单引号或反引号(mysql 可以)

对字符串，where `=` 区分大小写 (mysql一般情况不区分大小写)

`regexp_like(列名, 表达式)` 如：(对应 mysql regexp) [ref](https://docs.oracle.com/cd/B12037_01/server.101/b10759/ap_posix001.htm#i690819) [refeg](https://www.salvis.com/blog/2018/09/28/regular-expressions-sql-examples) [在线测试PCRE选项](https://www.regextester.com/) [regexp](https://www.cnblogs.com/lxl57610/p/8227599.html)

```sql
select * from Student
where regexp_like(lastName, '([aeiou])\1', 'i');
```

> [] : 表示目标字符可以是其内部任意一个
>
> （）： 圈起一个子表达式，其作用跟算术加减乘除表达式中的括号一样
>
>  \1: 表示目标字符要连续出现 2次。 ‘i’：表达不区分大小写；3次就 \1\1
>
>  也可以将\1替换为{n}表示目标字符要连续出现n次。

支持 full outer join

##### 函数

注意要点： substr 1-indexed

initcap(str) 去除开头空格，每个单词首字母大写其他小写；mysql 没有该函数

根据运行结果，课件字符串函数5 Note 疑似不对

power 精度良好，2^64 准确的，而 mysql 不准确

trunc(a, n) a保留小数n位不四舍五入 (当然也有 round)

date 的推荐格式 date 'yyyy-mm-dd' 来强转；不然标准格式是 `dd-mm月-yyyy`很不符合国内写法。注意课件的 JAN 英文缩写不行。

ADD_MONTHS(date, number) 在给定日期 `date` 上增加 `number` 个月后的日期；mysql 没有该函数，注意日期输出格式是日月年，具体见下面的代码

NEXT_DAY(date, weekday)

```sql
SELECT ADD_MONTHS(DATE '2023-01-15', 2) from dual;
SELECT NEXT_DAY(DATE '2024-11-12', 2) FROM dual;
-- 下周一 （1是周日，2是周一，以此类推，可以1-7
-- 要么就写成 '星期日' / 一到六，同理不能写英文的
select next_day('05-10月-20', 2);
```

current_date  mysql 要 ()

to_date 感觉不如 date 简写(见上)

```sql
select to_date('2020-03-23', 'yyyy-mm-dd') from dual;
```

返回纯数字字符串，没有月，JAN 之类的：

```sql
select to_char(date '2024-11-23', 'yyyy/mm/dd') from dual; -- 2024/11/23
```

stddev 是样本方差(分母n-1)，等价于 stddev_samp。

stddev_pop 是总体偏差(分母n) 对 mysql 使用 std

##### 数据库更改

alter table xxx add (x int) 的括号不加也行，但mysql不加

可以有下面的例子，但mysql不行：

```sql
alter table t1 modify managerid references m1(mid) on delete cascade;
```



#### 配置语法

```sql
set feedback off -- 不显示执行结果输出(如成功插入) 反义词on是默认
-- 即抑制显示 SELECT、DELETE、UPDATE 或 INSERT 语句处理的记录数
set heading off -- 不显示表格表头 反义词on默认
clear screen  -- 清空输出窗口
set linesize 150;
```

